{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Distributed tensorflow training using Horovod via OCI Jobs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Contents\n",
    "\n",
    "1. [Background](#Background)\n",
    "1. [Prerequisites](#Prerequisites)\n",
    "1. [Train](#Train)\n",
    "1. [Setup IAM](#Setup%20IAM)\n",
    "1. [Build](#Build)\n",
    "\n",
    "---\n",
    "\n",
    "## Background\n",
    "\n",
    "Horovod is a distributed deep learning training framework for TensorFlow, Keras, PyTorch, and MXNet. This notebook example shows how to use Horovod with Tensorflow in OCI Data Science Jobs .\n",
    "\n",
    "For more information about the Horovod with TensorFlow , please visit [Horovod-Tensorflow](https://horovod.readthedocs.io/en/stable/tensorflow.html)\n",
    "\n",
    "---\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Prerequisites"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1. Install ads package >= 2.5.9"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip3 install oracle-ads"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2. Install docker:\n",
    "\n",
    "https://docs.docker.com/get-docker"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train\n",
    "\n",
    "### Training script\n",
    "A sample training script 'sample.py' which will be used as by different workers in the setup for distributeed training\n",
    "\n",
    "This script uses Horovod framework for distributed training where Horovod related lines are commented starting with `Horovod:`. For example, `Horovod: add Horovod DistributedOptimizer`, `Horovod: initialize optimize` and etc."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%writefile sample.py\n",
    "\n",
    "# Copyright 2021 Uber Technologies, Inc. All Rights Reserved.\n",
    "#\n",
    "# Licensed under the Apache License, Version 2.0 (the \"License\");\n",
    "# you may not use this file except in compliance with the License.\n",
    "# You may obtain a copy of the License at\n",
    "#\n",
    "#     http://www.apache.org/licenses/LICENSE-2.0\n",
    "#\n",
    "# Unless required by applicable law or agreed to in writing, software\n",
    "# distributed under the License is distributed on an \"AS IS\" BASIS,\n",
    "# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n",
    "# See the License for the specific language governing permissions and\n",
    "# limitations under the License.\n",
    "# ==============================================================================\n",
    "\n",
    "import argparse\n",
    "import tensorflow as tf\n",
    "import horovod.tensorflow.keras as hvd\n",
    "from distutils.version import LooseVersion\n",
    "\n",
    "from ocifs import OCIFileSystem\n",
    "import os\n",
    "os.environ['TF_CPP_MIN_LOG_LEVEL'] = '2'\n",
    "\n",
    "parser = argparse.ArgumentParser(description='Tensorflow 2.0 Keras MNIST Example')\n",
    "\n",
    "parser.add_argument('--use-mixed-precision', action='store_true', default=False,\n",
    "                    help='use mixed precision for training')\n",
    "\n",
    "args = parser.parse_args()\n",
    "\n",
    "if args.use_mixed_precision:\n",
    "    print(f\"using mixed precision {args.use_mixed_precision}\")\n",
    "    if LooseVersion(tf.__version__) >= LooseVersion('2.4.0'):\n",
    "        from tensorflow.keras import mixed_precision\n",
    "        mixed_precision.set_global_policy('mixed_float16')\n",
    "    else:\n",
    "        policy = tf.keras.mixed_precision.experimental.Policy('mixed_float16')\n",
    "        tf.keras.mixed_precision.experimental.set_policy(policy)\n",
    "\n",
    "# Horovod: initialize Horovod.\n",
    "hvd.init()\n",
    "\n",
    "# Horovod: pin GPU to be used to process local rank (one GPU per process)\n",
    "gpus = tf.config.experimental.list_physical_devices('GPU')\n",
    "for gpu in gpus:\n",
    "    tf.config.experimental.set_memory_growth(gpu, True)\n",
    "if gpus:\n",
    "    tf.config.experimental.set_visible_devices(gpus[hvd.local_rank()], 'GPU')\n",
    "\n",
    "# (mnist_images, mnist_labels), _ = \\\n",
    "#     tf.keras.datasets.mnist.load_data(path='mnist-%d.npz' % hvd.rank())\n",
    "\n",
    "import numpy as np\n",
    "minist_local = \"/etc/datascience/horovod/examples/tf_data/mnist.npz\"\n",
    "\n",
    "def load_data():\n",
    "    print(\"using pre-fetched dataset\")\n",
    "    with np.load(minist_local, allow_pickle=True) as f:  # pylint: disable=unexpected-keyword-arg\n",
    "        x_train, y_train = f['x_train'], f['y_train']\n",
    "        x_test, y_test = f['x_test'], f['y_test']\n",
    "        return (x_train, y_train), (x_test, y_test)\n",
    "\n",
    "(mnist_images, mnist_labels), _ = load_data() if os.path.exists(minist_local) else tf.keras.datasets.mnist.load_data(path='mnist-%d.npz' % hvd.rank())\n",
    "\n",
    "\n",
    "dataset = tf.data.Dataset.from_tensor_slices(\n",
    "    (tf.cast(mnist_images[..., tf.newaxis] / 255.0, tf.float32),\n",
    "             tf.cast(mnist_labels, tf.int64))\n",
    ")\n",
    "dataset = dataset.repeat().shuffle(10000).batch(128)\n",
    "\n",
    "model = tf.keras.Sequential([\n",
    "    tf.keras.layers.Conv2D(32, [3, 3], activation='relu'),\n",
    "    tf.keras.layers.Conv2D(64, [3, 3], activation='relu'),\n",
    "    tf.keras.layers.MaxPooling2D(pool_size=(2, 2)),\n",
    "    tf.keras.layers.Dropout(0.25),\n",
    "    tf.keras.layers.Flatten(),\n",
    "    tf.keras.layers.Dense(128, activation='relu'),\n",
    "    tf.keras.layers.Dropout(0.5),\n",
    "    tf.keras.layers.Dense(10, activation='softmax')\n",
    "])\n",
    "\n",
    "# Horovod: adjust learning rate based on number of GPUs.\n",
    "scaled_lr = 0.001 * hvd.size()\n",
    "opt = tf.optimizers.Adam(scaled_lr)\n",
    "\n",
    "# Horovod: add Horovod DistributedOptimizer.\n",
    "opt = hvd.DistributedOptimizer(\n",
    "    opt, backward_passes_per_step=1, average_aggregated_gradients=True)\n",
    "\n",
    "# Horovod: Specify `experimental_run_tf_function=False` to ensure TensorFlow\n",
    "# uses hvd.DistributedOptimizer() to compute gradients.\n",
    "model.compile(loss=tf.losses.SparseCategoricalCrossentropy(),\n",
    "                    optimizer=opt,\n",
    "                    metrics=['accuracy'],\n",
    "                    experimental_run_tf_function=False)\n",
    "\n",
    "# Horovod: initialize optimizer state so we can synchronize across workers\n",
    "# Keras has empty optimizer variables() for TF2:\n",
    "# https://sourcegraph.com/github.com/tensorflow/tensorflow@v2.4.1/-/blob/tensorflow/python/keras/optimizer_v2/optimizer_v2.py#L351:10\n",
    "model.fit(dataset, steps_per_epoch=1, epochs=1, callbacks=None)\n",
    "\n",
    "state = hvd.elastic.KerasState(model, batch=0, epoch=0)\n",
    "\n",
    "def on_state_reset():\n",
    "    tf.keras.backend.set_value(state.model.optimizer.lr,  0.001 * hvd.size())\n",
    "    # Re-initialize, to join with possible new ranks\n",
    "    state.model.fit(dataset, steps_per_epoch=1, epochs=1, callbacks=None)\n",
    "\n",
    "state.register_reset_callbacks([on_state_reset])\n",
    "\n",
    "\n",
    "callbacks = [\n",
    "    hvd.callbacks.MetricAverageCallback(),\n",
    "    hvd.elastic.UpdateEpochStateCallback(state),\n",
    "    hvd.elastic.UpdateBatchStateCallback(state),\n",
    "    hvd.elastic.CommitStateCallback(state),\n",
    "]\n",
    "\n",
    "# Horovod: save checkpoints only on worker 0 to prevent other workers from corrupting them.\n",
    "# save the artifacts in the ARTIFACTS_DIR dir.\n",
    "artifacts_dir=os.environ.get(\"ARTIFACTS_DIR\")\n",
    "tb_logs_path = os.path.join(artifacts_dir,\"logs\")\n",
    "check_point_path =  os.path.join(artifacts_dir,\"ckpts\",'checkpoint-{epoch}.h5')\n",
    "if hvd.rank() == 0:\n",
    "    callbacks.append(tf.keras.callbacks.ModelCheckpoint(check_point_path))\n",
    "    callbacks.append(tf.keras.callbacks.TensorBoard(tb_logs_path))\n",
    "\n",
    "# Train the model.\n",
    "# Horovod: adjust number of steps based on number of GPUs.\n",
    "@hvd.elastic.run\n",
    "def train(state):\n",
    "    state.model.fit(dataset, steps_per_epoch=500 // hvd.size(),\n",
    "                    epochs=2-state.epoch, callbacks=callbacks,\n",
    "                    verbose=1)\n",
    "\n",
    "train(state)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setup IAM\n",
    "\n",
    "### Create the Dynamic Group\n",
    "```\n",
    "ALL {resource.type = ‘datasciencejobrun’, resource.compartment.id = <COMPARTMENT_OCID>}\n",
    "```\n",
    "\n",
    "### Create the following Policy\n",
    "```\n",
    "Allow dynamic-group <DYNAMIC_GROUP_NAME> to use log-content in compartment <COMPARTMENT_NAME>\n",
    "Allow dynamic-group <DYNAMIC_GROUP_NAME> to use log-groups in compartment <COMPARTMENT_NAME>\n",
    "Allow dynamic-group <DYNAMIC_GROUP_NAME> to inspect repos in compartment <COMPARTMENT_NAME>\n",
    "Allow dynamic-group <DYNAMIC_GROUP_NAME> to inspect vcns in compartment <COMPARTMENT_NAME>\n",
    "Allow dynamic-group <DYNAMIC_GROUP_NAME> to manage objects in compartment <COMPARTMENT_NAME> where any {target.bucket.name=<BUCKET_NAME>}\n",
    "Allow dynamic-group <DYNAMIC_GROUP_NAME> to manage buckets in compartment <COMPARTMENT_NAME> where any {target.bucket.name='BUCKET_NAME'}\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Build\n",
    "\n",
    "### Initialize a distributed-training folder\n",
    "By this time, you would have created a training file (or files) - sample.py from the above example. Now running the command below"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!ads opctl distributed-training init --framework horovod --version v1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Build Docker image\n",
    "\n",
    "The sample code is assumed to be in the current working directory."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!docker build -f oci_dist_training_artifacts/horovod/docker/tensorflow.cpu.Dockerfile -t (IMAGE-NAME):(TAG) ."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Push the Docker Image to your Tenancy OCIR\n",
    "\n",
    "#### Steps\n",
    "1. Follow the instructions to setup container registry from [here](https://docs.oracle.com/en-us/iaas/Content/Registry/Tasks/registrypushingimagesusingthedockercli.htm)\n",
    "2. Make sure you create a repository in OCIR to push the image\n",
    "3. Tag Local Docker image that needs to be pushed -> `docker tag \"image-identifier\" \"target-tag\"`\n",
    "4. Push the Docker image from the client machine to Container Registry -> `docker push \"target-tag\"`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Tag Docker image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!docker tag hvdjob-cpu-tf-1.0 iad.ocir.io/<TENANCY_NAME>/horovod:hvdjob-cpu-tf-1.0"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Push Docker Image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!docker push <REGION_CODE>.ocir.io/<TENANCY_NAME>/horovod:hvdjob-cpu-tf-1.0"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Define your workload yaml:\n",
    "\n",
    "The yaml file is a declarative way to express the workload.\n",
    "Edit the `\"oci_dist_training_artifacts/horovod/jobrun_config/jobrun_config_hvd_tf.yaml\"` file to specify run config\n",
    "\n",
    "The following variables are tenancy specific that needs to be modified\n",
    "\n",
    "| Variable | Description |\n",
    "| :-------- | :----------- |\n",
    "|compartmentId|OCID of the compartment where Data Science projects are created|\n",
    "|projectId|OCID of the project created in Data Science service|\n",
    "|subnetId|OCID of the subnet attached your Job|\n",
    "|logGroupId|OCID of the log group for JobRun logs|\n",
    "|image|Image from OCIR to be used for JobRuns|\n",
    "|workDir|URL to the working directory for opctl|\n",
    "|WORKSPACE|Workspace with the working directory to be used|\n",
    "|entryPoint|The script to be executed when launching the container|\n",
    "\n",
    "```yaml\n",
    "kind: distributed\n",
    "apiVersion: v1.0\n",
    "spec:\n",
    "  infrastructure: # This section maps to Job definition. Does not include environment variables\n",
    "    kind: infrastructure\n",
    "    type: dataScienceJob\n",
    "    apiVersion: v1.0\n",
    "    spec:\n",
    "      projectId: <PROJECT_OCID>\n",
    "      compartmentId: <COMPARTMENT_OCID>\n",
    "      displayName: <DISPLAY_NAME>\n",
    "      logGroupId: <LOG_GROUP_OCID>\n",
    "      subnetId: <SUBNET_OCID>\n",
    "      shapeName: <COMPUTE_SHAPE>\n",
    "      blockStorageSize: <SIZE_IN_GB>\n",
    "      blockStorageSizeInGBs: <SIZE_IN_GB>\n",
    "  cluster:\n",
    "    kind: HOROVOD\n",
    "    apiVersion: v1.0\n",
    "    spec:\n",
    "      image: \"<REGION_CODE>.ocir.io/<TENANCY_NAME>/<REPOSITORY_NAME>:<IMAGE_TAG>\"\n",
    "      workDir:  \"oci://<BUCKET_NAME>@<NAMESPACE>/\"\n",
    "      name: \"<DISPLAY_NAME>\"\n",
    "      config:\n",
    "        env:\n",
    "          - name: MIN_NP\n",
    "            value: 2\n",
    "          - name: MAX_NP\n",
    "            value: 8\n",
    "          - name: SLOTS\n",
    "            value: 2\n",
    "          - name: WORKER_PORT\n",
    "            value: 12345\n",
    "          - name: GLOO_TIMEOUT_SECONDS\n",
    "            value: 90\n",
    "          - name: START_TIMEOUT\n",
    "            value: 700\n",
    "          - name: ENABLE_TIMELINE\n",
    "            value: 1\n",
    "          - name: SYNC_ARTIFACTS\n",
    "            value: 1\n",
    "          - name: WORKSPACE\n",
    "            value: \"horovod-ws\"\n",
    "          - name: WORKSPACE_PREFIX\n",
    "            value: \"hvd\"\n",
    "      main:\n",
    "        name: \"scheduler\"\n",
    "        replicas: 1\n",
    "      worker:\n",
    "        name: \"worker\"\n",
    "        replicas: 2\n",
    "  runtime:\n",
    "    kind: python\n",
    "    apiVersion: v1.0\n",
    "    spec:\n",
    "      entryPoint: \"/code/sample.py\"\n",
    "      args: \"\"\n",
    "      kwargs: \"\"\n",
    "      env:\n",
    "        - name: ARTIFACTS_DIR\n",
    "          value: \"/opt/ml\"\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  },
  "notice": "Copyright 2020 Amazon.com, Inc. or its affiliates. All Rights Reserved.  Licensed under the Apache License, Version 2.0 (the \"License\"). You may not use this file except in compliance with the License. A copy of the License is located at http://aws.amazon.com/apache2.0/ or in the \"license\" file accompanying this file. This file is distributed on an \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied. See the License for the specific language governing permissions and limitations under the License."
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
