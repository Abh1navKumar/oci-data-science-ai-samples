{
 "cells": [
  {
   "cell_type": "raw",
   "id": "8d8b55ee",
   "metadata": {},
   "source": [
    "qweews@notebook{feature_store_schema_evolution.ipynb,\n",
    "    title: Schema Enforcement and Schema Evolution in Feature Store,\n",
    "    summary: Perform Schema Enforcement and Schema Evolution in Feature Store when materialising the data.,\n",
    "    developed_on: fspyspark32_p38_cpu_v2,\n",
    "    keywords: feature store, querying ,schema enforcement,schema evolution\n",
    "    license: Universal Permissive License v 1.0\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "27783b40",
   "metadata": {},
   "outputs": [],
   "source": [
    "!odsc conda install -s fspyspark32_p38_cpu_v2"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "48cc5c19",
   "metadata": {},
   "source": [
    "Oracle Data Science service sample notebook.\n",
    "\n",
    "Copyright (c) 2022, 2023 Oracle, Inc. All rights reserved. Licensed under the [Universal Permissive License v 1.0](https://oss.oracle.com/licenses/upl).\n",
    "\n",
    "***\n",
    "\n",
    "# <font color=\"red\">Schema enforcement and schema evolution</font>\n",
    "<p style=\"margin-left:10%; margin-right:10%;\">by the <font color=\"teal\">Oracle Cloud Infrastructure Data Science Service.</font></p>\n",
    "\n",
    "---\n",
    "# Overview:\n",
    "---\n",
    "Managing many datasets, data sources and transformations for machine learning is complex and costly. Poorly cleaned data, data issues, bugs in transformations, data drift, and training serving skew all lead to increased model development time and poor model performance. Feature store can be used to solve many of the problems becuase it provides a centralised way to transform and access data for training and serving time. Feature store helps define a standardised pipeline for ingestion of data and querying of data. This notebook shows how schema enforcement and schema evolution are carried out in Feature Store\n",
    "\n",
    "Compatible conda pack: [PySpark 3.2 and Feature store](https://docs.oracle.com/iaas/data-science/using/conda-pyspark-fam.htm) for CPU on Python 3.8\n",
    "\n",
    "<div>\n",
    "    <img src=\"https://objectstorage.us-ashburn-1.oraclecloud.com/p/hh2NOgFJbVSg4amcLM3G3hkTuHyBD-8aE_iCsuZKEvIav1Wlld-3zfCawG4ycQGN/n/ociodscdev/b/oci-feature-store/o/beta/_images/overview-roles.png\"  />\n",
    "</div>\n",
    "\n",
    "## Contents:\n",
    "\n",
    "- <a href='#introduction'>1. Introduction</a>\n",
    "- <a href='#pre_requisites'>2. Pre-requisites to Running this Notebook</a>\n",
    "    - <a href='#setup_setup'>2.1. Setup</a>\n",
    "    - <a href='#policies_'>2.2. Policies</a>\n",
    "    - <a href='#authentication'>2.3. Authentication</a>\n",
    "    - <a href='#variables'>2.4. Variables</a>\n",
    "- <a href='#schema'>3. Schema enforcement and schema evolution</a>\n",
    "    - <a href='#dataexploration'>3.1. Exploration of data in feature store</a>\n",
    "    - <a href='#feature_store'>3.2. Create feature store logical entities</a>\n",
    "    - <a href='#schema_enforcement'>3.3. Schema enforcement</a>\n",
    "    - <a href='#schema_evolution'>3.4. Schema evolution</a>\n",
    "    - <a href='#ingestion_modes'>3.5. Ingestion Modes</a>\n",
    "        - <a href='#append'>3.5.1. Append</a>\n",
    "        - <a href='#overwrite'>3.5.2. Overwrite</a>\n",
    "        - <a href='#upsert'>3.5.3. Upsert</a>\n",
    "    - <a href='#history'>3.6. Viewing Feature Group History</a>\n",
    "    - <a href='#preview'>3.7. Time travel Queries on Feature Group </a>\n",
    "- <a href='#references'>4. References</a>\n",
    "\n",
    "---\n",
    "\n",
    "**Important:**\n",
    "\n",
    "Placeholder text for required values are surrounded by angle brackets that must be removed when adding the indicated content. For example, when adding a database name to `database_name = \"<database_name>\"` would become `database_name = \"production\"`.\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "86503e57",
   "metadata": {},
   "source": [
    "<a id='introduction'></a>\n",
    "# 1. Introduction\n",
    "\n",
    "OCI Data Science feature store is a stack-based API solution that's deployed using OCI Resource Manager in your tenancy.\n",
    "\n",
    "Review the following key terms to understand the Data Science feature store:\n",
    "\n",
    "\n",
    "* **Feature Vector**: Set of feature values for any one primary or identifier key. For example, all or a subset of features of customer id ‘2536’ can be called as one feature vector.\n",
    "\n",
    "* **Feature**: A feature is an individual measurable property or characteristic of a phenomenon being observed.\n",
    "\n",
    "* **Entity**: An entity is a group of semantically related features. The first step a consumer of features would typically do when accessing the feature store service is to list the entities and the entities associated features. Or, an entity is an object or concept that is described by its features. Examples of entities are customer, product, transaction, review, image, document, and so on.\n",
    "\n",
    "* **Feature Group**: A feature group in a feature store is a collection of related features that are often used together in machine learning (ML) models. It serves as an organizational unit within the feature store for you to manage, version, and share features across different ML projects. By organizing features into groups, data scientists and ML engineers can efficiently discover, reuse, and collaborate on features reducing the redundant work and ensuring consistency in feature engineering.\n",
    "\n",
    "* **Feature Group Job**: A feature group job is the processing instance of a feature group. Each feature group job includes validation and statistics results.\n",
    "\n",
    "* **Dataset**: A dataset is a collection of features that are used together to either train a model or perform model inference.\n",
    "\n",
    "* **Dataset Job**: A dataset job is the processing instance of a dataset. Each dataset job includes validation and statistics results."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f8184056",
   "metadata": {},
   "source": [
    "<a id='pre_requisites'></a>\n",
    "# 2. Pre-requisites to Running this Notebook\n",
    "Notebook Sessions are accessible using the PySpark 3.2 and Feature Store Python 3.8 (fspyspark32_p38_cpu_v2) conda environment.\n",
    "\n",
    "You can customize `fspyspark32_p38_cpu_v2`, publish it, and use it as a runtime environment for a Notebook session.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1efef740",
   "metadata": {},
   "source": [
    "<a id='setup_setup'></a>\n",
    "### 2.1. Setup\n",
    "\n",
    "<a id='setup_spark-defaults'></a>\n",
    "### `spark-defaults.conf`\n",
    "\n",
    "The `spark-defaults.conf` file is used to define the properties that are used by Spark. A templated version is installed when you install a Data Science conda environment that supports PySpark. However, you must update the template so that the Data Catalog metastore can be accessed. You can do this manually. However, the `odsc data-catalog config` commandline tool is ideal for setting up the file because it gathers information about your environment, and uses that to build the file.\n",
    "\n",
    "The `odsc data-catalog config` command line tool needs the `--metastore` option to define the Data Catalog metastore OCID. No other command line option is needed because settings have default values, or they take values from your notebook session environment. Following are common parameters that you may need to override.\n",
    "\n",
    "The `--authentication` option sets the authentication mode. It supports resource principal and API keys. The preferred method for authentication is resource principal, which is sent with `--authentication resource_principal`. If you want to use API keys, then use the `--authentication api_key` option. If the `--authentication` isn't specified, API keys are used. When API keys are used, information from the OCI configuration file is used to create the `spark-defaults.conf` file.\n",
    "\n",
    "Object Storage and Data Catalog are regional services. By default, the region is set to the region your notebook session is running in. This information is taken from the environment variable, `NB_REGION`. Use the `--region` option to override this behavior.\n",
    "\n",
    "The default location of the `spark-defaults.conf` file is `/home/datascience/spark_conf_dir` as defined in the `SPARK_CONF_DIR` environment variable. Use the `--output` option to define the directory where to write the file.\n",
    "\n",
    "You need to determine what settings are appropriate for your configuration. However, the following works for most configurations and is run in a terminal window.\n",
    "\n",
    "```bash\n",
    "odsc data-catalog config --authentication resource_principal --metastore <metastore_id>\n",
    "```\n",
    "For more assistance, use the following command in a terminal window:\n",
    "\n",
    "```bash\n",
    "odsc data-catalog config --help\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3b714d5f",
   "metadata": {},
   "source": [
    "<a id='policies_'></a>\n",
    "### 2.2. Policies\n",
    "This section covers the creation of dynamic groups and policies needed to use the service.\n",
    "\n",
    "* [Data Flow Policies](https://docs.oracle.com/iaas/data-flow/using/policies.htm/)\n",
    "* [Data Catalog Metastore Required Policies](https://docs.oracle.com/en-us/iaas/data-catalog/using/metastore.htm)\n",
    "* [Getting Started with Data Flow](https://docs.oracle.com/iaas/data-flow/using/dfs_getting_started.htm)\n",
    "* [About Data Science Policies](https://docs.oracle.com/iaas/data-science/using/policies.htm)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6e486dc3",
   "metadata": {},
   "source": [
    "<a id=\"authentication\"></a>\n",
    "### 2.3. Authentication\n",
    "The [Oracle Accelerated Data Science SDK (ADS)](https://docs.oracle.com/iaas/tools/ads-sdk/latest/index.html) controls the authentication mechanism with the notebook session.<br>\n",
    "To setup authentication use the ```ads.set_auth(\"resource_principal\")``` or ```ads.set_auth(\"api_key\")```."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "0b996f10",
   "metadata": {
    "ExecuteTime": {
     "start_time": "2023-05-24T08:26:08.577504Z"
    },
    "is_executing": true,
    "pycharm": {
     "is_executing": true
    }
   },
   "outputs": [],
   "source": [
    "import ads\n",
    "ads.set_auth(auth=\"resource_principal\", client_kwargs={\"fs_service_endpoint\": \"https://{api_gateway}/20230101\"})"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "07b7639b",
   "metadata": {},
   "source": [
    "<a id=\"variables\"></a>\n",
    "### 2.4. Variables\n",
    "To run this notebook, you must provide some information about your tenancy configuration. To create and run a feature store, you must specify a `<compartment_id>` and  `<metastore_id>` for offline feature store."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "941a3d41",
   "metadata": {
    "pycharm": {
     "is_executing": true
    }
   },
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "compartment_id = os.environ.get(\"NB_SESSION_COMPARTMENT_OCID\")\n",
    "metastore_id = \"<metastore_id>\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "260f1458",
   "metadata": {},
   "source": [
    "<a id=\"schema\"></a>\n",
    "# 3. Schema enforcement and schema evolution\n",
    "By default the **PySpark 3.2, Feature store and Data Flow** conda environment includes pre-installed [great-expectations](https://legacy.docs.greatexpectations.io/en/latest/reference/core_concepts/validation.html).Schema enforcement is a Delta Lake feature that prevents you from appending data with a different schema to a table.To change a table's current schema and to accommodate data that is changing over time,schema evolution feature is used while performing an append or overwrite operation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "76e2c53e",
   "metadata": {
    "pycharm": {
     "is_executing": true
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:py.warnings:/home/datascience/conda/fspyspark32_p38_cpu_v2/lib/python3.8/site-packages/ads/model/deployment/model_deployment.py:54: DeprecationWarning: The `ads.model.deployment.model_deployment_properties` is deprecated in `oracle-ads 2.8.6` and will be removed in `oracle-ads 3.0`.Use `ModelDeploymentInfrastructure` and `ModelDeploymentRuntime` classes in `ads.model.deployment` module for configuring model deployment. Check https://accelerated-data-science.readthedocs.io/en/latest/user_guide/model_registration/introduction.html\n",
      "  from .model_deployment_properties import ModelDeploymentProperties\n",
      "\n",
      "WARNING:py.warnings:/home/datascience/conda/fspyspark32_p38_cpu_v2/lib/python3.8/site-packages/ads/model/deployment/__init__.py:7: DeprecationWarning: The `ads.model.deployment.model_deployer` is deprecated in `oracle-ads 2.8.6` and will be removed in `oracle-ads 3.0`.Use `ModelDeployment` class in `ads.model.deployment` module for initializing and deploying model deployment. Check https://accelerated-data-science.readthedocs.io/en/latest/user_guide/model_registration/introduction.html\n",
      "  from .model_deployer import ModelDeployer\n",
      "\n",
      "WARNING:py.warnings:/home/datascience/conda/fspyspark32_p38_cpu_v2/lib/python3.8/site-packages/pyspark/sql/pandas/utils.py:35: DeprecationWarning: distutils Version classes are deprecated. Use packaging.version instead.\n",
      "  if LooseVersion(pandas.__version__) < LooseVersion(minimum_pandas_version):\n",
      "\n",
      "WARNING:py.warnings:/home/datascience/conda/fspyspark32_p38_cpu_v2/lib/python3.8/site-packages/pyspark/sql/pandas/utils.py:57: DeprecationWarning: distutils Version classes are deprecated. Use packaging.version instead.\n",
      "  if LooseVersion(pyarrow.__version__) < LooseVersion(minimum_pyarrow_version):\n",
      "\n",
      "WARNING:py.warnings:/home/datascience/conda/fspyspark32_p38_cpu_v2/lib/python3.8/site-packages/pyspark/pandas/__init__.py:44: DeprecationWarning: distutils Version classes are deprecated. Use packaging.version instead.\n",
      "  LooseVersion(pyarrow.__version__) >= LooseVersion(\"2.0.0\")\n",
      "\n",
      "WARNING:root:'PYARROW_IGNORE_TIMEZONE' environment variable was not set. It is required to set this environment variable to '1' in both driver and executor sides if you use pyarrow>=2.0.0. pandas-on-Spark will set it for you but it does not work if there is a Spark context already launched.\n",
      "WARNING:py.warnings:/home/datascience/conda/fspyspark32_p38_cpu_v2/lib/python3.8/site-packages/pyspark/pandas/frame.py:62: DeprecationWarning: distutils Version classes are deprecated. Use packaging.version instead.\n",
      "  if LooseVersion(pd.__version__) >= LooseVersion(\"0.24\"):\n",
      "\n",
      "WARNING:py.warnings:/home/datascience/conda/fspyspark32_p38_cpu_v2/lib/python3.8/site-packages/pyspark/pandas/missing/frame.py:81: DeprecationWarning: distutils Version classes are deprecated. Use packaging.version instead.\n",
      "  if LooseVersion(pd.__version__) < LooseVersion(\"1.0\"):\n",
      "\n",
      "WARNING:py.warnings:/home/datascience/conda/fspyspark32_p38_cpu_v2/lib/python3.8/site-packages/pyspark/pandas/missing/indexes.py:85: DeprecationWarning: distutils Version classes are deprecated. Use packaging.version instead.\n",
      "  if LooseVersion(pd.__version__) < LooseVersion(\"1.0\"):\n",
      "\n",
      "WARNING:py.warnings:/home/datascience/conda/fspyspark32_p38_cpu_v2/lib/python3.8/site-packages/pyspark/pandas/missing/indexes.py:191: DeprecationWarning: distutils Version classes are deprecated. Use packaging.version instead.\n",
      "  if LooseVersion(pd.__version__) < LooseVersion(\"1.0\"):\n",
      "\n",
      "WARNING:py.warnings:/home/datascience/conda/fspyspark32_p38_cpu_v2/lib/python3.8/site-packages/pyspark/pandas/missing/series.py:89: DeprecationWarning: distutils Version classes are deprecated. Use packaging.version instead.\n",
      "  if LooseVersion(pd.__version__) < LooseVersion(\"1.0\"):\n",
      "\n",
      "WARNING:py.warnings:/home/datascience/conda/fspyspark32_p38_cpu_v2/lib/python3.8/site-packages/pyspark/pandas/groupby.py:50: DeprecationWarning: distutils Version classes are deprecated. Use packaging.version instead.\n",
      "  if LooseVersion(pd.__version__) >= LooseVersion(\"1.3.0\"):\n",
      "\n",
      "WARNING:py.warnings:/home/datascience/conda/fspyspark32_p38_cpu_v2/lib/python3.8/site-packages/fs/__init__.py:4: DeprecationWarning: Deprecated call to `pkg_resources.declare_namespace('fs')`.\n",
      "Implementing implicit namespace packages (as specified in PEP 420) is preferred to `pkg_resources.declare_namespace`. See https://setuptools.pypa.io/en/latest/references/keywords.html#keyword-namespace-packages\n",
      "  __import__(\"pkg_resources\").declare_namespace(__name__)  # type: ignore\n",
      "\n",
      "WARNING:py.warnings:/home/datascience/conda/fspyspark32_p38_cpu_v2/lib/python3.8/site-packages/fs/opener/__init__.py:6: DeprecationWarning: Deprecated call to `pkg_resources.declare_namespace('fs.opener')`.\n",
      "Implementing implicit namespace packages (as specified in PEP 420) is preferred to `pkg_resources.declare_namespace`. See https://setuptools.pypa.io/en/latest/references/keywords.html#keyword-namespace-packages\n",
      "  __import__(\"pkg_resources\").declare_namespace(__name__)  # type: ignore\n",
      "\n",
      "WARNING:py.warnings:/home/datascience/conda/fspyspark32_p38_cpu_v2/lib/python3.8/site-packages/pkg_resources/__init__.py:2350: DeprecationWarning: Deprecated call to `pkg_resources.declare_namespace('fs')`.\n",
      "Implementing implicit namespace packages (as specified in PEP 420) is preferred to `pkg_resources.declare_namespace`. See https://setuptools.pypa.io/en/latest/references/keywords.html#keyword-namespace-packages\n",
      "  declare_namespace(parent)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from ads.feature_store.feature_store import FeatureStore\n",
    "from ads.feature_store.feature_group import FeatureGroup\n",
    "from ads.feature_store.model_details import ModelDetails\n",
    "from ads.feature_store.dataset import Dataset\n",
    "from ads.feature_store.common.enums import DatasetIngestionMode\n",
    "\n",
    "from ads.feature_store.feature_group_expectation import ExpectationType\n",
    "from great_expectations.core import ExpectationSuite, ExpectationConfiguration\n",
    "from ads.feature_store.feature_store_registrar import FeatureStoreRegistrar"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c2b9167f",
   "metadata": {},
   "source": [
    "<a id='dataexploration'></a>\n",
    "### 3.1. Exploration of data in feature store\n",
    "\n",
    "<div>\n",
    "    <img src=\"https://objectstorage.us-ashburn-1.oraclecloud.com/p/hh2NOgFJbVSg4amcLM3G3hkTuHyBD-8aE_iCsuZKEvIav1Wlld-3zfCawG4ycQGN/n/ociodscdev/b/oci-feature-store/o/beta/_images/feature_store_demo.jpg\" width=\"700\" height=\"350\" />\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "8a69bfa3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:py.warnings:/tmp/ipykernel_2335/906484602.py:1: DtypeWarning: Columns (7,8) have mixed types. Specify dtype option on import or set low_memory=False.\n",
      "  flights_df = pd.read_csv(\"https://objectstorage.us-ashburn-1.oraclecloud.com/p/hh2NOgFJbVSg4amcLM3G3hkTuHyBD-8aE_iCsuZKEvIav1Wlld-3zfCawG4ycQGN/n/ociodscdev/b/oci-feature-store/o/beta/data/flights/flights.csv\")[['YEAR', 'MONTH', 'DAY', 'DAY_OF_WEEK', 'AIRLINE', 'FLIGHT_NUMBER', 'ORIGIN_AIRPORT', 'DESTINATION_AIRPORT']]\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>YEAR</th>\n",
       "      <th>MONTH</th>\n",
       "      <th>DAY</th>\n",
       "      <th>DAY_OF_WEEK</th>\n",
       "      <th>AIRLINE</th>\n",
       "      <th>FLIGHT_NUMBER</th>\n",
       "      <th>ORIGIN_AIRPORT</th>\n",
       "      <th>DESTINATION_AIRPORT</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>2015</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>4</td>\n",
       "      <td>AS</td>\n",
       "      <td>98</td>\n",
       "      <td>ANC</td>\n",
       "      <td>SEA</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2015</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>4</td>\n",
       "      <td>AA</td>\n",
       "      <td>2336</td>\n",
       "      <td>LAX</td>\n",
       "      <td>PBI</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2015</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>4</td>\n",
       "      <td>US</td>\n",
       "      <td>840</td>\n",
       "      <td>SFO</td>\n",
       "      <td>CLT</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>2015</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>4</td>\n",
       "      <td>AA</td>\n",
       "      <td>258</td>\n",
       "      <td>LAX</td>\n",
       "      <td>MIA</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>2015</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>4</td>\n",
       "      <td>AS</td>\n",
       "      <td>135</td>\n",
       "      <td>SEA</td>\n",
       "      <td>ANC</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   YEAR  MONTH  DAY  DAY_OF_WEEK AIRLINE  FLIGHT_NUMBER ORIGIN_AIRPORT  \\\n",
       "0  2015      1    1            4      AS             98            ANC   \n",
       "1  2015      1    1            4      AA           2336            LAX   \n",
       "2  2015      1    1            4      US            840            SFO   \n",
       "3  2015      1    1            4      AA            258            LAX   \n",
       "4  2015      1    1            4      AS            135            SEA   \n",
       "\n",
       "  DESTINATION_AIRPORT  \n",
       "0                 SEA  \n",
       "1                 PBI  \n",
       "2                 CLT  \n",
       "3                 MIA  \n",
       "4                 ANC  "
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "flights_df = pd.read_csv(\"https://objectstorage.us-ashburn-1.oraclecloud.com/p/hh2NOgFJbVSg4amcLM3G3hkTuHyBD-8aE_iCsuZKEvIav1Wlld-3zfCawG4ycQGN/n/ociodscdev/b/oci-feature-store/o/beta/data/flights/flights.csv\")[['YEAR', 'MONTH', 'DAY', 'DAY_OF_WEEK', 'AIRLINE', 'FLIGHT_NUMBER', 'ORIGIN_AIRPORT', 'DESTINATION_AIRPORT']]\n",
    "flights_df = flights_df.head(100)\n",
    "flights_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "98ec9ed9",
   "metadata": {
    "pycharm": {
     "is_executing": true
    }
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>IATA_CODE</th>\n",
       "      <th>AIRPORT</th>\n",
       "      <th>CITY</th>\n",
       "      <th>STATE</th>\n",
       "      <th>LATITUDE</th>\n",
       "      <th>LONGITUDE</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>ABE</td>\n",
       "      <td>Lehigh Valley International Airport</td>\n",
       "      <td>Allentown</td>\n",
       "      <td>PA</td>\n",
       "      <td>40.65236</td>\n",
       "      <td>-75.44040</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>ABI</td>\n",
       "      <td>Abilene Regional Airport</td>\n",
       "      <td>Abilene</td>\n",
       "      <td>TX</td>\n",
       "      <td>32.41132</td>\n",
       "      <td>-99.68190</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>ABQ</td>\n",
       "      <td>Albuquerque International Sunport</td>\n",
       "      <td>Albuquerque</td>\n",
       "      <td>NM</td>\n",
       "      <td>35.04022</td>\n",
       "      <td>-106.60919</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>ABR</td>\n",
       "      <td>Aberdeen Regional Airport</td>\n",
       "      <td>Aberdeen</td>\n",
       "      <td>SD</td>\n",
       "      <td>45.44906</td>\n",
       "      <td>-98.42183</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>ABY</td>\n",
       "      <td>Southwest Georgia Regional Airport</td>\n",
       "      <td>Albany</td>\n",
       "      <td>GA</td>\n",
       "      <td>31.53552</td>\n",
       "      <td>-84.19447</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  IATA_CODE                              AIRPORT         CITY STATE  LATITUDE  \\\n",
       "0       ABE  Lehigh Valley International Airport    Allentown    PA  40.65236   \n",
       "1       ABI             Abilene Regional Airport      Abilene    TX  32.41132   \n",
       "2       ABQ    Albuquerque International Sunport  Albuquerque    NM  35.04022   \n",
       "3       ABR            Aberdeen Regional Airport     Aberdeen    SD  45.44906   \n",
       "4       ABY   Southwest Georgia Regional Airport       Albany    GA  31.53552   \n",
       "\n",
       "   LONGITUDE  \n",
       "0  -75.44040  \n",
       "1  -99.68190  \n",
       "2 -106.60919  \n",
       "3  -98.42183  \n",
       "4  -84.19447  "
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "columns = ['IATA_CODE', 'AIRPORT', 'CITY', 'STATE', 'LATITUDE', 'LONGITUDE']\n",
    "airports_df = pd.read_csv(\"https://objectstorage.us-ashburn-1.oraclecloud.com/p/hh2NOgFJbVSg4amcLM3G3hkTuHyBD-8aE_iCsuZKEvIav1Wlld-3zfCawG4ycQGN/n/ociodscdev/b/oci-feature-store/o/beta/data/flights/airports.csv\")[columns]\n",
    "airports_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "7cdd3bb7",
   "metadata": {
    "pycharm": {
     "is_executing": true
    }
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>IATA_CODE</th>\n",
       "      <th>AIRLINE</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>UA</td>\n",
       "      <td>United Air Lines Inc.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>AA</td>\n",
       "      <td>American Airlines Inc.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>US</td>\n",
       "      <td>US Airways Inc.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>F9</td>\n",
       "      <td>Frontier Airlines Inc.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>B6</td>\n",
       "      <td>JetBlue Airways</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  IATA_CODE                 AIRLINE\n",
       "0        UA   United Air Lines Inc.\n",
       "1        AA  American Airlines Inc.\n",
       "2        US         US Airways Inc.\n",
       "3        F9  Frontier Airlines Inc.\n",
       "4        B6         JetBlue Airways"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "airlines_df = pd.read_csv(\"https://objectstorage.us-ashburn-1.oraclecloud.com/p/hh2NOgFJbVSg4amcLM3G3hkTuHyBD-8aE_iCsuZKEvIav1Wlld-3zfCawG4ycQGN/n/ociodscdev/b/oci-feature-store/o/beta/data/flights/airlines.csv\")\n",
    "airlines_df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "61117abb",
   "metadata": {},
   "source": [
    "<a id=\"feature_store\"></a>\n",
    "### 3.2. Create feature store logical entities"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "23f9df76",
   "metadata": {},
   "source": [
    "#### 3.2.1. Feature Store\n",
    "Feature store is the top level entity for feature store service"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "15c0d4e5",
   "metadata": {
    "pycharm": {
     "is_executing": true
    }
   },
   "outputs": [],
   "source": [
    "feature_store_resource = (\n",
    "    FeatureStore().\n",
    "    with_description(\"Data consisting of flights\").\n",
    "    with_compartment_id(compartment_id).\n",
    "    with_display_name(\"flights details\").\n",
    "    with_offline_config(metastore_id=metastore_id)\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6196b38b",
   "metadata": {},
   "source": [
    "<a id=\"create_feature_store\"></a>\n",
    "##### Create Feature Store\n",
    "\n",
    "Call the ```.create()``` method of the Feature store instance to create a feature store."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "a3df58e0",
   "metadata": {
    "pycharm": {
     "is_executing": true
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\n",
       "kind: featurestore\n",
       "spec:\n",
       "  compartmentId: ocid1.tenancy.oc1..aaaaaaaa462hfhplpx652b32ix62xrdijppq2c7okwcqjlgrbknhgtj2kofa\n",
       "  description: Data consisting of flights\n",
       "  displayName: flights details\n",
       "  id: 147791C6CF90DDEC594A53655BA8EDA6\n",
       "  offlineConfig:\n",
       "    metastoreId: ocid1.datacatalogmetastore.oc1.iad.amaaaaaabiudgxya2ipeqjr2m7npnn3kboq4s27erl3ts56wggl6ls6gpn3q\n",
       "type: featureStore"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "feature_store = feature_store_resource.create()\n",
    "feature_store"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "23946fff",
   "metadata": {},
   "source": [
    "#### 3.2.2. Entity\n",
    "An entity is a group of semantically related features."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "13cd6c27",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\n",
       "kind: entity\n",
       "spec:\n",
       "  compartmentId: ocid1.tenancy.oc1..aaaaaaaa462hfhplpx652b32ix62xrdijppq2c7okwcqjlgrbknhgtj2kofa\n",
       "  description: description for flight details\n",
       "  featureStoreId: 147791C6CF90DDEC594A53655BA8EDA6\n",
       "  id: 32E8CCEC72BD73F665B0B6180CB2100A\n",
       "  name: Flight details schema evolution/enforcement\n",
       "type: entity"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "entity = feature_store.create_entity(\n",
    "    display_name=\"Flight details schema evolution/enforcement\",\n",
    "    description=\"description for flight details\"\n",
    ")\n",
    "entity"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "21f81071",
   "metadata": {},
   "source": [
    "<a id=\"create_feature_group_airport\"></a>\n",
    "#### 3.2.3. Feature Group\n",
    "\n",
    "Create feature group for airport"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "80b44f1d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{\"expectation_type\": \"expect_column_values_to_be_between\", \"kwargs\": {\"column\": \"LONGITUDE\", \"min_value\": -1.0, \"max_value\": 1.0}, \"meta\": {}}"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from great_expectations.core import ExpectationSuite, ExpectationConfiguration\n",
    "\n",
    "expectation_suite_airports = ExpectationSuite(\n",
    "    expectation_suite_name=\"test_airports_df\"\n",
    ")\n",
    "expectation_suite_airports.add_expectation(\n",
    "    ExpectationConfiguration(\n",
    "        expectation_type=\"expect_column_values_to_not_be_null\",\n",
    "        kwargs={\"column\": \"IATA_CODE\"},\n",
    "    )\n",
    ")\n",
    "expectation_suite_airports.add_expectation(\n",
    "    ExpectationConfiguration(\n",
    "        expectation_type=\"expect_column_values_to_be_between\",\n",
    "        kwargs={\"column\": \"LATITUDE\", \"min_value\": -1.0, \"max_value\": 1.0},\n",
    "    )\n",
    ")\n",
    "\n",
    "expectation_suite_airports.add_expectation(\n",
    "    ExpectationConfiguration(\n",
    "        expectation_type=\"expect_column_values_to_be_between\",\n",
    "        kwargs={\"column\": \"LONGITUDE\", \"min_value\": -1.0, \"max_value\": 1.0},\n",
    "    )\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "eded396d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:ads.common.oci_client:OCI SDK with feature store support is not installed\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting default log level to \"WARN\".\n",
      "To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).\n",
      "2023/12/11 07:54:41 NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:py.warnings:/home/datascience/conda/fspyspark32_p38_cpu_v2/lib/python3.8/site-packages/pyspark/sql/pandas/utils.py:35: DeprecationWarning: distutils Version classes are deprecated. Use packaging.version instead.\n",
      "  if LooseVersion(pandas.__version__) < LooseVersion(minimum_pandas_version):\n",
      "\n",
      "WARNING:py.warnings:/home/datascience/conda/fspyspark32_p38_cpu_v2/lib/python3.8/site-packages/pyspark/sql/pandas/utils.py:57: DeprecationWarning: distutils Version classes are deprecated. Use packaging.version instead.\n",
      "  if LooseVersion(pyarrow.__version__) < LooseVersion(minimum_pyarrow_version):\n",
      "\n",
      "WARNING:py.warnings:/home/datascience/conda/fspyspark32_p38_cpu_v2/lib/python3.8/site-packages/pyspark/sql/pandas/conversion.py:471: FutureWarning: iteritems is deprecated and will be removed in a future version. Use .items instead.\n",
      "  arrow_data = [[(c, t) for (_, c), t in zip(pdf_slice.iteritems(), arrow_types)]\n",
      "\n"
     ]
    }
   ],
   "source": [
    "feature_group_airports = (\n",
    "    FeatureGroup()\n",
    "    .with_feature_store_id(feature_store.id)\n",
    "    .with_primary_keys([\"IATA_CODE\"])\n",
    "    .with_name(\"airport_feature_group\")\n",
    "    .with_entity_id(entity.id)\n",
    "    .with_compartment_id(compartment_id)\n",
    "    .with_schema_details_from_dataframe(airports_df)\n",
    "    .with_expectation_suite(\n",
    "        expectation_suite=expectation_suite_airports,\n",
    "        expectation_type=ExpectationType.LENIENT,\n",
    "     )\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "99eb917e",
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\n",
       "kind: FeatureGroup\n",
       "spec:\n",
       "  compartmentId: ocid1.tenancy.oc1..aaaaaaaa462hfhplpx652b32ix62xrdijppq2c7okwcqjlgrbknhgtj2kofa\n",
       "  entityId: 32E8CCEC72BD73F665B0B6180CB2100A\n",
       "  expectationDetails:\n",
       "    createRuleDetails:\n",
       "    - arguments:\n",
       "        column: IATA_CODE\n",
       "      levelType: ERROR\n",
       "      name: Rule-0\n",
       "      ruleType: expect_column_values_to_not_be_null\n",
       "    - arguments:\n",
       "        column: LATITUDE\n",
       "        max_value: 1.0\n",
       "        min_value: -1.0\n",
       "      levelType: ERROR\n",
       "      name: Rule-1\n",
       "      ruleType: expect_column_values_to_be_between\n",
       "    - arguments:\n",
       "        column: LONGITUDE\n",
       "        max_value: 1.0\n",
       "        min_value: -1.0\n",
       "      levelType: ERROR\n",
       "      name: Rule-2\n",
       "      ruleType: expect_column_values_to_be_between\n",
       "    expectationType: LENIENT\n",
       "    name: test_airports_df\n",
       "    validationEngineType: GREAT_EXPECTATIONS\n",
       "  featureStoreId: 147791C6CF90DDEC594A53655BA8EDA6\n",
       "  id: ED84DFFD7CF86AAA612CAFD01DA9EBAA\n",
       "  inputFeatureDetails:\n",
       "  - featureType: STRING\n",
       "    name: IATA_CODE\n",
       "    orderNumber: 1\n",
       "  - featureType: STRING\n",
       "    name: AIRPORT\n",
       "    orderNumber: 2\n",
       "  - featureType: STRING\n",
       "    name: CITY\n",
       "    orderNumber: 3\n",
       "  - featureType: STRING\n",
       "    name: STATE\n",
       "    orderNumber: 4\n",
       "  - featureType: DOUBLE\n",
       "    name: LATITUDE\n",
       "    orderNumber: 5\n",
       "  - featureType: DOUBLE\n",
       "    name: LONGITUDE\n",
       "    orderNumber: 6\n",
       "  isInferSchema: true\n",
       "  name: airport_feature_group\n",
       "  primaryKeys:\n",
       "    items:\n",
       "    - name: IATA_CODE\n",
       "  statisticsConfig:\n",
       "    isEnabled: true\n",
       "type: featureGroup"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "feature_group_airports.create()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "6ff882dc",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/svg+xml": [
       "<?xml version=\"1.0\" encoding=\"UTF-8\" standalone=\"no\"?>\n",
       "<!DOCTYPE svg PUBLIC \"-//W3C//DTD SVG 1.1//EN\"\n",
       " \"http://www.w3.org/Graphics/SVG/1.1/DTD/svg11.dtd\">\n",
       "<!-- Generated by graphviz version 2.50.0 (0)\n",
       " -->\n",
       "<!-- Pages: 1 -->\n",
       "<svg width=\"640pt\" height=\"64pt\"\n",
       " viewBox=\"0.00 0.00 640.00 64.00\" xmlns=\"http://www.w3.org/2000/svg\" xmlns:xlink=\"http://www.w3.org/1999/xlink\">\n",
       "<g id=\"graph0\" class=\"graph\" transform=\"scale(1 1) rotate(0) translate(4 60)\">\n",
       "<polygon fill=\"white\" stroke=\"transparent\" points=\"-4,4 -4,-60 636,-60 636,4 -4,4\"/>\n",
       "<!-- 147791C6CF90DDEC594A53655BA8EDA6 -->\n",
       "<g id=\"node1\" class=\"node\">\n",
       "<title>147791C6CF90DDEC594A53655BA8EDA6</title>\n",
       "<path fill=\"#dedede\" stroke=\"#dedede\" d=\"M133,-56C133,-56 12,-56 12,-56 6,-56 0,-50 0,-44 0,-44 0,-12 0,-12 0,-6 6,0 12,0 12,0 133,0 133,0 139,0 145,-6 145,-12 145,-12 145,-44 145,-44 145,-50 139,-56 133,-56\"/>\n",
       "<text text-anchor=\"start\" x=\"35.5\" y=\"-39.2\" font-family=\"Helvetica,Arial,sans-serif\" font-size=\"11.00\">flights details</text>\n",
       "<text text-anchor=\"start\" x=\"48\" y=\"-24.4\" font-family=\"Helvetica,Arial,sans-serif\" font-size=\"7.00\">Feature Store</text>\n",
       "<text text-anchor=\"start\" x=\"12.5\" y=\"-10.4\" font-family=\"Courier New\" font-size=\"7.00\">147791C6CF90DDEC594A53655BA8EDA6</text>\n",
       "</g>\n",
       "<!-- 32E8CCEC72BD73F665B0B6180CB2100A -->\n",
       "<g id=\"node2\" class=\"node\">\n",
       "<title>32E8CCEC72BD73F665B0B6180CB2100A</title>\n",
       "<path fill=\"#dedede\" stroke=\"#dedede\" d=\"M439,-56C439,-56 193,-56 193,-56 187,-56 181,-50 181,-44 181,-44 181,-12 181,-12 181,-6 187,0 193,0 193,0 439,0 439,0 445,0 451,-6 451,-12 451,-12 451,-44 451,-44 451,-50 445,-56 439,-56\"/>\n",
       "<text text-anchor=\"start\" x=\"193\" y=\"-39.2\" font-family=\"Helvetica,Arial,sans-serif\" font-size=\"11.00\">Flight details schema evolution/enforcement</text>\n",
       "<text text-anchor=\"start\" x=\"305\" y=\"-24.4\" font-family=\"Helvetica,Arial,sans-serif\" font-size=\"7.00\">Entity</text>\n",
       "<text text-anchor=\"start\" x=\"255.5\" y=\"-10.4\" font-family=\"Courier New\" font-size=\"7.00\">32E8CCEC72BD73F665B0B6180CB2100A</text>\n",
       "</g>\n",
       "<!-- 147791C6CF90DDEC594A53655BA8EDA6&#45;&gt;32E8CCEC72BD73F665B0B6180CB2100A -->\n",
       "<g id=\"edge1\" class=\"edge\">\n",
       "<title>147791C6CF90DDEC594A53655BA8EDA6&#45;&gt;32E8CCEC72BD73F665B0B6180CB2100A</title>\n",
       "<path fill=\"none\" stroke=\"black\" d=\"M145.3,-28C153.4,-28 161.85,-28 170.46,-28\"/>\n",
       "<polygon fill=\"black\" stroke=\"black\" points=\"170.64,-31.5 180.64,-28 170.64,-24.5 170.64,-31.5\"/>\n",
       "</g>\n",
       "<!-- ED84DFFD7CF86AAA612CAFD01DA9EBAA -->\n",
       "<g id=\"node3\" class=\"node\">\n",
       "<title>ED84DFFD7CF86AAA612CAFD01DA9EBAA</title>\n",
       "<path fill=\"#dedede\" stroke=\"#dedede\" d=\"M620,-56C620,-56 499,-56 499,-56 493,-56 487,-50 487,-44 487,-44 487,-12 487,-12 487,-6 493,0 499,0 499,0 620,0 620,0 626,0 632,-6 632,-12 632,-12 632,-44 632,-44 632,-50 626,-56 620,-56\"/>\n",
       "<text text-anchor=\"start\" x=\"500.5\" y=\"-39.2\" font-family=\"Helvetica,Arial,sans-serif\" font-size=\"11.00\">airport_feature_group</text>\n",
       "<text text-anchor=\"start\" x=\"534\" y=\"-24.4\" font-family=\"Helvetica,Arial,sans-serif\" font-size=\"7.00\">Feature Group</text>\n",
       "<text text-anchor=\"start\" x=\"499.5\" y=\"-10.4\" font-family=\"Courier New\" font-size=\"7.00\">ED84DFFD7CF86AAA612CAFD01DA9EBAA</text>\n",
       "</g>\n",
       "<!-- 32E8CCEC72BD73F665B0B6180CB2100A&#45;&gt;ED84DFFD7CF86AAA612CAFD01DA9EBAA -->\n",
       "<g id=\"edge2\" class=\"edge\">\n",
       "<title>32E8CCEC72BD73F665B0B6180CB2100A&#45;&gt;ED84DFFD7CF86AAA612CAFD01DA9EBAA</title>\n",
       "<path fill=\"none\" stroke=\"black\" d=\"M451.17,-28C459.89,-28 468.52,-28 476.86,-28\"/>\n",
       "<polygon fill=\"black\" stroke=\"black\" points=\"476.9,-31.5 486.9,-28 476.9,-24.5 476.9,-31.5\"/>\n",
       "</g>\n",
       "</g>\n",
       "</svg>\n"
      ],
      "text/plain": [
       "<graphviz.dot.Digraph at 0x7f126b154dc0>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "feature_group_airports.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "dd0c9208",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Hive Session ID = d2e52dd8-8573-42c6-bd04-d83dcd66bc48\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:great_expectations.validator.validator:\t3 expectation(s) included in expectation_suite.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "75a8a8012c0e41eab8c3305f45ccd02d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Calculating Metrics:   0%|          | 0/16 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:ads.feature_store.common.utils.utility:Validation Summary \n",
      "╒════════════════════╤══════════════════════════╤═══════════════════════════╤═════════════════════════════╤═══════════════════╤═══════════════════════╕\n",
      "│  expectation_type  │  evaluated_expectations  │  successful_expectations  │  unsuccessful_expectations  │  success_percent  │   ingestion_status    │\n",
      "╞════════════════════╪══════════════════════════╪═══════════════════════════╪═════════════════════════════╪═══════════════════╪═══════════════════════╡\n",
      "│      LENIENT       │            3             │             1             │              2              │      33.3333      │ Ingestion in progress │\n",
      "╘════════════════════╧══════════════════════════╧═══════════════════════════╧═════════════════════════════╧═══════════════════╧═══════════════════════╛\n",
      "INFO:ads.feature_store.common.utils.utility:Validations Rules Summary \n",
      "╒═════════════════════════════════════╤══════════════════════════════════════════════════════════════╤══════════╕\n",
      "│              rule_type              │                          arguments                           │  status  │\n",
      "╞═════════════════════════════════════╪══════════════════════════════════════════════════════════════╪══════════╡\n",
      "│ expect_column_values_to_not_be_null │                   {'column': 'IATA_CODE'}                    │   True   │\n",
      "├─────────────────────────────────────┼──────────────────────────────────────────────────────────────┼──────────┤\n",
      "│ expect_column_values_to_be_between  │ {'column': 'LATITUDE', 'min_value': -1.0, 'max_value': 1.0}  │  False   │\n",
      "├─────────────────────────────────────┼──────────────────────────────────────────────────────────────┼──────────┤\n",
      "│ expect_column_values_to_be_between  │ {'column': 'LONGITUDE', 'min_value': -1.0, 'max_value': 1.0} │  False   │\n",
      "╘═════════════════════════════════════╧══════════════════════════════════════════════════════════════╧══════════╛\n",
      "WARNING:py.warnings:/home/datascience/conda/fspyspark32_p38_cpu_v2/lib/python3.8/site-packages/pyspark/sql/pandas/utils.py:35: DeprecationWarning: distutils Version classes are deprecated. Use packaging.version instead.\n",
      "  if LooseVersion(pandas.__version__) < LooseVersion(minimum_pandas_version):\n",
      "\n",
      "WARNING:py.warnings:/home/datascience/conda/fspyspark32_p38_cpu_v2/lib/python3.8/site-packages/pyspark/sql/pandas/utils.py:57: DeprecationWarning: distutils Version classes are deprecated. Use packaging.version instead.\n",
      "  if LooseVersion(pyarrow.__version__) < LooseVersion(minimum_pyarrow_version):\n",
      "\n",
      "WARNING:py.warnings:/home/datascience/conda/fspyspark32_p38_cpu_v2/lib/python3.8/site-packages/pyspark/sql/pandas/conversion.py:471: FutureWarning: iteritems is deprecated and will be removed in a future version. Use .items instead.\n",
      "  arrow_data = [[(c, t) for (_, c), t in zip(pdf_slice.iteritems(), arrow_types)]\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 8:>                                                          (0 + 2) / 2]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:py.warnings:/home/datascience/conda/fspyspark32_p38_cpu_v2/lib/python3.8/site-packages/mlm_insights/core/metrics/drift_metrics/chi_square.py:239: DeprecationWarning: The 'warn' method is deprecated, use 'warning' instead\n",
      "  logger.warn(\"Reference profile is empty\")\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:ads.feature_store.common.utils.utility:Ingestion Summary \n",
      "╒══════════════════════════════════╤═══════════════╤════════════════════╤═════════════════╕\n",
      "│            entity_id             │  entity_type  │  ingestion_status  │  error_details  │\n",
      "╞══════════════════════════════════╪═══════════════╪════════════════════╪═════════════════╡\n",
      "│ ED84DFFD7CF86AAA612CAFD01DA9EBAA │ FEATURE_GROUP │     Succeeded      │      None       │\n",
      "╘══════════════════════════════════╧═══════════════╧════════════════════╧═════════════════╛\n"
     ]
    }
   ],
   "source": [
    "feature_group_airports.materialise(airports_df)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4fc8616f",
   "metadata": {},
   "source": [
    "<a id=\"schema_enforcement\"></a>\n",
    "### 3.3. Schema enforcement\n",
    "\n",
    "Schema enforcement, also known as schema validation, is a safeguard in Delta Lake that ensures data quality by rejecting writes to a table that do not match the table's schema. For example, a front desk manager at a busy restaurant that only accepts reservations, the schema enforcement checks to see whether each column in the data inserted into the table is in the list of expected columns. Meaning each one has a \"reservation\", and rejects any writes with columns that aren't on the list."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "a80b03b8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>IATA_CODE</th>\n",
       "      <th>AIRPORT</th>\n",
       "      <th>CITY</th>\n",
       "      <th>STATE</th>\n",
       "      <th>LATITUDE</th>\n",
       "      <th>LONGITUDE</th>\n",
       "      <th>COUNTRY</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>ABE</td>\n",
       "      <td>Lehigh Valley International Airport</td>\n",
       "      <td>Allentown</td>\n",
       "      <td>PA</td>\n",
       "      <td>40.65236</td>\n",
       "      <td>-75.44040</td>\n",
       "      <td>USA</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>ABI</td>\n",
       "      <td>Abilene Regional Airport</td>\n",
       "      <td>Abilene</td>\n",
       "      <td>TX</td>\n",
       "      <td>32.41132</td>\n",
       "      <td>-99.68190</td>\n",
       "      <td>USA</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>ABQ</td>\n",
       "      <td>Albuquerque International Sunport</td>\n",
       "      <td>Albuquerque</td>\n",
       "      <td>NM</td>\n",
       "      <td>35.04022</td>\n",
       "      <td>-106.60919</td>\n",
       "      <td>USA</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>ABR</td>\n",
       "      <td>Aberdeen Regional Airport</td>\n",
       "      <td>Aberdeen</td>\n",
       "      <td>SD</td>\n",
       "      <td>45.44906</td>\n",
       "      <td>-98.42183</td>\n",
       "      <td>USA</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>ABY</td>\n",
       "      <td>Southwest Georgia Regional Airport</td>\n",
       "      <td>Albany</td>\n",
       "      <td>GA</td>\n",
       "      <td>31.53552</td>\n",
       "      <td>-84.19447</td>\n",
       "      <td>USA</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  IATA_CODE                              AIRPORT         CITY STATE  LATITUDE  \\\n",
       "0       ABE  Lehigh Valley International Airport    Allentown    PA  40.65236   \n",
       "1       ABI             Abilene Regional Airport      Abilene    TX  32.41132   \n",
       "2       ABQ    Albuquerque International Sunport  Albuquerque    NM  35.04022   \n",
       "3       ABR            Aberdeen Regional Airport     Aberdeen    SD  45.44906   \n",
       "4       ABY   Southwest Georgia Regional Airport       Albany    GA  31.53552   \n",
       "\n",
       "   LONGITUDE COUNTRY  \n",
       "0  -75.44040     USA  \n",
       "1  -99.68190     USA  \n",
       "2 -106.60919     USA  \n",
       "3  -98.42183     USA  \n",
       "4  -84.19447     USA  "
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "columns = ['IATA_CODE', 'AIRPORT', 'CITY', 'STATE', 'LATITUDE', 'LONGITUDE', 'COUNTRY']\n",
    "airports_df = pd.read_csv(\"https://objectstorage.us-ashburn-1.oraclecloud.com/p/hh2NOgFJbVSg4amcLM3G3hkTuHyBD-8aE_iCsuZKEvIav1Wlld-3zfCawG4ycQGN/n/ociodscdev/b/oci-feature-store/o/beta/data/flights/airports.csv\")[columns]\n",
    "airports_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "bc55a626",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:py.warnings:/home/datascience/conda/fspyspark32_p38_cpu_v2/lib/python3.8/site-packages/pyspark/sql/pandas/utils.py:35: DeprecationWarning: distutils Version classes are deprecated. Use packaging.version instead.\n",
      "  if LooseVersion(pandas.__version__) < LooseVersion(minimum_pandas_version):\n",
      "\n",
      "WARNING:py.warnings:/home/datascience/conda/fspyspark32_p38_cpu_v2/lib/python3.8/site-packages/pyspark/sql/pandas/utils.py:57: DeprecationWarning: distutils Version classes are deprecated. Use packaging.version instead.\n",
      "  if LooseVersion(pyarrow.__version__) < LooseVersion(minimum_pyarrow_version):\n",
      "\n",
      "WARNING:py.warnings:/home/datascience/conda/fspyspark32_p38_cpu_v2/lib/python3.8/site-packages/pyspark/sql/pandas/conversion.py:471: FutureWarning: iteritems is deprecated and will be removed in a future version. Use .items instead.\n",
      "  arrow_data = [[(c, t) for (_, c), t in zip(pdf_slice.iteritems(), arrow_types)]\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "\n",
       "kind: FeatureGroup\n",
       "spec:\n",
       "  compartmentId: ocid1.tenancy.oc1..aaaaaaaa462hfhplpx652b32ix62xrdijppq2c7okwcqjlgrbknhgtj2kofa\n",
       "  entityId: 32E8CCEC72BD73F665B0B6180CB2100A\n",
       "  expectationDetails:\n",
       "    createRuleDetails:\n",
       "    - arguments:\n",
       "        column: IATA_CODE\n",
       "      levelType: ERROR\n",
       "      name: Rule-0\n",
       "      ruleType: expect_column_values_to_not_be_null\n",
       "    - arguments:\n",
       "        column: LATITUDE\n",
       "        max_value: 1.0\n",
       "        min_value: -1.0\n",
       "      levelType: ERROR\n",
       "      name: Rule-1\n",
       "      ruleType: expect_column_values_to_be_between\n",
       "    - arguments:\n",
       "        column: LONGITUDE\n",
       "        max_value: 1.0\n",
       "        min_value: -1.0\n",
       "      levelType: ERROR\n",
       "      name: Rule-2\n",
       "      ruleType: expect_column_values_to_be_between\n",
       "    expectationType: LENIENT\n",
       "    name: test_airports_df\n",
       "    validationEngineType: GREAT_EXPECTATIONS\n",
       "  featureStoreId: 147791C6CF90DDEC594A53655BA8EDA6\n",
       "  id: ED84DFFD7CF86AAA612CAFD01DA9EBAA\n",
       "  inputFeatureDetails:\n",
       "  - featureType: STRING\n",
       "    name: IATA_CODE\n",
       "    orderNumber: 1\n",
       "  - featureType: STRING\n",
       "    name: AIRPORT\n",
       "    orderNumber: 2\n",
       "  - featureType: STRING\n",
       "    name: CITY\n",
       "    orderNumber: 3\n",
       "  - featureType: STRING\n",
       "    name: STATE\n",
       "    orderNumber: 4\n",
       "  - featureType: DOUBLE\n",
       "    name: LATITUDE\n",
       "    orderNumber: 5\n",
       "  - featureType: DOUBLE\n",
       "    name: LONGITUDE\n",
       "    orderNumber: 6\n",
       "  - featureType: STRING\n",
       "    name: COUNTRY\n",
       "    orderNumber: 7\n",
       "  isInferSchema: true\n",
       "  jobId: beebdc9c-e84d-418e-bc00-1a391be99232\n",
       "  name: airport_feature_group\n",
       "  outputFeatureDetails:\n",
       "    items:\n",
       "    - featureGroupId: ED84DFFD7CF86AAA612CAFD01DA9EBAA\n",
       "      featureType: STRING\n",
       "      name: IATA_CODE\n",
       "    - featureGroupId: ED84DFFD7CF86AAA612CAFD01DA9EBAA\n",
       "      featureType: STRING\n",
       "      name: AIRPORT\n",
       "    - featureGroupId: ED84DFFD7CF86AAA612CAFD01DA9EBAA\n",
       "      featureType: STRING\n",
       "      name: CITY\n",
       "    - featureGroupId: ED84DFFD7CF86AAA612CAFD01DA9EBAA\n",
       "      featureType: STRING\n",
       "      name: STATE\n",
       "    - featureGroupId: ED84DFFD7CF86AAA612CAFD01DA9EBAA\n",
       "      featureType: DOUBLE\n",
       "      name: LATITUDE\n",
       "    - featureGroupId: ED84DFFD7CF86AAA612CAFD01DA9EBAA\n",
       "      featureType: DOUBLE\n",
       "      name: LONGITUDE\n",
       "  primaryKeys:\n",
       "    items:\n",
       "    - name: IATA_CODE\n",
       "  statisticsConfig:\n",
       "    isEnabled: true\n",
       "type: featureGroup"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "feature_group_airports.with_schema_details_from_dataframe(airports_df)\n",
    "feature_group_airports.update()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "e03b32fb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:great_expectations.validator.validator:\t3 expectation(s) included in expectation_suite.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "94236958f6154fcb9fe03d3e70c595e5",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Calculating Metrics:   0%|          | 0/16 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:ads.feature_store.common.utils.utility:Validation Summary \n",
      "╒════════════════════╤══════════════════════════╤═══════════════════════════╤═════════════════════════════╤═══════════════════╤═══════════════════════╕\n",
      "│  expectation_type  │  evaluated_expectations  │  successful_expectations  │  unsuccessful_expectations  │  success_percent  │   ingestion_status    │\n",
      "╞════════════════════╪══════════════════════════╪═══════════════════════════╪═════════════════════════════╪═══════════════════╪═══════════════════════╡\n",
      "│      LENIENT       │            3             │             1             │              2              │      33.3333      │ Ingestion in progress │\n",
      "╘════════════════════╧══════════════════════════╧═══════════════════════════╧═════════════════════════════╧═══════════════════╧═══════════════════════╛\n",
      "INFO:ads.feature_store.common.utils.utility:Validations Rules Summary \n",
      "╒═════════════════════════════════════╤══════════════════════════════════════════════════════════════╤══════════╕\n",
      "│              rule_type              │                          arguments                           │  status  │\n",
      "╞═════════════════════════════════════╪══════════════════════════════════════════════════════════════╪══════════╡\n",
      "│ expect_column_values_to_not_be_null │                   {'column': 'IATA_CODE'}                    │   True   │\n",
      "├─────────────────────────────────────┼──────────────────────────────────────────────────────────────┼──────────┤\n",
      "│ expect_column_values_to_be_between  │ {'column': 'LATITUDE', 'min_value': -1.0, 'max_value': 1.0}  │  False   │\n",
      "├─────────────────────────────────────┼──────────────────────────────────────────────────────────────┼──────────┤\n",
      "│ expect_column_values_to_be_between  │ {'column': 'LONGITUDE', 'min_value': -1.0, 'max_value': 1.0} │  False   │\n",
      "╘═════════════════════════════════════╧══════════════════════════════════════════════════════════════╧══════════╛\n",
      "ERROR:ads.feature_store.execution_strategy.spark.spark_execution:FeatureGroup Materialization Failed with : <class 'pyspark.sql.utils.AnalysisException'> with error message: A schema mismatch detected when writing to the Delta table (Table ID: ff186a2c-0145-4b08-b886-284d291daef2).\n",
      "To enable schema migration using DataFrameWriter or DataStreamWriter, please set:\n",
      "'.option(\"mergeSchema\", \"true\")'.\n",
      "For other operations, set the session configuration\n",
      "spark.databricks.delta.schema.autoMerge.enabled to \"true\". See the documentation\n",
      "specific to the operation for details.\n",
      "\n",
      "Table schema:\n",
      "root\n",
      "-- IATA_CODE: string (nullable = true)\n",
      "-- AIRPORT: string (nullable = true)\n",
      "-- CITY: string (nullable = true)\n",
      "-- STATE: string (nullable = true)\n",
      "-- LATITUDE: double (nullable = true)\n",
      "-- LONGITUDE: double (nullable = true)\n",
      "\n",
      "\n",
      "Data schema:\n",
      "root\n",
      "-- IATA_CODE: string (nullable = true)\n",
      "-- AIRPORT: string (nullable = true)\n",
      "-- CITY: string (nullable = true)\n",
      "-- STATE: string (nullable = true)\n",
      "-- LATITUDE: double (nullable = true)\n",
      "-- LONGITUDE: double (nullable = true)\n",
      "-- COUNTRY: string (nullable = true)\n",
      "\n",
      "         \n",
      "To overwrite your schema or change partitioning, please set:\n",
      "'.option(\"overwriteSchema\", \"true\")'.\n",
      "\n",
      "Note that the schema can't be overwritten when using\n",
      "'replaceWhere'.\n",
      "          and stacktrace Traceback (most recent call last):\n",
      "  File \"/home/datascience/conda/fspyspark32_p38_cpu_v2/lib/python3.8/site-packages/ads/feature_store/execution_strategy/spark/spark_execution.py\", line 298, in _save_offline_dataframe\n",
      "    self.delta_lake_service.write_dataframe_to_delta_lake(\n",
      "  File \"/home/datascience/conda/fspyspark32_p38_cpu_v2/lib/python3.8/site-packages/ads/feature_store/execution_strategy/delta_lake/delta_lake_service.py\", line 113, in write_dataframe_to_delta_lake\n",
      "    self.save_delta_dataframe(\n",
      "  File \"/home/datascience/conda/fspyspark32_p38_cpu_v2/lib/python3.8/site-packages/ads/feature_store/execution_strategy/delta_lake/delta_lake_service.py\", line 245, in save_delta_dataframe\n",
      "    dataframe.write.format(\"delta\").mode(dataframe_ingestion_mode).partitionBy(\n",
      "  File \"/home/datascience/conda/fspyspark32_p38_cpu_v2/lib/python3.8/site-packages/pyspark/sql/readwriter.py\", line 806, in saveAsTable\n",
      "    self._jwrite.saveAsTable(name)\n",
      "  File \"/home/datascience/conda/fspyspark32_p38_cpu_v2/lib/python3.8/site-packages/py4j/java_gateway.py\", line 1321, in __call__\n",
      "    return_value = get_return_value(\n",
      "  File \"/home/datascience/conda/fspyspark32_p38_cpu_v2/lib/python3.8/site-packages/pyspark/sql/utils.py\", line 117, in deco\n",
      "    raise converted from None\n",
      "pyspark.sql.utils.AnalysisException: A schema mismatch detected when writing to the Delta table (Table ID: ff186a2c-0145-4b08-b886-284d291daef2).\n",
      "To enable schema migration using DataFrameWriter or DataStreamWriter, please set:\n",
      "'.option(\"mergeSchema\", \"true\")'.\n",
      "For other operations, set the session configuration\n",
      "spark.databricks.delta.schema.autoMerge.enabled to \"true\". See the documentation\n",
      "specific to the operation for details.\n",
      "\n",
      "Table schema:\n",
      "root\n",
      "-- IATA_CODE: string (nullable = true)\n",
      "-- AIRPORT: string (nullable = true)\n",
      "-- CITY: string (nullable = true)\n",
      "-- STATE: string (nullable = true)\n",
      "-- LATITUDE: double (nullable = true)\n",
      "-- LONGITUDE: double (nullable = true)\n",
      "\n",
      "\n",
      "Data schema:\n",
      "root\n",
      "-- IATA_CODE: string (nullable = true)\n",
      "-- AIRPORT: string (nullable = true)\n",
      "-- CITY: string (nullable = true)\n",
      "-- STATE: string (nullable = true)\n",
      "-- LATITUDE: double (nullable = true)\n",
      "-- LONGITUDE: double (nullable = true)\n",
      "-- COUNTRY: string (nullable = true)\n",
      "\n",
      "         \n",
      "To overwrite your schema or change partitioning, please set:\n",
      "'.option(\"overwriteSchema\", \"true\")'.\n",
      "\n",
      "Note that the schema can't be overwritten when using\n",
      "'replaceWhere'.\n",
      "         \n",
      "\n",
      "INFO:ads.feature_store.common.utils.utility:Ingestion Summary \n",
      "╒══════════════════════════════════╤═══════════════╤════════════════════╤══════════════════════════════════════════════════════════════════════════════════════════════════════════════╕\n",
      "│            entity_id             │  entity_type  │  ingestion_status  │                                                error_details                                                 │\n",
      "╞══════════════════════════════════╪═══════════════╪════════════════════╪══════════════════════════════════════════════════════════════════════════════════════════════════════════════╡\n",
      "│ ED84DFFD7CF86AAA612CAFD01DA9EBAA │ FEATURE_GROUP │       Failed       │ A schema mismatch detected when writing to the Delta table (Table ID: ff186a2c-0145-4b08-b886-284d291daef2). │\n",
      "│                                  │               │                    │              To enable schema migration using DataFrameWriter or DataStreamWriter, please set:               │\n",
      "│                                  │               │                    │                                      '.option(\"mergeSchema\", \"true\")'.                                       │\n",
      "│                                  │               │                    │                             For other operations, set the session configuration                              │\n",
      "│                                  │               │                    │               spark.databricks.delta.schema.autoMerge.enabled to \"true\". See the documentation               │\n",
      "│                                  │               │                    │                                    specific to the operation for details.                                    │\n",
      "│                                  │               │                    │                                                                                                              │\n",
      "│                                  │               │                    │                                                Table schema:                                                 │\n",
      "│                                  │               │                    │                                                     root                                                     │\n",
      "│                                  │               │                    │                                    -- IATA_CODE: string (nullable = true)                                    │\n",
      "│                                  │               │                    │                                     -- AIRPORT: string (nullable = true)                                     │\n",
      "│                                  │               │                    │                                      -- CITY: string (nullable = true)                                       │\n",
      "│                                  │               │                    │                                      -- STATE: string (nullable = true)                                      │\n",
      "│                                  │               │                    │                                    -- LATITUDE: double (nullable = true)                                     │\n",
      "│                                  │               │                    │                                    -- LONGITUDE: double (nullable = true)                                    │\n",
      "│                                  │               │                    │                                                                                                              │\n",
      "│                                  │               │                    │                                                                                                              │\n",
      "│                                  │               │                    │                                                 Data schema:                                                 │\n",
      "│                                  │               │                    │                                                     root                                                     │\n",
      "│                                  │               │                    │                                    -- IATA_CODE: string (nullable = true)                                    │\n",
      "│                                  │               │                    │                                     -- AIRPORT: string (nullable = true)                                     │\n",
      "│                                  │               │                    │                                      -- CITY: string (nullable = true)                                       │\n",
      "│                                  │               │                    │                                      -- STATE: string (nullable = true)                                      │\n",
      "│                                  │               │                    │                                    -- LATITUDE: double (nullable = true)                                     │\n",
      "│                                  │               │                    │                                    -- LONGITUDE: double (nullable = true)                                    │\n",
      "│                                  │               │                    │                                     -- COUNTRY: string (nullable = true)                                     │\n",
      "│                                  │               │                    │                                                                                                              │\n",
      "│                                  │               │                    │                                                                                                              │\n",
      "│                                  │               │                    │                         To overwrite your schema or change partitioning, please set:                         │\n",
      "│                                  │               │                    │                                    '.option(\"overwriteSchema\", \"true\")'.                                     │\n",
      "│                                  │               │                    │                                                                                                              │\n",
      "│                                  │               │                    │                             Note that the schema can't be overwritten when using                             │\n",
      "│                                  │               │                    │                                               'replaceWhere'.                                                │\n",
      "╘══════════════════════════════════╧═══════════════╧════════════════════╧══════════════════════════════════════════════════════════════════════════════════════════════════════════════╛\n"
     ]
    }
   ],
   "source": [
    "feature_group_airports.materialise(airports_df)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5e478772",
   "metadata": {},
   "source": [
    "<a id=\"schema_evolution\"></a>\n",
    "### 3.4. Schema evolution\n",
    "\n",
    "Schema evolution allows you to change a table's current schema to accommodate data that is changing over time. Typically, it's used when performing an append or overwrite operation to automatically adapt the schema to include one or more new columns."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "f92edb62",
   "metadata": {},
   "outputs": [],
   "source": [
    "from ads.feature_store.feature_option_details import FeatureOptionDetails\n",
    "feature_option_details = FeatureOptionDetails().with_feature_option_write_config_details(merge_schema=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "8cb7e213",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:great_expectations.validator.validator:\t3 expectation(s) included in expectation_suite.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5446079dfa324f7ab2a964f560840588",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Calculating Metrics:   0%|          | 0/16 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:ads.feature_store.common.utils.utility:Validation Summary \n",
      "╒════════════════════╤══════════════════════════╤═══════════════════════════╤═════════════════════════════╤═══════════════════╤═══════════════════════╕\n",
      "│  expectation_type  │  evaluated_expectations  │  successful_expectations  │  unsuccessful_expectations  │  success_percent  │   ingestion_status    │\n",
      "╞════════════════════╪══════════════════════════╪═══════════════════════════╪═════════════════════════════╪═══════════════════╪═══════════════════════╡\n",
      "│      LENIENT       │            3             │             1             │              2              │      33.3333      │ Ingestion in progress │\n",
      "╘════════════════════╧══════════════════════════╧═══════════════════════════╧═════════════════════════════╧═══════════════════╧═══════════════════════╛\n",
      "INFO:ads.feature_store.common.utils.utility:Validations Rules Summary \n",
      "╒═════════════════════════════════════╤══════════════════════════════════════════════════════════════╤══════════╕\n",
      "│              rule_type              │                          arguments                           │  status  │\n",
      "╞═════════════════════════════════════╪══════════════════════════════════════════════════════════════╪══════════╡\n",
      "│ expect_column_values_to_not_be_null │                   {'column': 'IATA_CODE'}                    │   True   │\n",
      "├─────────────────────────────────────┼──────────────────────────────────────────────────────────────┼──────────┤\n",
      "│ expect_column_values_to_be_between  │ {'column': 'LATITUDE', 'min_value': -1.0, 'max_value': 1.0}  │  False   │\n",
      "├─────────────────────────────────────┼──────────────────────────────────────────────────────────────┼──────────┤\n",
      "│ expect_column_values_to_be_between  │ {'column': 'LONGITUDE', 'min_value': -1.0, 'max_value': 1.0} │  False   │\n",
      "╘═════════════════════════════════════╧══════════════════════════════════════════════════════════════╧══════════╛\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:py.warnings:/home/datascience/conda/fspyspark32_p38_cpu_v2/lib/python3.8/site-packages/mlm_insights/core/metrics/drift_metrics/chi_square.py:239: DeprecationWarning: The 'warn' method is deprecated, use 'warning' instead\n",
      "  logger.warn(\"Reference profile is empty\")\n",
      "\n",
      "INFO:ads.feature_store.common.utils.utility:Ingestion Summary \n",
      "╒══════════════════════════════════╤═══════════════╤════════════════════╤═════════════════╕\n",
      "│            entity_id             │  entity_type  │  ingestion_status  │  error_details  │\n",
      "╞══════════════════════════════════╪═══════════════╪════════════════════╪═════════════════╡\n",
      "│ ED84DFFD7CF86AAA612CAFD01DA9EBAA │ FEATURE_GROUP │     Succeeded      │      None       │\n",
      "╘══════════════════════════════════╧═══════════════╧════════════════════╧═════════════════╛\n"
     ]
    }
   ],
   "source": [
    "feature_group_airports.materialise(\n",
    "    input_dataframe=airports_df,\n",
    "    feature_option_details=feature_option_details\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "94cbc497",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\n",
       "kind: FeatureGroup\n",
       "spec:\n",
       "  compartmentId: ocid1.tenancy.oc1..aaaaaaaa462hfhplpx652b32ix62xrdijppq2c7okwcqjlgrbknhgtj2kofa\n",
       "  entityId: 32E8CCEC72BD73F665B0B6180CB2100A\n",
       "  expectationDetails:\n",
       "    createRuleDetails:\n",
       "    - arguments:\n",
       "        column: IATA_CODE\n",
       "      levelType: ERROR\n",
       "      name: Rule-0\n",
       "      ruleType: expect_column_values_to_not_be_null\n",
       "    - arguments:\n",
       "        column: LATITUDE\n",
       "        max_value: 1.0\n",
       "        min_value: -1.0\n",
       "      levelType: ERROR\n",
       "      name: Rule-1\n",
       "      ruleType: expect_column_values_to_be_between\n",
       "    - arguments:\n",
       "        column: LONGITUDE\n",
       "        max_value: 1.0\n",
       "        min_value: -1.0\n",
       "      levelType: ERROR\n",
       "      name: Rule-2\n",
       "      ruleType: expect_column_values_to_be_between\n",
       "    expectationType: LENIENT\n",
       "    name: test_airports_df\n",
       "    validationEngineType: GREAT_EXPECTATIONS\n",
       "  featureStoreId: 147791C6CF90DDEC594A53655BA8EDA6\n",
       "  id: ED84DFFD7CF86AAA612CAFD01DA9EBAA\n",
       "  inputFeatureDetails:\n",
       "  - featureType: STRING\n",
       "    name: IATA_CODE\n",
       "    orderNumber: 1\n",
       "  - featureType: STRING\n",
       "    name: AIRPORT\n",
       "    orderNumber: 2\n",
       "  - featureType: STRING\n",
       "    name: CITY\n",
       "    orderNumber: 3\n",
       "  - featureType: STRING\n",
       "    name: STATE\n",
       "    orderNumber: 4\n",
       "  - featureType: DOUBLE\n",
       "    name: LATITUDE\n",
       "    orderNumber: 5\n",
       "  - featureType: DOUBLE\n",
       "    name: LONGITUDE\n",
       "    orderNumber: 6\n",
       "  - featureType: STRING\n",
       "    name: COUNTRY\n",
       "    orderNumber: 7\n",
       "  isInferSchema: true\n",
       "  jobId: 4135f7bc-0103-4c14-bc70-7b9279fa68a2\n",
       "  name: airport_feature_group\n",
       "  outputFeatureDetails:\n",
       "    items:\n",
       "    - featureGroupId: ED84DFFD7CF86AAA612CAFD01DA9EBAA\n",
       "      featureType: STRING\n",
       "      name: IATA_CODE\n",
       "    - featureGroupId: ED84DFFD7CF86AAA612CAFD01DA9EBAA\n",
       "      featureType: STRING\n",
       "      name: AIRPORT\n",
       "    - featureGroupId: ED84DFFD7CF86AAA612CAFD01DA9EBAA\n",
       "      featureType: STRING\n",
       "      name: CITY\n",
       "    - featureGroupId: ED84DFFD7CF86AAA612CAFD01DA9EBAA\n",
       "      featureType: STRING\n",
       "      name: STATE\n",
       "    - featureGroupId: ED84DFFD7CF86AAA612CAFD01DA9EBAA\n",
       "      featureType: DOUBLE\n",
       "      name: LATITUDE\n",
       "    - featureGroupId: ED84DFFD7CF86AAA612CAFD01DA9EBAA\n",
       "      featureType: DOUBLE\n",
       "      name: LONGITUDE\n",
       "    - featureGroupId: ED84DFFD7CF86AAA612CAFD01DA9EBAA\n",
       "      featureType: STRING\n",
       "      name: COUNTRY\n",
       "  primaryKeys:\n",
       "    items:\n",
       "    - name: IATA_CODE\n",
       "  statisticsConfig:\n",
       "    isEnabled: true\n",
       "type: featureGroup"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "feature_group_airports"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1680d10c",
   "metadata": {},
   "source": [
    "<a id=\"ingestion_modes\"></a>\n",
    "### 3.5. Ingestion modes"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "60f35679",
   "metadata": {},
   "source": [
    "<a id=\"append\"></a>\n",
    "#### 3.5.1. Append\n",
    "\n",
    "In ``append`` mode, new data is added to the existing table. If the table already exists, the new data is appended to it, extending the dataset. This mode is suitable for scenarios where you want to continuously add new records without modifying or deleting existing data. It preserves the existing data and only appends the new data to the end of the table."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "d6a1e629",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:great_expectations.validator.validator:\t3 expectation(s) included in expectation_suite.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "24d8414aac864bc5b53fe6c18200478d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Calculating Metrics:   0%|          | 0/16 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:ads.feature_store.common.utils.utility:Validation Summary \n",
      "╒════════════════════╤══════════════════════════╤═══════════════════════════╤═════════════════════════════╤═══════════════════╤═══════════════════════╕\n",
      "│  expectation_type  │  evaluated_expectations  │  successful_expectations  │  unsuccessful_expectations  │  success_percent  │   ingestion_status    │\n",
      "╞════════════════════╪══════════════════════════╪═══════════════════════════╪═════════════════════════════╪═══════════════════╪═══════════════════════╡\n",
      "│      LENIENT       │            3             │             1             │              2              │      33.3333      │ Ingestion in progress │\n",
      "╘════════════════════╧══════════════════════════╧═══════════════════════════╧═════════════════════════════╧═══════════════════╧═══════════════════════╛\n",
      "INFO:ads.feature_store.common.utils.utility:Validations Rules Summary \n",
      "╒═════════════════════════════════════╤══════════════════════════════════════════════════════════════╤══════════╕\n",
      "│              rule_type              │                          arguments                           │  status  │\n",
      "╞═════════════════════════════════════╪══════════════════════════════════════════════════════════════╪══════════╡\n",
      "│ expect_column_values_to_not_be_null │                   {'column': 'IATA_CODE'}                    │   True   │\n",
      "├─────────────────────────────────────┼──────────────────────────────────────────────────────────────┼──────────┤\n",
      "│ expect_column_values_to_be_between  │ {'column': 'LATITUDE', 'min_value': -1.0, 'max_value': 1.0}  │  False   │\n",
      "├─────────────────────────────────────┼──────────────────────────────────────────────────────────────┼──────────┤\n",
      "│ expect_column_values_to_be_between  │ {'column': 'LONGITUDE', 'min_value': -1.0, 'max_value': 1.0} │  False   │\n",
      "╘═════════════════════════════════════╧══════════════════════════════════════════════════════════════╧══════════╛\n",
      "WARNING:py.warnings:/home/datascience/conda/fspyspark32_p38_cpu_v2/lib/python3.8/site-packages/pyspark/sql/pandas/utils.py:35: DeprecationWarning: distutils Version classes are deprecated. Use packaging.version instead.\n",
      "  if LooseVersion(pandas.__version__) < LooseVersion(minimum_pandas_version):\n",
      "\n",
      "WARNING:py.warnings:/home/datascience/conda/fspyspark32_p38_cpu_v2/lib/python3.8/site-packages/pyspark/sql/pandas/utils.py:57: DeprecationWarning: distutils Version classes are deprecated. Use packaging.version instead.\n",
      "  if LooseVersion(pyarrow.__version__) < LooseVersion(minimum_pyarrow_version):\n",
      "\n",
      "WARNING:py.warnings:/home/datascience/conda/fspyspark32_p38_cpu_v2/lib/python3.8/site-packages/pyspark/sql/pandas/conversion.py:471: FutureWarning: iteritems is deprecated and will be removed in a future version. Use .items instead.\n",
      "  arrow_data = [[(c, t) for (_, c), t in zip(pdf_slice.iteritems(), arrow_types)]\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:py.warnings:/home/datascience/conda/fspyspark32_p38_cpu_v2/lib/python3.8/site-packages/mlm_insights/core/metrics/drift_metrics/chi_square.py:239: DeprecationWarning: The 'warn' method is deprecated, use 'warning' instead\n",
      "  logger.warn(\"Reference profile is empty\")\n",
      "\n",
      "INFO:ads.feature_store.common.utils.utility:Ingestion Summary \n",
      "╒══════════════════════════════════╤═══════════════╤════════════════════╤═════════════════╕\n",
      "│            entity_id             │  entity_type  │  ingestion_status  │  error_details  │\n",
      "╞══════════════════════════════════╪═══════════════╪════════════════════╪═════════════════╡\n",
      "│ ED84DFFD7CF86AAA612CAFD01DA9EBAA │ FEATURE_GROUP │     Succeeded      │      None       │\n",
      "╘══════════════════════════════════╧═══════════════╧════════════════════╧═════════════════╛\n"
     ]
    }
   ],
   "source": [
    "from ads.feature_store.feature_group_job import BatchIngestionMode\n",
    "feature_group_airports.materialise(airports_df, ingestion_mode=BatchIngestionMode.APPEND)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "496b47ce",
   "metadata": {},
   "source": [
    "<a id=\"overwrite\"></a>\n",
    "#### 3.5.2. Overwrite\n",
    "In ``overwrite`` mode, the existing table is replaced entirely with the new data being saved. If the table already exists, it is dropped and a new table is created with the new data. This mode is useful when you want to completely refresh the data in the table with the latest data and discard all previous records."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "016a193d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:great_expectations.validator.validator:\t3 expectation(s) included in expectation_suite.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5f4a9a6dac0d4e0fab26e361a9ebf5be",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Calculating Metrics:   0%|          | 0/16 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:ads.feature_store.common.utils.utility:Validation Summary \n",
      "╒════════════════════╤══════════════════════════╤═══════════════════════════╤═════════════════════════════╤═══════════════════╤═══════════════════════╕\n",
      "│  expectation_type  │  evaluated_expectations  │  successful_expectations  │  unsuccessful_expectations  │  success_percent  │   ingestion_status    │\n",
      "╞════════════════════╪══════════════════════════╪═══════════════════════════╪═════════════════════════════╪═══════════════════╪═══════════════════════╡\n",
      "│      LENIENT       │            3             │             1             │              2              │      33.3333      │ Ingestion in progress │\n",
      "╘════════════════════╧══════════════════════════╧═══════════════════════════╧═════════════════════════════╧═══════════════════╧═══════════════════════╛\n",
      "INFO:ads.feature_store.common.utils.utility:Validations Rules Summary \n",
      "╒═════════════════════════════════════╤══════════════════════════════════════════════════════════════╤══════════╕\n",
      "│              rule_type              │                          arguments                           │  status  │\n",
      "╞═════════════════════════════════════╪══════════════════════════════════════════════════════════════╪══════════╡\n",
      "│ expect_column_values_to_not_be_null │                   {'column': 'IATA_CODE'}                    │   True   │\n",
      "├─────────────────────────────────────┼──────────────────────────────────────────────────────────────┼──────────┤\n",
      "│ expect_column_values_to_be_between  │ {'column': 'LATITUDE', 'min_value': -1.0, 'max_value': 1.0}  │  False   │\n",
      "├─────────────────────────────────────┼──────────────────────────────────────────────────────────────┼──────────┤\n",
      "│ expect_column_values_to_be_between  │ {'column': 'LONGITUDE', 'min_value': -1.0, 'max_value': 1.0} │  False   │\n",
      "╘═════════════════════════════════════╧══════════════════════════════════════════════════════════════╧══════════╛\n",
      "WARNING:py.warnings:/home/datascience/conda/fspyspark32_p38_cpu_v2/lib/python3.8/site-packages/pyspark/sql/pandas/utils.py:35: DeprecationWarning: distutils Version classes are deprecated. Use packaging.version instead.\n",
      "  if LooseVersion(pandas.__version__) < LooseVersion(minimum_pandas_version):\n",
      "\n",
      "WARNING:py.warnings:/home/datascience/conda/fspyspark32_p38_cpu_v2/lib/python3.8/site-packages/pyspark/sql/pandas/utils.py:57: DeprecationWarning: distutils Version classes are deprecated. Use packaging.version instead.\n",
      "  if LooseVersion(pyarrow.__version__) < LooseVersion(minimum_pyarrow_version):\n",
      "\n",
      "WARNING:py.warnings:/home/datascience/conda/fspyspark32_p38_cpu_v2/lib/python3.8/site-packages/pyspark/sql/pandas/conversion.py:471: FutureWarning: iteritems is deprecated and will be removed in a future version. Use .items instead.\n",
      "  arrow_data = [[(c, t) for (_, c), t in zip(pdf_slice.iteritems(), arrow_types)]\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:py.warnings:/home/datascience/conda/fspyspark32_p38_cpu_v2/lib/python3.8/site-packages/mlm_insights/core/metrics/drift_metrics/chi_square.py:239: DeprecationWarning: The 'warn' method is deprecated, use 'warning' instead\n",
      "  logger.warn(\"Reference profile is empty\")\n",
      "\n",
      "INFO:ads.feature_store.common.utils.utility:Ingestion Summary \n",
      "╒══════════════════════════════════╤═══════════════╤════════════════════╤═════════════════╕\n",
      "│            entity_id             │  entity_type  │  ingestion_status  │  error_details  │\n",
      "╞══════════════════════════════════╪═══════════════╪════════════════════╪═════════════════╡\n",
      "│ ED84DFFD7CF86AAA612CAFD01DA9EBAA │ FEATURE_GROUP │     Succeeded      │      None       │\n",
      "╘══════════════════════════════════╧═══════════════╧════════════════════╧═════════════════╛\n"
     ]
    }
   ],
   "source": [
    "from ads.feature_store.feature_group_job import BatchIngestionMode\n",
    "feature_group_airports.materialise(airports_df, ingestion_mode=BatchIngestionMode.OVERWRITE)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "689c010b",
   "metadata": {},
   "source": [
    "<a id=\"upsert\"></a>\n",
    "#### 3.5.3. Upsert\n",
    "``Upsert`` mode (merge mode) is used to update existing records in the table based on a primary key or a specified condition. If a record with the same key exists, it is updated with the new data. Otherwise, a new record is inserted. This mode is useful for maintaining and synchronizing data between the source and destination tables while avoiding duplicates."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "aaf4b4e8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:great_expectations.validator.validator:\t3 expectation(s) included in expectation_suite.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9e87ea846ba74478a1bd434e0696bbd0",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Calculating Metrics:   0%|          | 0/16 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:ads.feature_store.common.utils.utility:Validation Summary \n",
      "╒════════════════════╤══════════════════════════╤═══════════════════════════╤═════════════════════════════╤═══════════════════╤═══════════════════════╕\n",
      "│  expectation_type  │  evaluated_expectations  │  successful_expectations  │  unsuccessful_expectations  │  success_percent  │   ingestion_status    │\n",
      "╞════════════════════╪══════════════════════════╪═══════════════════════════╪═════════════════════════════╪═══════════════════╪═══════════════════════╡\n",
      "│      LENIENT       │            3             │             1             │              2              │      33.3333      │ Ingestion in progress │\n",
      "╘════════════════════╧══════════════════════════╧═══════════════════════════╧═════════════════════════════╧═══════════════════╧═══════════════════════╛\n",
      "INFO:ads.feature_store.common.utils.utility:Validations Rules Summary \n",
      "╒═════════════════════════════════════╤══════════════════════════════════════════════════════════════╤══════════╕\n",
      "│              rule_type              │                          arguments                           │  status  │\n",
      "╞═════════════════════════════════════╪══════════════════════════════════════════════════════════════╪══════════╡\n",
      "│ expect_column_values_to_not_be_null │                   {'column': 'IATA_CODE'}                    │   True   │\n",
      "├─────────────────────────────────────┼──────────────────────────────────────────────────────────────┼──────────┤\n",
      "│ expect_column_values_to_be_between  │ {'column': 'LATITUDE', 'min_value': -1.0, 'max_value': 1.0}  │  False   │\n",
      "├─────────────────────────────────────┼──────────────────────────────────────────────────────────────┼──────────┤\n",
      "│ expect_column_values_to_be_between  │ {'column': 'LONGITUDE', 'min_value': -1.0, 'max_value': 1.0} │  False   │\n",
      "╘═════════════════════════════════════╧══════════════════════════════════════════════════════════════╧══════════╛\n",
      "WARNING:py.warnings:/home/datascience/conda/fspyspark32_p38_cpu_v2/lib/python3.8/site-packages/pyspark/sql/pandas/utils.py:35: DeprecationWarning: distutils Version classes are deprecated. Use packaging.version instead.\n",
      "  if LooseVersion(pandas.__version__) < LooseVersion(minimum_pandas_version):\n",
      "\n",
      "WARNING:py.warnings:/home/datascience/conda/fspyspark32_p38_cpu_v2/lib/python3.8/site-packages/pyspark/sql/pandas/utils.py:57: DeprecationWarning: distutils Version classes are deprecated. Use packaging.version instead.\n",
      "  if LooseVersion(pyarrow.__version__) < LooseVersion(minimum_pyarrow_version):\n",
      "\n",
      "WARNING:py.warnings:/home/datascience/conda/fspyspark32_p38_cpu_v2/lib/python3.8/site-packages/pyspark/sql/pandas/conversion.py:471: FutureWarning: iteritems is deprecated and will be removed in a future version. Use .items instead.\n",
      "  arrow_data = [[(c, t) for (_, c), t in zip(pdf_slice.iteritems(), arrow_types)]\n",
      "\n",
      "ERROR:ads.feature_store.execution_strategy.spark.spark_execution:FeatureGroup Materialization Failed with : <class 'AttributeError'> with error message: 'SparkEngine' object has no attribute 'get_columns_from_table' and stacktrace Traceback (most recent call last):\n",
      "  File \"/home/datascience/conda/fspyspark32_p38_cpu_v2/lib/python3.8/site-packages/ads/feature_store/execution_strategy/spark/spark_execution.py\", line 298, in _save_offline_dataframe\n",
      "    self.delta_lake_service.write_dataframe_to_delta_lake(\n",
      "  File \"/home/datascience/conda/fspyspark32_p38_cpu_v2/lib/python3.8/site-packages/ads/feature_store/execution_strategy/delta_lake/delta_lake_service.py\", line 72, in write_dataframe_to_delta_lake\n",
      "    for column_details in self.spark_engine.get_columns_from_table(\n",
      "AttributeError: 'SparkEngine' object has no attribute 'get_columns_from_table'\n",
      "\n",
      "INFO:ads.feature_store.common.utils.utility:Ingestion Summary \n",
      "╒══════════════════════════════════╤═══════════════╤════════════════════╤════════════════════════════════════════════════════════════════╕\n",
      "│            entity_id             │  entity_type  │  ingestion_status  │                         error_details                          │\n",
      "╞══════════════════════════════════╪═══════════════╪════════════════════╪════════════════════════════════════════════════════════════════╡\n",
      "│ ED84DFFD7CF86AAA612CAFD01DA9EBAA │ FEATURE_GROUP │       Failed       │ 'SparkEngine' object has no attribute 'get_columns_from_table' │\n",
      "╘══════════════════════════════════╧═══════════════╧════════════════════╧════════════════════════════════════════════════════════════════╛\n"
     ]
    }
   ],
   "source": [
    "from ads.feature_store.feature_group_job import BatchIngestionMode\n",
    "feature_group_airports.materialise(airports_df, ingestion_mode=BatchIngestionMode.UPSERT)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1edfbdbd",
   "metadata": {},
   "source": [
    "<a id=\"history\"></a>\n",
    "### 3.6. Viewing Feature Group History\n",
    "You can call the ``history()`` method of the FeatureGroup instance to show history of the feature group."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "ffdeab37",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:py.warnings:/home/datascience/conda/fspyspark32_p38_cpu_v2/lib/python3.8/site-packages/pyspark/sql/pandas/types.py:63: DeprecationWarning: distutils Version classes are deprecated. Use packaging.version instead.\n",
      "  if LooseVersion(pa.__version__) < LooseVersion(\"2.0.0\"):\n",
      "\n",
      "WARNING:py.warnings:/home/datascience/conda/fspyspark32_p38_cpu_v2/lib/python3.8/site-packages/pyspark/sql/pandas/utils.py:35: DeprecationWarning: distutils Version classes are deprecated. Use packaging.version instead.\n",
      "  if LooseVersion(pandas.__version__) < LooseVersion(minimum_pandas_version):\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>version</th>\n",
       "      <th>timestamp</th>\n",
       "      <th>userId</th>\n",
       "      <th>userName</th>\n",
       "      <th>operation</th>\n",
       "      <th>operationParameters</th>\n",
       "      <th>job</th>\n",
       "      <th>notebook</th>\n",
       "      <th>clusterId</th>\n",
       "      <th>readVersion</th>\n",
       "      <th>isolationLevel</th>\n",
       "      <th>isBlindAppend</th>\n",
       "      <th>operationMetrics</th>\n",
       "      <th>userMetadata</th>\n",
       "      <th>engineInfo</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>3</td>\n",
       "      <td>2023-12-11 07:57:00</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>CREATE OR REPLACE TABLE AS SELECT</td>\n",
       "      <td>{'isManaged': 'true', 'description': None, 'partitionBy': '[]', 'properties': '{}'}</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>2.0</td>\n",
       "      <td>Serializable</td>\n",
       "      <td>False</td>\n",
       "      <td>{'numFiles': '2', 'numOutputRows': '322', 'numOutputBytes': '20732'}</td>\n",
       "      <td>None</td>\n",
       "      <td>Apache-Spark/3.2.1 Delta-Lake/2.0.1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2</td>\n",
       "      <td>2023-12-11 07:56:39</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>WRITE</td>\n",
       "      <td>{'mode': 'Append', 'partitionBy': '[]'}</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>1.0</td>\n",
       "      <td>Serializable</td>\n",
       "      <td>True</td>\n",
       "      <td>{'numFiles': '2', 'numOutputRows': '322', 'numOutputBytes': '20732'}</td>\n",
       "      <td>None</td>\n",
       "      <td>Apache-Spark/3.2.1 Delta-Lake/2.0.1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1</td>\n",
       "      <td>2023-12-11 07:55:54</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>CREATE OR REPLACE TABLE AS SELECT</td>\n",
       "      <td>{'isManaged': 'true', 'description': None, 'partitionBy': '[]', 'properties': '{}'}</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>0.0</td>\n",
       "      <td>Serializable</td>\n",
       "      <td>False</td>\n",
       "      <td>{'numFiles': '2', 'numOutputRows': '322', 'numOutputBytes': '20732'}</td>\n",
       "      <td>None</td>\n",
       "      <td>Apache-Spark/3.2.1 Delta-Lake/2.0.1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0</td>\n",
       "      <td>2023-12-11 07:55:17</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>CREATE OR REPLACE TABLE AS SELECT</td>\n",
       "      <td>{'isManaged': 'true', 'description': None, 'partitionBy': '[]', 'properties': '{}'}</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Serializable</td>\n",
       "      <td>False</td>\n",
       "      <td>{'numFiles': '2', 'numOutputRows': '322', 'numOutputBytes': '20174'}</td>\n",
       "      <td>None</td>\n",
       "      <td>Apache-Spark/3.2.1 Delta-Lake/2.0.1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   version           timestamp userId userName  \\\n",
       "0        3 2023-12-11 07:57:00   None     None   \n",
       "1        2 2023-12-11 07:56:39   None     None   \n",
       "2        1 2023-12-11 07:55:54   None     None   \n",
       "3        0 2023-12-11 07:55:17   None     None   \n",
       "\n",
       "                           operation  \\\n",
       "0  CREATE OR REPLACE TABLE AS SELECT   \n",
       "1                              WRITE   \n",
       "2  CREATE OR REPLACE TABLE AS SELECT   \n",
       "3  CREATE OR REPLACE TABLE AS SELECT   \n",
       "\n",
       "                                                                   operationParameters  \\\n",
       "0  {'isManaged': 'true', 'description': None, 'partitionBy': '[]', 'properties': '{}'}   \n",
       "1                                              {'mode': 'Append', 'partitionBy': '[]'}   \n",
       "2  {'isManaged': 'true', 'description': None, 'partitionBy': '[]', 'properties': '{}'}   \n",
       "3  {'isManaged': 'true', 'description': None, 'partitionBy': '[]', 'properties': '{}'}   \n",
       "\n",
       "    job notebook clusterId  readVersion isolationLevel  isBlindAppend  \\\n",
       "0  None     None      None          2.0   Serializable          False   \n",
       "1  None     None      None          1.0   Serializable           True   \n",
       "2  None     None      None          0.0   Serializable          False   \n",
       "3  None     None      None          NaN   Serializable          False   \n",
       "\n",
       "                                                       operationMetrics  \\\n",
       "0  {'numFiles': '2', 'numOutputRows': '322', 'numOutputBytes': '20732'}   \n",
       "1  {'numFiles': '2', 'numOutputRows': '322', 'numOutputBytes': '20732'}   \n",
       "2  {'numFiles': '2', 'numOutputRows': '322', 'numOutputBytes': '20732'}   \n",
       "3  {'numFiles': '2', 'numOutputRows': '322', 'numOutputBytes': '20174'}   \n",
       "\n",
       "  userMetadata                           engineInfo  \n",
       "0         None  Apache-Spark/3.2.1 Delta-Lake/2.0.1  \n",
       "1         None  Apache-Spark/3.2.1 Delta-Lake/2.0.1  \n",
       "2         None  Apache-Spark/3.2.1 Delta-Lake/2.0.1  \n",
       "3         None  Apache-Spark/3.2.1 Delta-Lake/2.0.1  "
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "feature_group_airports.history().toPandas()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "83ce6ce4",
   "metadata": {},
   "source": [
    "<a id=\"preview\"></a>\n",
    "### 3.7. Time travel Queries on Feature Group\n",
    "\n",
    "You can call the ``as_of()`` method of the FeatureGroup instance to get specified point in time and time traveled data.\n",
    "The ``.as_of()`` method takes the following optional parameter:\n",
    "\n",
    "- commit_timestamp: date-time. Commit timestamp for feature group\n",
    "- version_number: int. Version number for feature group"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "aec5c685",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------+--------------------+-------------+-----+--------+----------+\n",
      "|IATA_CODE|             AIRPORT|         CITY|STATE|LATITUDE| LONGITUDE|\n",
      "+---------+--------------------+-------------+-----+--------+----------+\n",
      "|      ABE|Lehigh Valley Int...|    Allentown|   PA|40.65236|  -75.4404|\n",
      "|      ABI|Abilene Regional ...|      Abilene|   TX|32.41132|  -99.6819|\n",
      "|      ABQ|Albuquerque Inter...|  Albuquerque|   NM|35.04022|-106.60919|\n",
      "|      ABR|Aberdeen Regional...|     Aberdeen|   SD|45.44906| -98.42183|\n",
      "|      ABY|Southwest Georgia...|       Albany|   GA|31.53552| -84.19447|\n",
      "|      ACK|Nantucket Memoria...|    Nantucket|   MA|41.25305| -70.06018|\n",
      "|      ACT|Waco Regional Air...|         Waco|   TX|31.61129| -97.23052|\n",
      "|      ACV|      Arcata Airport|Arcata/Eureka|   CA|40.97812|-124.10862|\n",
      "|      ACY|Atlantic City Int...|Atlantic City|   NJ|39.45758| -74.57717|\n",
      "|      ADK|        Adak Airport|         Adak|   AK|51.87796|-176.64603|\n",
      "|      ADQ|      Kodiak Airport|       Kodiak|   AK|57.74997|-152.49386|\n",
      "|      AEX|Alexandria Intern...|   Alexandria|   LA|31.32737| -92.54856|\n",
      "|      AGS|Augusta Regional ...|      Augusta|   GA|33.36996|  -81.9645|\n",
      "|      AKN| King Salmon Airport|  King Salmon|   AK| 58.6768|-156.64922|\n",
      "|      ALB|Albany Internatio...|       Albany|   NY|42.74812| -73.80298|\n",
      "|      ALO|Waterloo Regional...|     Waterloo|   IA|42.55708| -92.40034|\n",
      "|      AMA|Rick Husband Amar...|     Amarillo|   TX|35.21937|-101.70593|\n",
      "|      ANC|Ted Stevens Ancho...|    Anchorage|   AK|61.17432|-149.99619|\n",
      "|      APN|Alpena County Reg...|       Alpena|   MI|45.07807| -83.56029|\n",
      "|      ASE|Aspen-Pitkin Coun...|        Aspen|   CO|39.22316|-106.86885|\n",
      "+---------+--------------------+-------------+-----+--------+----------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "feature_group_airports.as_of(version_number = 0).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "f385ea9e",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------+--------------------+-------------+-----+--------+----------+-------+\n",
      "|IATA_CODE|             AIRPORT|         CITY|STATE|LATITUDE| LONGITUDE|COUNTRY|\n",
      "+---------+--------------------+-------------+-----+--------+----------+-------+\n",
      "|      ABE|Lehigh Valley Int...|    Allentown|   PA|40.65236|  -75.4404|    USA|\n",
      "|      ABI|Abilene Regional ...|      Abilene|   TX|32.41132|  -99.6819|    USA|\n",
      "|      ABQ|Albuquerque Inter...|  Albuquerque|   NM|35.04022|-106.60919|    USA|\n",
      "|      ABR|Aberdeen Regional...|     Aberdeen|   SD|45.44906| -98.42183|    USA|\n",
      "|      ABY|Southwest Georgia...|       Albany|   GA|31.53552| -84.19447|    USA|\n",
      "|      ACK|Nantucket Memoria...|    Nantucket|   MA|41.25305| -70.06018|    USA|\n",
      "|      ACT|Waco Regional Air...|         Waco|   TX|31.61129| -97.23052|    USA|\n",
      "|      ACV|      Arcata Airport|Arcata/Eureka|   CA|40.97812|-124.10862|    USA|\n",
      "|      ACY|Atlantic City Int...|Atlantic City|   NJ|39.45758| -74.57717|    USA|\n",
      "|      ADK|        Adak Airport|         Adak|   AK|51.87796|-176.64603|    USA|\n",
      "|      ADQ|      Kodiak Airport|       Kodiak|   AK|57.74997|-152.49386|    USA|\n",
      "|      AEX|Alexandria Intern...|   Alexandria|   LA|31.32737| -92.54856|    USA|\n",
      "|      AGS|Augusta Regional ...|      Augusta|   GA|33.36996|  -81.9645|    USA|\n",
      "|      AKN| King Salmon Airport|  King Salmon|   AK| 58.6768|-156.64922|    USA|\n",
      "|      ALB|Albany Internatio...|       Albany|   NY|42.74812| -73.80298|    USA|\n",
      "|      ALO|Waterloo Regional...|     Waterloo|   IA|42.55708| -92.40034|    USA|\n",
      "|      AMA|Rick Husband Amar...|     Amarillo|   TX|35.21937|-101.70593|    USA|\n",
      "|      ANC|Ted Stevens Ancho...|    Anchorage|   AK|61.17432|-149.99619|    USA|\n",
      "|      APN|Alpena County Reg...|       Alpena|   MI|45.07807| -83.56029|    USA|\n",
      "|      ASE|Aspen-Pitkin Coun...|        Aspen|   CO|39.22316|-106.86885|    USA|\n",
      "+---------+--------------------+-------------+-----+--------+----------+-------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "feature_group_airports.as_of(version_number = 1).show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "33b2f784",
   "metadata": {},
   "source": [
    "<a id='references'></a>\n",
    "# 4. References\n",
    "- [Feature Store Documentation](https://feature-store-accelerated-data-science.readthedocs.io/en/latest/overview.html)\n",
    "- [ADS Library Documentation](https://accelerated-data-science.readthedocs.io/en/latest/index.html)\n",
    "- [Data Science YouTube Videos](https://www.youtube.com/playlist?list=PLKCk3OyNwIzv6CWMhvqSB_8MLJIZdO80L)\n",
    "- [OCI Data Science Documentation](https://docs.cloud.oracle.com/en-us/iaas/data-science/using/data-science.htm)\n",
    "- [Oracle Data & AI Blog](https://blogs.oracle.com/datascience/)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aef80bd0",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:fspyspark32_p38_cpu_v2]",
   "language": "python",
   "name": "conda-env-fspyspark32_p38_cpu_v2-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
