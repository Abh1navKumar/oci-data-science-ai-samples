PySpark and Data Flow 
=====================

The [PySpark conda environment family](https://docs.oracle.com/en-us/iaas/data-science/using/conda-pyspark-fam.htm) allows you to create and run [PySpark](https://spark.apache.org/docs/latest/api/python/) operations within the notebook session. The PySpark and Data Flow conda allow you to leverage the power of Apache Spark. It is also a great create and debug PySpark application before submitting them to the [OCI Data Flow service](https://www.oracle.com/big-data/data-flow/) which is OCI's Spark service. You can interactively develop Spark applications and submit them to Oracle Data Flow without blocking the notebook session. PySpark MLlib implements a wide collection of powerful machine-learning algorithms. Use the SQL-like language of PySparkSQL to analyze huge amounts of structure and semi-structured data stored on Oracle Object Storage. Speed up your workflow by using sparksql-magic to run PySparkSQL queries directly in the notebook.

The notebooks in this folder are meant to be run in the [PySpark conda environment family](https://docs.oracle.com/en-us/iaas/data-science/using/conda-pyspark-fam.htm) conda environments.

# Notebook Descriptions

* `dataflow.ipynb`: The purpose of the `dataflow` module is to provide an efficient and convenient way for users to launch a Spark application and run Spark jobs. This notebook demonstrates operations that can be performed using the Advanced Data Science (ADS) Data Flow module.
* `pyspark.ipynb`: This notebook demonstrates spark operations for customers by bridging the existing local spark workflows with cloud-based capabilities. Data scientists can use JupyterLab and work with remote data and clusters. The operations that will be demonstrated are: how to use the interactive spark environment and produce a spark script, prepare and create an application; prepare and create a run; list existing Data Flow applications; and retrieve and display the logs.
* `pyspark_adb.ipynb`: This notebook demonstrates how to use PySpark to process data in Oracle Cloud Infrastructure (OCI) Object Storage and save the results to an Oracle Autonomous Database. It also demonstrates how to query data from an Autonomous Database (ADB) using a local PySpark session. 
* `pyspark_adb_dtypes.ipynb`: Spark, Python and the Autonomous Database (ADB) have different data types. When moving data between these systems there is a data type conversion that happens. While the drivers generally do a good job converting that data types, it is possible that the default type that is chosen is not ideal for your data. This notebook walks you through the process of specifying the specific data types to be used for each column on each platform.
* `pyspark_adb_partition.ipynb`: This notebook demonstrates how to extract information from an Oracle Autonomous Database (ADB) and load it into a partitioned PySpark dataframe. It demonstrates two methods to do this. The notebook shows you how to extract an entire ADB table, partition it, and then load it into a PySpark dataframe. Then it demonstrates how to create a partitioned Spark dataframe using a [Data Query Language (DQL)](https://en.wikipedia.org/wiki/Data_query_language) to extract relational data or subsets of data from an ADB.

