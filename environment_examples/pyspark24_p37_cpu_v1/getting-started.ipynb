{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<font color=gray>ADS Sample Notebook.\n",
    "\n",
    "Copyright (c) 2021 Oracle, Inc. All rights reserved. Licensed under the Universal Permissive License v 1.0 as shown at https://oss.oracle.com/licenses/upl.\n",
    "</font>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***\n",
    "# <font color=red>Getting Started with Oracle Cloud Infrastructure Data Science</font>\n",
    "<p style=\"margin-left:10%; margin-right:10%;\">by the <font color=teal> Oracle Cloud Infrastructure Data Science Service Team </font></p>\n",
    "\n",
    "***"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Service Overview\n",
    "\n",
    "Welcome to Oracle Cloud Infrastructure Data Science Service!\n",
    "\n",
    "Oracle Cloud Infrastructure Data Science Service is a fully managed platform for data science teams to build, train, and manage machine learning models using Oracle Cloud Infrastructure.\n",
    "\n",
    "The Data Science Service:\n",
    "\n",
    "* Provides data scientists with a collaborative, project-driven workspace.\n",
    "* Enables self-service access to infrastructure for data science workloads.\n",
    "* Includes Python-centric tools, libraries, and packages developed by the open-source community and the [Oracle Accelerated Data Science Library](https://docs.cloud.oracle.com/en-us/iaas/tools/ads-sdk/latest/index.html), which supports the end-to-end lifecycle of predictive models:\n",
    "* Data acquisition, profiling, preparation, and visualization.\n",
    "* Feature engineering.\n",
    "* Model training.\n",
    "* Model evaluation, explanation, and interpretation.\n",
    "* Model storage through the [Model Catalog](https://docs.cloud.oracle.com/en-us/iaas/data-science/using/manage-models.htm). \n",
    "* Model deployment.\n",
    "* Integrates with the rest of the Oracle Cloud Infrastructure stack, including [Oracle Functions](https://docs.cloud.oracle.com/en-us/iaas/Content/Functions/Concepts/functionsoverview.htm), [Data Flow](https://docs.cloud.oracle.com/en-us/iaas/data-flow/using/dfs_data_flow.htm), [Autonomous Data Warehouse](https://docs.cloud.oracle.com/en-us/iaas/Content/Database/Concepts/adboverview.htm), [Streaming](https://docs.cloud.oracle.com/en-us/iaas/Content/Streaming/Concepts/streamingoverview.htm), [Vault](https://docs.cloud.oracle.com/en-us/iaas/Content/KeyManagement/Concepts/keyoverview.htm), [Logging](https://docs.cloud.oracle.com/en-us/iaas/Content/Logging/Concepts/loggingoverview.htm#loggingoverview), and [Object Storage](https://docs.cloud.oracle.com/en-us/iaas/Content/Object/Concepts/objectstorageoverview.htm).\n",
    "* Helps data scientists concentrate on methodology and domain expertise to deliver more models to production.\n",
    "\n",
    "For more details, check out the [Data Science service guide](https://docs.cloud.oracle.com/en-us/iaas/data-science/using/data-science.htm).\n",
    "\n",
    "---\n",
    "\n",
    "## Overview\n",
    "\n",
    "The PySpark and Data Flow conda allows you to leverage the power of Apache Spark. Use it to access the full computational power of a notebook session by using parallel computing. For larger jobs, you can interactively develop Spark applications and submit them to Oracle Data Flow without blocking the notebook session. PySpark MLlib implements a wide collection of powerful machine-learning algorithms. Use the SQL-like language of PySparkSQL to analyze huge amounts of structure and semi-structured data stored on Oracle Object Storage. Speed up your workflow by using sparksql-magic to run PySparkSQL queries directly in the notebook.\n",
    "\n",
    "In this notebook, we will go through how to authenticate to Oracle Cloud Infrastructure Resources and how to configure the `core-site.xml` file so PySpark can access Object Storage.\n",
    "\n",
    "---\n",
    "\n",
    "## Prerequisites:\n",
    "- Experience with a specific topic: Novice\n",
    "- Professional experience: None\n",
    "\n",
    "---\n",
    "\n",
    "## Objectives:\n",
    "\n",
    "- <a href='#authentication'>Understanding Authentication to Oracle Cloud Infrastructure Resources from a Notebook Session</a>\n",
    " - <a href='#resource_principals'>Authentication with Resource Principals</a>\n",
    "    - <a href='#resource_principals_ads'>Resource Principals Authentication using the ADS SDK</a>\n",
    "    - <a href='#resource_principals_oci'>Resource Principals Authentication using the OCI SDK</a>\n",
    "    - <a href='#resource_principals_cli'>Resource Principals Authentication using the OCI CLI</a> \n",
    "- <a href='#conda'>Conda</a>\n",
    "    - <a href='#conda_overview'>Overview</a>\n",
    "    - <a href='#conda_libraries'>Principal Conda Libraries</a>\n",
    "    - <a href='#conda_configuration'>Configuration</a>\n",
    "        - <a href='#odsc_coresite_command'>Configuration of `core-site.mxl` Using the `odsc` Command Line Tool</a>\n",
    "        - <a href='#manually_update_coresite'>Manually Configurating `core-site.xml`</a>\n",
    "        - <a href='#conda_configuration_testing'>Testing the Configuration</a>\n",
    "- <a href='#ref'>References</a> \n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import logging\n",
    "import warnings\n",
    "import os\n",
    "import ads\n",
    "from ads import set_documentation_mode\n",
    "from oci.auth.signers import get_resource_principals_signer\n",
    "from oci.data_science import DataScienceClient\n",
    "from os import path\n",
    "from os import cpu_count\n",
    "from pyspark.sql import SparkSession\n",
    "import re\n",
    "\n",
    "set_documentation_mode(False)\n",
    "warnings.filterwarnings('ignore')\n",
    "logging.basicConfig(format='%(levelname)s:%(message)s', level=logging.ERROR)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='authentication'></a>\n",
    "# Understanding Authentication to Oracle Cloud Infrastructure Resources from a Notebook Session\n",
    "\n",
    "When working within a notebook session, the `datascience` user is used. This user does not have an Oracle Cloud Infrastructure Identity and Access Management (IAM) identity, so it has no access to the Oracle Cloud Infrastructure API. To access Oracle Cloud Infrastructure resources, including Data Science projects, models and any other Oracle Cloud Infrastructure service resources from the notebook environment, you must configure either resource principals or API keys. \n",
    "\n",
    "PySpark cannot reach Object Storage using resource principals. The only auth mechanism that would allow PySpark to connect with Object Storage is through setting up the Oracle Cloud Infrastructure configuration and key files.  In addition, the configuration and key files cannot contain a passphrase.  Please refer to [OCI Documentation on Setting up Keys and Configuration File](https://docs.cloud.oracle.com/en-us/iaas/Content/API/Concepts/devguidesetupprereq.htm) and the example notebook `api_keys.ipynb` on how to set up configuration file and API keys.\n",
    "\n",
    "If you have to have a passphrase in your configuration and key files, you can download the file from Object Storage locally with the OCI Python SDK and then load the local file in Spark context.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='resource_principals'></a>\n",
    "## Authentication with Resource Principals \n",
    "\n",
    "**Note: If you authenticate with Resource Principals, PySpark will not be able to access Object Storage.** \n",
    "\n",
    "Oracle Cloud Infrastructure Data Science enables easy and secure authentication using the notebook session's resource principal to access other Oracle Cloud Infrastructure resources, including Data Science projects and models. Follow the steps below to utilize your notebook session's resource principal.\n",
    "\n",
    "In advance, a tenancy administrator must write policies to grant permissions to the resource principal to access other Oracle Cloud Infrastructure resources, see [Manually Configuring Your Tenancy for Data Science](https://docs.cloud.oracle.com/en-us/iaas/data-science/using/configure-tenancy.htm) for more details.\n",
    "\n",
    "There are two methods to configure the notebook to use resource principals and they are `ads` library or using the `oci` library. While both these libraries provide the required authentication, the `ads` library  has been specifically designed for easy operation within a Data Science notebook session.\n",
    "\n",
    "If you do not wish to take on these library dependencies, it is also possible to use the `oci` command on the command line.\n",
    "\n",
    "For more details on using resource principals in the Data Science service, see the [ADS Configuration Guide](https://docs.cloud.oracle.com/en-us/iaas/tools/ads-sdk/latest/user_guide/configuration/configuration.html#) and the [Authenticating to the Oracle Cloud Infrastructure APIs from a Notebook Session](https://docs.cloud.oracle.com/en-us/iaas/data-science/using/use-notebook-sessions.htm#topic_kxj_znw_pkb).\n",
    "\n",
    "<a id='resource_principals_ads'></a>\n",
    "### Resource Principals Authentication using the ADS SDK\n",
    "\n",
    "Within a notebook session, configure the use of a resource principal for the ADS SDK by running this in a notebook cell:\n",
    "\n",
    "The `set_auth()` method sets the proper authentication mechanism for ADS. ADS uses the `oci` SDK to access resources like the model catalog or Object Storage."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ads.set_auth(auth='resource_principal') "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='resource_principals_oci'></a>\n",
    "### Resource Principals Authentication using the OCI SDK\n",
    "\n",
    "Within your notebook session, the `oci` library can use the resource principal. This method gives more flexibility but this flexibility is generally not needed within a Data Science notebook session. The following cell demonstrates how to make a basic connection using the default settings."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "resource_principal = get_resource_principals_signer() \n",
    "dsc = DataScienceClient(config={}, signer=resource_principal)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='resource_principals_cli'></a>\n",
    "### Resource Principals Authentication using the OCI CLI\n",
    "\n",
    "Within a notebook session, the Oracle Cloud Infrastructure CLI can be used to configure the resource principal using the `--auth=resource_principal` flag. For example:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cmd = \"oci data-science project get --project-id=$PROJECT_OCID --auth=resource_principal 2>&1\"\n",
    "print(os.popen(cmd).read())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If the resource principal is correctly configured, a message similar to the following will be printed.\n",
    "\n",
    "```{json}\n",
    "{\n",
    "\"data\": {\n",
    "\"compartment-id\": \"ocid1.compartment.oc1..aaaaaaaafl3avkal72rrwuy4m5rumpwh7r4axzzzzzz...\",\n",
    "\"created-by\": \"ocid1.user.oc1..aaaaaaaabfrlcbiyvjmjvgh3ns6trdyoewxytzzzzzz...\",\n",
    "\"defined-tags\": {},\n",
    "\"description\": \"my favorite demo project\\n\",\n",
    "\"display-name\": \"demo-project\",\n",
    "\"freeform-tags\": {},\n",
    "\"id\": \"ocid1.datascienceproject.oc1.iad.aaaaaaaappvg4tp5kmbkurcyeghxaqmaknw3szzzzzz...\",\n",
    "\"lifecycle-state\": \"ACTIVE\",\n",
    "\"time-created\": \"2019-11-14T22:29:06.870000+00:00\"\n",
    "},\n",
    "\"etag\": \"b4d66fb733748f3454206d5de6b9acb3634edc804zzzzzz...\"\n",
    "}\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='conda'></a>\n",
    "# Conda\n",
    "\n",
    "<a id='conda_overview'></a>\n",
    "## Overview\n",
    "\n",
    "This conda allows data scientists to leverage Apache Spark.  You can set up Spark applications and submit them to Oracle Cloud Infrastructure Data Flow.  Also, you can use PySpark including PySpark MLib and PySparkSQL.  \n",
    "\n",
    "<a id='conda_libraries'></a>\n",
    "## Principal Conda Libraries\n",
    "\n",
    "Here are some of the libraries included in this conda:\n",
    "\n",
    "- ads: Partial ADS distribution. This distribution excludes Oracle AutoML and MLX. \n",
    "- oraclejdk: Oracle Java Development Kits\n",
    "- pyspark: python API for Apache Spark \n",
    "- scikit-learn: library for building machine learning models including regressions, classifiers and clustering algorithms\n",
    "- sparksql-magic: library for Spark SQL magic commands for Jupyter Notebook"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='conda_configuration'></a>\n",
    "## Configuration\n",
    "\n",
    "**To access Oracle Cloud Infrastructure Object Storage the `core-site.xml` file must be configured.**  \n",
    "\n",
    "`core-site.xml` can be manually configured or configured with the help of the `odsc` program.\n",
    "\n",
    "<a id='odsc_coresite_command'></a>\n",
    "### Configuration of `core-site.mxl` Using the `odsc` Command Line Tool\n",
    "\n",
    "With an oci config file, you can run `odsc core-site config -o`. This by default uses the oci config file stored at `~/.oci/config`, auto-populates `core-site.xml`, and saves to `~/spark_conf_dir/core-site.xml`. \n",
    "\n",
    "The following command line options are available: \n",
    "- -a, --authentication Authentication mode. Only api_key is supported.\n",
    "- -c, --config Path to the oci config file.\n",
    "- -p, --profile Name of the profile.\n",
    "- -r, --region Name of the region.\n",
    "- -o, --overwrite Overwrite core-site.xml.\n",
    "- -O, --output Output path for core-site.xml.\n",
    "- -q, --quiet Suppress non-error output.\n",
    "\n",
    "Run `odsc core-site config --help` to checking the usage of this cli through command line\n",
    "\n",
    "<a id='manually_update_coresite'></a>\n",
    "### Manually Configuring `core-site.xml`\n",
    "When the conda package is installed, a templated version of `core-site.xml` is installed. This file can be manually updated.\n",
    "\n",
    "The `core-site.xml` file has several parameters you need to provide. Here are their descriptions:\n",
    "\n",
    "`fs.oci.client.hostname`: address of object storage i.e. `https://objectstorage.us-ashburn-1.oraclecloud.com` You will need to replace us-ashburn-1 with the region you are in.\n",
    "\n",
    "`fs.oci.client.auth.tenantId`: OCID of your tenancy.\n",
    "\n",
    "`fs.oci.client.auth.userId`: your user OCID.\n",
    "\n",
    "`fs.oci.client.auth.fingerprint`: fingerprint for the key pair being used.\n",
    "\n",
    "`fs.oci.client.auth.pemfilepath`: The full path and file name of the private key used for authentication. \n",
    "\n",
    "The values of these parameters can be found inside the OCI configuration file.\n",
    "\n",
    "The following is an example `core-site.xml` file that has been updated. Place all the parameter values between the `<value>` and `</value>` tags.\n",
    "\n",
    "```{xml}\n",
    "<configuration><!-- reference: https://docs.cloud.oracle.com/en-us/iaas/Content/API/SDKDocs/hdfsconnector.htm -->\n",
    "  <property>\n",
    "    <name>fs.oci.client.hostname</name>\n",
    "    <value>https://objectstorage.us-ashburn-1.oraclecloud.com</value>\n",
    "  </property>\n",
    "  <!--<property>-->\n",
    "    <!--<name>fs.oci.client.hostname.myBucket.myNamespace</name>-->\n",
    "    <!--<value></value>&lt;!&ndash; myBucket@myNamespace &ndash;&gt;-->\n",
    "  <!--</property>-->\n",
    "  <property>\n",
    "    <name>fs.oci.client.auth.tenantId</name>\n",
    "    <value>ocid1.tenancy.oc1..aaaaaaaa25c5a2zpfki3wo4ofza5l72aehvwkzzzzz...</value> \n",
    "  </property>\n",
    "  <property>\n",
    "    <name>fs.oci.client.auth.userId</name>\n",
    "    <value>ocid1.user.oc1..aaaaaaaacdxbfmyhe7sxc6iwi73okzuf3src6zzzzzz...</value>\n",
    "  </property>\n",
    "  <property>\n",
    "    <name>fs.oci.client.auth.fingerprint</name>\n",
    "    <value>01:01:02:03:05:08:13:1b:2e:49:77:c0:01:37:01:f7</value>\n",
    "  </property>\n",
    "  <property>\n",
    "    <name>fs.oci.client.auth.pemfilepath</name>\n",
    "    <value>/home/datascience/.oci/key.pem</value>\n",
    "  </property>\n",
    "</configuration>\n",
    "```\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='conda_configuration_testing'></a>\n",
    "### Testing the Configuration\n",
    "\n",
    "Set up a spark session in your PySpark conda environment to test if the configuration has been set up properly.  Run the following cells and make sure there are no errors."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create a spark session\n",
    "spark = SparkSession \\\n",
    "    .builder \\\n",
    "    .appName(\"Python Spark SQL basic example\") \\\n",
    "    .config(\"spark.driver.cores\", str(max(1, cpu_count() - 1))) \\\n",
    "    .config(\"spark.executor.cores\", str(max(1, cpu_count() - 1))) \\\n",
    "    .getOrCreate()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We are going to load a CSV file from a public bucket."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "berlin_airbnb = spark\\\n",
    "      .read\\\n",
    "      .format(\"csv\")\\\n",
    "      .option(\"header\", \"true\")\\\n",
    "      .option(\"multiLine\", \"true\")\\\n",
    "      .load(\"oci://oow_2019_dataflow_lab@bigdatadatasciencelarge/usercontent/kaggle_berlin_airbnb_listings_summary.csv\")\\\n",
    "      .cache() # cache the dataset to increase computing speed\n",
    "# the dataframe as a sql view so we can perform SQL on it\n",
    "berlin_airbnb.createOrReplaceTempView(\"berlin\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "One can also use the sparksql magic to run a query on the view and store the results as a dataframe: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext sparksql_magic\n",
    "%config SparkSql.max_num_rows=20"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%sparksql --cache --view result df \n",
    "\n",
    "SELECT latitude, longitude FROM berlin LIMIT 10"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='ref'></a>\n",
    "# References\n",
    "\n",
    "* [Understanding and Using Conda Environments](https://docs.cloud.oracle.com/en-us/iaas/data-science/using/use-notebook-sessions.htm#conda_understand_environments)\n",
    "* [ADS Configuration Guide](https://docs.cloud.oracle.com/en-us/iaas/tools/ads-sdk/latest/user_guide/configuration/configuration.html#)\n",
    "* [Authenticating to the Oracle Cloud Infrastructure APIs from a Notebook Session](https://docs.cloud.oracle.com/en-us/iaas/data-science/using/use-notebook-sessions.htm#topic_kxj_znw_pkb)\n",
    "* [Manually Configuring Your Tenancy for Data Science](https://docs.cloud.oracle.com/en-us/iaas/data-science/using/configure-tenancy.htm)\n",
    "* [Data Science service guide](https://docs.cloud.oracle.com/en-us/iaas/data-science/using/data-science.htm)\n",
    "* [Data Flow service guide](https://docs.cloud.oracle.com/en-us/iaas/data-flow/using/dfs_getting_started.htm)\n",
    "* [Our Data Science & AI Blog](https://blogs.oracle.com/datascience/)\n",
    "* [PySpark Documentation](https://spark.apache.org/docs/latest/api/python/index.html)\n",
    "* [SparkSQL Magic Documentation](https://github.com/cryeo/sparksql-magic)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:dbv1]",
   "language": "python",
   "name": "conda-env-dbv1-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
