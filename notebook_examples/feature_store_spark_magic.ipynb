{
 "cells": [
  {
   "cell_type": "raw",
   "id": "5c01b54f",
   "metadata": {},
   "source": [
    "qweews@notebook{feature_store-querying.ipynb,\n",
    "    title: Data Flow Studio : Big Data Operations in Feature Store,\n",
    "    summary: Run Feature Store on interactive Spark workloads on a long lasting Data Flow Cluster.,\n",
    "    developed_on: fspyspark32_p38_cpu_v1,\n",
    "    keywords: feature store, querying,\n",
    "    license: Universal Permissive License v 1.0\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8df0ae4c",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "Oracle Data Science service sample notebook.\n",
    "\n",
    "Copyright (c) 2022 Oracle, Inc. All rights reserved. Licensed under the [Universal Permissive License v 1.0](https://oss.oracle.com/licenses/upl).\n",
    "\n",
    "***\n",
    "\n",
    "# <font color=\"red\">Data Flow Studio: Big Data Operations in Feature Store</font>\n",
    "<p style=\"margin-left:10%; margin-right:10%;\">by the <font color=\"teal\">Oracle Cloud Infrastructure Data Science Service.</font></p>\n",
    "\n",
    "---\n",
    "# Overview:\n",
    "\n",
    "This notebook demonstrates how to run Feature Store on interactive Spark workloads on a long lasting [Oracle Cloud Infrastructure Data Flow](https://docs.oracle.com/en-us/iaas/data-flow/using/home.htm) cluster through [Apache Livy](https://livy.apache.org/) integration. **Data Flow Spark Magic** is used for interactively working with remote Spark clusters through Livy, a Spark REST server, in Jupyter notebooks. It includes a set of magic commands for interactively running Spark code.\n",
    "\n",
    "\n",
    "\n",
    "## Contents:\n",
    "\n",
    "- <a href=\"#introduction\">1. Introduction</a>\n",
    "- <a href='#pre_requisites'>2. Pre-requisites</a>\n",
    "    - <a href='#policies_'>2.1 Policies</a>\n",
    "    - <a href='#prerequisites_helpers'>2.2 Helpers</a>\n",
    "    - <a href='#authentication'>2.3 Authentication</a>\n",
    "    - <a href='#variables'>2.4 Variables</a>\n",
    "- <a href='#dataflow_magic'>3. Dataflow Magic</a>\n",
    "    - <a href='#load_extension'>3.1. Load extension</a>\n",
    "    - <a href='#create_session'>3.2. Create DataFlow Session</a>\n",
    "    - <a href='#data_exploration'>3.3. Data exploration</a>\n",
    "    - <a href='#load_featuregroup'>3.4. Creation of logical entities of feature group</a>\n",
    "        - <a href='#create_feature_store'>3.4.1 Creation of feature store</a>\n",
    "        - <a href='#create_entity'>3.4.2 Creation of entity</a>\n",
    "        - <a href='#create_feature_group'>3.4.3 Creation of feature group</a>\n",
    "        - <a href='#materialise_feature_store'>3.4.4 Materialisation of feature group</a>\n",
    "        - <a href='#query_feature_group'>3.4.5 Querying of feature group</a>\n",
    "- <a href='#references'>4. References</a>\n",
    "\n",
    "---\n",
    "\n",
    "\n",
    "Compatible conda pack: [PySpark 3.2 and Feature Store](https://docs.oracle.com/iaas/data-science/using/conda-pyspark-fam.htm) for CPU on Python 3.8 (version 1.0)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e8463c70",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Upgrade Oracle ADS to pick up the latest preview version to maintain compatibility with Oracle Cloud Infrastructure.\n",
    "!pip install --pre --no-deps oracle-ads==2.9.0rc0"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "496a7e98",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "<a id=\"introduction\"></a>\n",
    "# 1. Introduction\n",
    "\n",
    "Oracle feature store is a stack based solution that is deployed in the customer enclave using OCI resource manager. Customer can stand up the service with infrastructure in their own tenancy. The service consists of API which are deployed in customer tenancy using resource manager.\n",
    "\n",
    "The following are some key terms that will help you understand OCI Data Science Feature Store:\n",
    "\n",
    "\n",
    "* **Feature Vector**: Set of feature values for any one primary/identifier key. Eg. All/subset of features of customer id ‘2536’ can be called as one feature vector.\n",
    "\n",
    "* **Feature**: A feature is an individual measurable property or characteristic of a phenomenon being observed.\n",
    "\n",
    "* **Entity**: An entity is a group of semantically related features. The first step a consumer of features would typically do when accessing the feature store service is to list the entities and the entities associated features. Another way to look at it is that an entity is an object or concept that is described by its features. Examples of entities could be customer, product, transaction, review, image, document, etc.\n",
    "\n",
    "* **Feature Group**: A feature group in a feature store is a collection of related features that are often used together in ml models. It serves as an organizational unit within the feature store for users to manage, version and share features across different ml projects. By organizing features into groups, data scientists and ml engineers can efficiently discover, reuse and collaborate on features reducing the redundant work and ensuring consistency in feature engineering.\n",
    "\n",
    "* **Feature Group Job**: Feature group job is the execution instance of a feature group. Each feature group job will include validation results and statistics results.\n",
    "\n",
    "* **Dataset**: A dataset is a collection of feature that are used together to either train a model or perform model inference.\n",
    "\n",
    "* **Dataset Job**: Dataset job is the execution instance of a dataset. Each dataset job will include validation results and statistics results."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "22bcec86",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "<a id='pre_requisites'></a>\n",
    "# 2. Pre-requisites to Running this Notebook\n",
    "\n",
    "Data Flow Sessions are accessible through the following conda environment: \n",
    "\n",
    "* **PySpark 3.2 and Feature Store Python 3.8 (fspyspark32_p38_cpu_v1)**\n",
    "\n",
    "The [Data Catalog Hive Metastore](https://docs.oracle.com/en-us/iaas/data-catalog/using/metastore.htm) provides schema definitions for objects in structured and unstructured data assets. The Metastore is the central metadata repository to understand tables backed by files on object storage. You can customize `fs_pyspark32_p38_cpu_v1`, publish it, and use it as a runtime environment for a Data Flow session cluster. The metastore id of hive metastore is tied to feature store construct of feature store service."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "28fd82db",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "<a id='policies_'></a>\n",
    "## 2.1. Policies\n",
    "This section covers the creation of dynamic groups and policies needed to use the service.\n",
    "\n",
    "* [Data Flow Policies](https://docs.oracle.com/iaas/data-flow/using/policies.htm)\n",
    "* [Getting Started with Data Flow](https://docs.oracle.com/iaas/data-flow/using/dfs_getting_started.htm)\n",
    "* [About Data Science Policies](https://docs.oracle.com/iaas/data-science/using/policies.htm)\n",
    "* [Data Catalog Metastore](https://docs.oracle.com/en-us/iaas/data-catalog/using/metastore.htm)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d92fd7df",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "<a id=\"prerequisites_helpers\"></a>\n",
    "## 2.2 Helpers\n",
    "This section provides a helper method used across the notebook to prepare arguments for the magic commands. This function is particularly useful when you want to pass Python variables as arguments to the spark magic commands "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3c535a71",
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "import json\n",
    "\n",
    "\n",
    "def prepare_command(command: dict) -> str:\n",
    "    \"\"\"Converts dictionary command to the string formatted commands.\"\"\"\n",
    "    return f\"'{json.dumps(command)}'\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4d699131",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "<a id=\"authentication\"></a>\n",
    "## 2.3. Authentication\n",
    "The [Oracle Accelerated Data Science SDK (ADS)](https://docs.oracle.com/iaas/tools/ads-sdk/latest/index.html) controls the authentication mechanism with the Data Flow Session Spark cluster.<br> \n",
    "To setup authentication use the ```ads.set_auth(\"resource_principal\")``` or ```ads.set_auth(\"api_key\")```. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0ed15b93",
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "import ads\n",
    "\n",
    "ads.set_auth(\"resource_principal\")  # Supported values: resource_principal, api_key"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "44d0c3f1",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "<a id=\"variables\"></a>\n",
    "## 2.4. Variables\n",
    "To run this notebook, you must provide some information about your tenancy configuration. To connect to the HIVE metastore, replace `<metastore_id>` with the OCID for the HIVE metastore.\n",
    "\n",
    "To create and run a Data Flow session, you must specify a `<compartment_id>`, `<metastoreId>`, bucket `<logs_bucket_uri>` and `<custom_conda_environment_uri>` for storing logs. These resources must be in the same compartment."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c3ab9476",
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "import os\n",
    "compartment_id = os.environ.get(\"NB_SESSION_COMPARTMENT_OCID\")\n",
    "metastore_id = \"<metastore_id>\"\n",
    "logs_bucket_uri = \"<logs-bucket-url>\"\n",
    "\n",
    "custom_conda_environment_uri = \"oci://service-conda-packs@id19sfcrra6z/service_pack/cpu/PySpark_3.2_and_Feature_Store/1.0/fspyspark32_p38_cpu_v1#conda\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "835fa366",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "<a id=\"dataflow_magic\"></a>\n",
    "# 3. Data Flow Spark Magic\n",
    "Data Flow Spark Magic commands allow you to interactively work with Data Flow Spark clusters (sessions) in Jupyter notebooks through the Livy REST API. It provides a set of Jupyter Notebook cell magic commands to turn Jupyter into an integrated Spark development environment for remote clusters. \n",
    "\n",
    "**Data Flow Magic allows you to:**\n",
    "\n",
    "* Run Spark code against Data Flow remote Spark cluster\n",
    "* Create a Data Flow Spark Session with SparkContext and HiveContext against Data Flow remote Spark cluster\n",
    "* Capture the output of Spark queries as a local Pandas data frame to interact easily with other Python libraries (e.g. matplotlib)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1b977b2b",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "<a id=\"load_extension\"></a>\n",
    "### 3.1. Load Spark Magic Commands and Getting Help\n",
    "Data Flow Spark Magic is a JupyterLab extension that you need to activate in your notebook using the `%load_ext dataflow.magics` magic command.<br>\n",
    "After the extension is activated, the `%help` command can be used to get the list of supported commands."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "da895c49",
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "%load_ext dataflow.magics"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f48aa78c",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "<a id=\"create_session\"></a>\n",
    "### 3.2. Create Session\n",
    "To create a new Data Flow cluster session use the `%create_session` magic command."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "79775a26",
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "command = prepare_command(\n",
    "    {\n",
    "        \"compartmentId\": compartment_id,\n",
    "        \"displayName\": \"spark_session_via_notebook\",\n",
    "        \"language\": \"PYTHON\",\n",
    "        \"sparkVersion\": \"3.2.1\",\n",
    "        \"numExecutors\": 8,\n",
    "        \"metastoreId\": metastore_id,\n",
    "        \"driverShape\": \"VM.Standard2.1\",\n",
    "        \"executorShape\": \"VM.Standard2.1\",\n",
    "        \"driverShapeConfig\": {\"ocpus\": 2, \"memoryInGBs\": 16},\n",
    "        \"executorShapeConfig\": {\"ocpus\": 2, \"memoryInGBs\": 16},\n",
    "        \"type\": \"SESSION\",\n",
    "        \"logsBucketUri\": logs_bucket_uri,\n",
    "        \"configuration\": {\n",
    "            \"spark.archives\": custom_conda_environment_uri,\n",
    "            \"fs.oci.client.hostname\": \"https://objectstorage.us-ashburn-1.oraclecloud.com\"\n",
    "        },\n",
    "    }\n",
    ")\n",
    "\n",
    "%create_session -l python -c $command"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a00cb706",
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "%%spark\n",
    "from great_expectations.core import ExpectationSuite, ExpectationConfiguration\n",
    "\n",
    "import ads\n",
    "from ads.feature_store.entity import Entity\n",
    "from ads.feature_store.feature_group import FeatureGroup\n",
    "from ads.feature_store.feature_group_expectation import ExpectationType\n",
    "from ads.feature_store.feature_store import FeatureStore\n",
    "from ads.feature_store.input_feature_detail import FeatureDetail, FeatureType\n",
    "from ads.feature_store.statistics_config import StatisticsConfig\n",
    "from ads.feature_store.transformation import TransformationMode\n",
    "import os\n",
    "\n",
    "# Set the Authentications for the feature store operations\n",
    "ads.set_auth(auth=\"resource_principal\", client_kwargs={\"fs_service_endpoint\": \"https://{api_gateway}/20230101\"})\n",
    "\n",
    "# Variables\n",
    "compartment_id = \"<compartment_id>\"\n",
    "metastore_id = \"<metastore_id>\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f43808d1",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "<a id=\"data_exploration\"></a>\n",
    "### 3.3. Data exploration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "90465a30",
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "%%spark\n",
    "df_nyc_tlc = spark.read.parquet(\"oci://hosted-ds-datasets@bigdatadatasciencelarge/nyc_tlc/201[1,2,3,4,5,6,7,8]/**/data.parquet\", header=False, inferSchema=True)\n",
    "df_nyc_tlc = df_nyc_tlc.select(\"vendor_id\", \"pickup_at\", \"dropoff_at\")\n",
    "\n",
    "df_nyc_tlc.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "47927e0f",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "<a id=\"load_featuregroup\"></a>\n",
    "### 3.4. Create feature store logical entities"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8a5cd5b9",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "<a id=\"create_feature_store\"></a>\n",
    "#### 3.4.1 Creation of Feature Store\n",
    "Feature store is the top level entity for feature store service"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "26067de7",
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "%%spark\n",
    "feature_store_resource = FeatureStore(). \\\n",
    "    with_description(\"Feature Store Description\"). \\\n",
    "    with_compartment_id(compartment_id). \\\n",
    "    with_display_name(\"FeatureStore\"). \\\n",
    "    with_offline_config(metastore_id=metastore_id)\n",
    "\n",
    "feature_store = feature_store_resource.create()\n",
    "feature_store"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "67bafdd2",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "<a id=\"create_entity\"></a>\n",
    "#### 3.4.2 Creation of Entity\n",
    "An entity is a group of semantically related features."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "657d3fe4",
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "%%spark\n",
    "entity = feature_store.create_entity()\n",
    "entity"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "47440cde",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "<a id=\"create_feature_group\"></a>\n",
    "#### 3.4.3 Creation of Feature group\n",
    "A feature group is an object that represents a logical group of time-series feature data as it is found in a datasource."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "690aa9f7",
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "%%spark\n",
    "\n",
    "# Initialize Expectation Suite\n",
    "expectation_suite_trans = ExpectationSuite(expectation_suite_name=\"feature_definition\")\n",
    "expectation_suite_trans.add_expectation(\n",
    "    ExpectationConfiguration(\n",
    "        expectation_type=\"EXPECT_COLUMN_VALUES_TO_NOT_BE_NULL\",\n",
    "        kwargs={\"column\": \"vendor_id\"}\n",
    "    )\n",
    ")\n",
    "\n",
    "stats_config = StatisticsConfig().with_is_enabled(False)\n",
    "\n",
    "feature_group = entity.create_feature_group(\n",
    "    primary_keys=[\"vendor_id\"],\n",
    "    schema_details_dataframe=df_nyc_tlc, #infer the schema from the data frame\n",
    "    expectation_suite=expectation_suite_trans,\n",
    "    expectation_type=ExpectationType.LENIENT,\n",
    "    statistics_config=stats_config,\n",
    "    name=\"feature_group_big_data\",\n",
    ")\n",
    "\n",
    "feature_group"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "55482958",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "<a id=\"materialise_feature_store\"></a>\n",
    "#### 3.4.4 Materialisation of Feature group"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2896b65c",
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "%%spark\n",
    "import pandas as pd\n",
    "df_nyc_tlc = spark.read.parquet(\"oci://hosted-ds-datasets@bigdatadatasciencelarge/nyc_tlc/201[1,2,3,4,5,6,7,8]/**/data.parquet\", header=False, inferSchema=True)\n",
    "df_nyc_tlc = df_nyc_tlc.select(\"vendor_id\", \"pickup_at\", \"dropoff_at\").limit(1000)\n",
    "\n",
    "feature_group.materialise(df_nyc_tlc)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d7dee26d",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "<a id=\"query_feature_group\"></a>\n",
    "#### 3.4.5 Feature group Querying"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c44d5877",
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "%%spark\n",
    "feature_group.select().show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "feb2593e",
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "%%spark\n",
    "feature_group.select([\"vendor_id\", \"pickup_at\"]).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f7fb9882",
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "%%spark\n",
    "feature_group.filter(feature_group.vendor_id == \"CMT\").show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "180bf37b",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "<a id='references'></a>\n",
    "# References\n",
    "- [Feature Store Documentation](https://feature-store-accelerated-data-science.readthedocs.io/en/latest/overview.html)\n",
    "- [ADS Library Documentation](https://accelerated-data-science.readthedocs.io/en/latest/index.html)\n",
    "- [Data Science YouTube Videos](https://www.youtube.com/playlist?list=PLKCk3OyNwIzv6CWMhvqSB_8MLJIZdO80L)\n",
    "- [OCI Data Science Documentation](https://docs.cloud.oracle.com/en-us/iaas/data-science/using/data-science.htm)\n",
    "- [Oracle Data & AI Blog](https://blogs.oracle.com/datascience/)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "009f9008",
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:fspyspark32_p38_cpu_v1]",
   "language": "python",
   "name": "conda-env-fspyspark32_p38_cpu_v1-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.17"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
