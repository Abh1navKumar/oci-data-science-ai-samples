{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<font color=gray>ADS Sample Notebook.\n",
    "\n",
    "Copyright (c) 2019,2020 Oracle, Inc.  All rights reserved.\n",
    "Licensed under the Universal Permissive License v 1.0 as shown at https://oss.oracle.com/licenses/upl.\n",
    "</font>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***\n",
    "# <font color=red>Develop pyspark jobs locally - from Local to Remote Workflows</font>\n",
    "<p style=\"margin-left:10%; margin-right:10%;\">by the <font color=teal> Oracle Cloud Infrastructure Data Science Team </font></p>\n",
    "\n",
    "***"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Overview of this Notebook"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This notebook provides spark operations for customers by bridging the existing local spark workflows with cloud based capabilities. Data scientists can use their familiar local environments with JupyterLab and work with remote data and remote clusters simply by selecting a kernel. The operations that will be demonstrated are: how to use the interactive spark env and product a spark script, how to prepare and create an application; how to prepare and create a run; how to list existing dataflow applications; and how to retrieve and display the logs.\n",
    "\n",
    "The purpose of the dataflow module is to provide an efficient and convenient way for users to launch a spark application and run spark jobs. The interactive spark kernel provides a simple and efficient way to edit and build your spark script, and easy access to read from oci filesystem.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='prereq'></a>\n",
    "## Prerequisites:\n",
    "\n",
    "1. Before accessing oci filesystem from your local spark env, please make sure that you have the `core-site.xml` under `spark_conf_dir` configured properly, because it sets the connector properties that we use to connect to oci.\n",
    "\n",
    "2. Before creating applications in the Oracle Cloud Infrastructure Data Flow service, make sure that you have configured your tenancy for the service. Please follow the steps listed in this section of the Data Flow documentation: \n",
    "\n",
    "\n",
    "https://docs.cloud.oracle.com/en-us/iaas/data-flow/using/dfs_getting_started.htm#getting_started"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import io\n",
    "import matplotlib.pyplot as plt\n",
    "import os\n",
    "from os import path\n",
    "import pandas as pd\n",
    "import tempfile\n",
    "import uuid\n",
    "\n",
    "from ads.dataflow.dataflow import DataFlow\n",
    "\n",
    "from pyspark.sql import SparkSession"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='kernel'></a>\n",
    "### Build your pyspark script using interactive spark kernel "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Set up spark session in your pyspark conda env"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create a spark session\n",
    "spark = SparkSession \\\n",
    "    .builder \\\n",
    "    .appName(\"Python Spark SQL basic example\") \\\n",
    "    .config(\"spark.driver.cores\", \"4\") \\\n",
    "    .config(\"spark.executor.cores\", \"4\") \\\n",
    "    .getOrCreate()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Load the Employee Attrition data file from Oracle Cloud Infrastructure Object Storage into a Spark DataFrame"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "emp_attrition = spark\\\n",
    "      .read\\\n",
    "      .format(\"csv\")\\\n",
    "      .option(\"header\", \"true\")\\\n",
    "      .option(\"inferSchema\", \"true\")\\\n",
    "      .option(\"multiLine\", \"true\")\\\n",
    "      .load(\"oci://hosted-ds-datasets@bigdatadatasciencelarge/synthetic/orcl_attrition.csv\") \\\n",
    "      .cache() # cache the dataset to increase computing speed\n",
    "emp_attrition.createOrReplaceTempView(\"emp_attrition\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Explore the dataframe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "spark.sql('select * from emp_attrition limit 5').toPandas()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Visualize how monthly income and age relate to one another in the context of years in industry"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots() \n",
    "plot = spark.sql(\"\"\"\n",
    "          SELECT \n",
    "              Age,\n",
    "              MonthlyIncome,\n",
    "              YearsInIndustry\n",
    "          FROM\n",
    "            emp_attrition \n",
    "          \"\"\").toPandas().plot.scatter(x=\"Age\", y=\"MonthlyIncome\", title='Age vs Monthly Income',\n",
    "                                       c=\"YearsInIndustry\", cmap=\"viridis\", figsize=(12,12), ax=ax)\n",
    "plot.set_xlabel(\"Age\")\n",
    "plot.set_ylabel(\"Monthly Income\")\n",
    "plot"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "View all of the columns in the table"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "spark.sql(\"show columns from emp_attrition\").show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Select a few columns using spark and convert it into a pandas DataFrame"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = spark.sql(\"\"\"\n",
    "         SELECT\n",
    "            Age,\n",
    "            MonthlyIncome,\n",
    "            YearsInIndustry\n",
    "          FROM\n",
    "            emp_attrition \"\"\").limit(10).toPandas()\n",
    "df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can also work with different compression formats within Dataflow. For example snappy parquet: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Writing to a snappy parquet file\n",
    "df.to_parquet('emp_attrition.parquet.snappy', compression='snappy')\n",
    "pd.read_parquet('emp_attrition.parquet.snappy')  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# We are able to read in this snappy parquet file to a spark dataframe\n",
    "read_snappy_df = SparkSession \\\n",
    "    .builder \\\n",
    "    .appName(\"Snappy Compression Loading Example\") \\\n",
    "    .config(\"spark.io.compression.codec\", \"org.apache.spark.io.SnappyCompressionCodec\") \\\n",
    "    .getOrCreate() \\\n",
    "    .read \\\n",
    "    .format(\"parquet\") \\\n",
    "    .load(f\"{os.getcwd()}/emp_attrition.parquet.snappy\")\n",
    "\n",
    "read_snappy_df.first()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note: other compression formats Data Flow supports today include snappy parquet (example above) and gzip on both csv and parquet."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We have come to a query that we want to run in Data Flow from previous explorations. Please refer to the dataflow.ipynb on how to submit a job to dataflow"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataflow_base_folder = tempfile.mkdtemp()\n",
    "data_flow = DataFlow(dataflow_base_folder=dataflow_base_folder)\n",
    "print(\"Data flow directory: {}\".format(dataflow_base_folder))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pyspark_file_path = path.join(dataflow_base_folder, \"example-{}.py\".format(str(uuid.uuid4())[-6:]))\n",
    "script = '''\n",
    "from pyspark.sql import SparkSession\n",
    "\n",
    "def main():\n",
    "    \n",
    "    # Create a Spark session\n",
    "    spark = SparkSession \\\\\n",
    "        .builder \\\\\n",
    "        .appName(\"Python Spark SQL basic example\") \\\\\n",
    "        .getOrCreate()\n",
    "    \n",
    "    # Load a csv file from dataflow public storage\n",
    "    df = spark \\\\\n",
    "        .read \\\\\n",
    "        .format(\"csv\") \\\\\n",
    "        .option(\"header\", \"true\") \\\\\n",
    "        .option(\"multiLine\", \"true\") \\\\\n",
    "        .load(\"oci://hosted-ds-datasets@bigdatadatasciencelarge/synthetic/orcl_attrition.csv\")\n",
    "    \n",
    "    # Create a temp view and do some SQL operations\n",
    "    df.createOrReplaceTempView(\"emp_attrition\")\n",
    "    query_result_df = spark.sql(\"\"\"\n",
    "        SELECT \n",
    "            Age,\n",
    "            MonthlyIncome,\n",
    "            YearsInIndustry\n",
    "        FROM emp_attrition \n",
    "    \"\"\")\n",
    "    \n",
    "    # Convert the filtered Spark DataFrame into JSON format\n",
    "    # Note: we are writing to the spark stdout log so that we can retrieve the log later at the end of the notebook.\n",
    "    print('\\\\n'.join(query_result_df.toJSON().collect()))\n",
    "    \n",
    "if __name__ == '__main__':\n",
    "    main()\n",
    "'''\n",
    "\n",
    "with open(pyspark_file_path, 'w') as f:\n",
    "    print(script.strip(), file=f)\n",
    "    \n",
    "print(\"Script path: {}\".format(pyspark_file_path))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "script_bucket = \"test\"                     # Update the value\n",
    "logs_bucket = \"dataflow-log\"               # Update the value\n",
    "display_name = \"sample_Data_Flow_app\"      \n",
    "\n",
    "app_config = data_flow.prepare_app(display_name=display_name,\n",
    "                                   script_bucket=script_bucket,\n",
    "                                   pyspark_file_path=pyspark_file_path,\n",
    "                                   logs_bucket=logs_bucket)\n",
    "\n",
    "app = data_flow.create_app(app_config)\n",
    "\n",
    "run_display_name = \"sample_Data_Flow_run\"\n",
    "run_config = app.prepare_run(run_display_name=run_display_name)\n",
    "\n",
    "run = app.run(run_config, save_log_to_local=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "run.status"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "run.config"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "run.oci_link"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:pysparkvv2]",
   "language": "python",
   "name": "conda-env-pysparkvv2-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
