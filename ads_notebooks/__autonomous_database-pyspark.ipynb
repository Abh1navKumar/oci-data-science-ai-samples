{
 "cells": [
  {
   "cell_type": "raw",
   "id": "bc797619",
   "metadata": {},
   "source": [
    "@notebook{autonomous_database-pyspark.ipynb,\n",
    "    title: Partition an Apache Spark DataFrame from an Autonomous Database,\n",
    "    summary: Extract information from an Oracle Autonomous Database (ADB) and load it into a partitioned PySpark dataframe using two different methods. First, by extracting and partitioning an entire table and loading into PyPpark, and then by using a Data Query Lanuage (DQL) to extract relational subsets of the data.,\n",
    "    developed on: pyspark24_p37_cpu_v3,\n",
    "    keywords: autonomous database, apache spark, pyspark, partitioning, data query language,\n",
    "    license: Universal Permissive License v 1.0\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "91ad43c4",
   "metadata": {},
   "source": [
    "Oracle Data Science service sample notebook.\n",
    "\n",
    "Copyright (c) 2021, 2022 Oracle, Inc. All rights reserved. Licensed under the [Universal Permissive License v 1.0](https://oss.oracle.com/licenses/upl).\n",
    "\n",
    "---\n",
    "\n",
    "# <font color=\"red\">Partition an Apache Spark DataFrame from an Autonomous Database</font>\n",
    "<p style=\"margin-left:10%; margin-right:10%;\">by the <font color=\"teal\">Oracle Cloud Infrastructure Data Science Service.</font></p>\n",
    "\n",
    "---\n",
    "# Overview:\n",
    "\n",
    "This notebook demonstrates how to extract information from an Oracle Autonomous Database (ADB) and load it into a partitioned PySpark dataframe. It demonstrates two methods to do this. The notebook shows you how to extract an entire ADB table, partition it, and then load it into a PySpark dataframe. Then it demonstrates how to create a partitioned Spark dataframe using a [Data Query Language (DQL)](https://en.wikipedia.org/wiki/Data_query_language) to extract relational data or subsets of data from an ADB.\n",
    "\n",
    "Developed on [PySpark 2.4 and Data Flow](https://docs.oracle.com/iaas/data-science/using/conda-pyspark-fam.htm) for CPU on Python 3.7 (version 3.0)\n",
    "\n",
    "---\n",
    "\n",
    "## Contents:\n",
    "\n",
    "<a href='intro'>Introduction</a>\n",
    "<a href='setup'>Setup</a>\n",
    "  - <a href='setup_vars'>Required Variables</a>\n",
    "  - <a href='#coresite'>`coresite.xml` Configuraton</a>\n",
    "  - <a href='setup_credentials'>Obtain Credentials from the Vault</a>\n",
    "  - <a href='setup_wallet'>Setup the Wallet</a>\n",
    "- <a href='read_adb'>Creating Partitions from a Database</a> \n",
    "  - <a href='read_table'>Partition a Table</a>\n",
    "  - <a href='read_subquery'>Partition a Subquery</a>\n",
    "- <a href='#ref'>References</a>\n",
    "\n",
    "---\n",
    "\n",
    "\n",
    "Datasets are provided as a convenience.  Datasets are considered third-party content and are not considered materials \n",
    "under your agreement with Oracle.\n",
    "    \n",
    "You can access the `SH.SALES` dataset license [here](https://oss.oracle.com/licenses/upl).\n",
    "\n",
    "---\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f308a1fa",
   "metadata": {},
   "outputs": [],
   "source": [
    "import base64\n",
    "import cx_Oracle\n",
    "import oci\n",
    "import os\n",
    "import shutil\n",
    "import tempfile\n",
    "import zipfile\n",
    "\n",
    "from ads.database import connection\n",
    "from ads.vault.vault import Vault\n",
    "from pyspark import SparkConf\n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.functions import spark_partition_id, asc\n",
    "from urllib.parse import urlparse"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eb22208e",
   "metadata": {},
   "source": [
    "<a id='intro'></a>\n",
    "# Introduction\n",
    "\n",
    "Oracle Cloud Infrastructure (OCI) [Data Flow](https://www.oracle.com/big-data/data-flow/) with [PySpark](https://spark.apache.org/docs/latest/api/python/index.html) provides a scalable solution to store and process vast quantities of data. However, the source of truth is often a relational database management system (RDMS) like [ABD](https://www.oracle.com/autonomous-database/). ADB provides advantages like ACID (atomicity, consistency, isolation, durability) compliance, rapid relational joins, support for complex business logic, and more. However, [Apache Spark](http://spark.apache.org/)-based systems provide distributed computing, streaming, graph computation, access to a wide array of machine learning libraries, and more.\n",
    "\n",
    "To improve parallelism in Apache Spark, it is often best to partition the data across the cluster so that the executors do not have to shift data around. Connections between ABD and Apache Spark are made with a [JDBC](https://spark.apache.org/docs/latest/sql-data-sources-jdbc.html) driver. The properties, `partitionColumn`, `lowerBound`, `upperBound`, and `numPartitions` control the behavior of the partitioning. `partitionColumn` is the column in ADB that the data is partitioned on. Since Apache Spark does not have indexes, it is important to choose a partition that improves your specific processing requirements. The `partitionColumn` column must be a numeric, date, or timestamp.\n",
    "\n",
    "`numPartitions` defines the maximum number of partitions that the data is to be split into and therefore the amount of parallelism in the cluster. It also determines the maximum number of JDBC connections.\n",
    "\n",
    "The `lowerBound` and `upperBound` properties are used to define the stride. It is a common misconception that they act as a filter when retrieving data, which is not the case. The stride defines the range of data that is stored in each partition.\n",
    "\n",
    "Since the partition column is a numeric, date, or timestamp, it can be split into strides. The size of a stride is defined by:\n",
    "\n",
    "```python\n",
    "stride = (upperBound / numPartitions) - (lowerBound / numPartitions)\n",
    "```\n",
    "\n",
    "If `upperBound = 1000`, `lowerBound = 0`, and `numPartitions = 10`, then the stride is equals 10.\n",
    "\n",
    "If the following query was run using [SparkSQL](https://spark.apache.org/sql/):\n",
    "\n",
    "```SQL\n",
    "SELECT * FROM table\n",
    "```\n",
    "\n",
    "Assume that `partitionColumn = 'CUST_ID'`, then the query is mutated into the following set of queries to take advantage of the partitions:\n",
    "\n",
    "```SQL\n",
    "SELECT * FROM table WHERE CUST_ID IS NULL or CUST_ID < 100\n",
    "SELECT * FROM table WHERE CUST_ID >= 100 AND CUST_ID < 200\n",
    "SELECT * FROM table WHERE CUST_ID >= 200 AND CUST_ID < 300\n",
    "SELECT * FROM table WHERE CUST_ID >= 300 AND CUST_ID < 400\n",
    "...\n",
    "SELECT * FROM table WHERE CUST_ID >= 800 AND CUST_ID < 900\n",
    "SELECT * FROM table WHERE CUST_ID >= 900\n",
    "```\n",
    "\n",
    "<a id='setup'></a>\n",
    "# Setup\n",
    "\n",
    "An ADB must be configured with permissions read from the preinstalled `SH.SALES` table. The\\is notebook also assumes that the credentials to access the database are stored in the [OCI Vault](https://www.oracle.com/security/cloud-security/key-management/). This is the best practice as it prevents the credentials from being stored locally or in the notebook where they may be accessible to others. If you do not have credentials stored in the Vault, use the `vault.ipynb` example notebook to guide you through the process.\n",
    "\n",
    "In addition to the user credentials, the ADB requires a wallet file. You can obtain the wallet file from your account administrator or download it using the steps that are outlined in [downloading a wallet](https://docs.oracle.com/en-us/iaas/Content/Database/Tasks/adbconnecting.htm#access). The wallet file is a ZIP file. This notebook unzips the wallet and updates the configuration settings so you don't have to.\n",
    "\n",
    "The database connection also needs the TNS name of the database. Your database administrator can give you the TNS name of the database that you have access to.\n",
    "\n",
    "<a id='setup_vars'></a>\n",
    "## Required Variables\n",
    "\n",
    "Update the next cell with the values for these variables.\n",
    "\n",
    "1. `vault_id`, `key_id`, `secret_ocid`: The OCID of the secret by storing the username and password required to connect to your ADB in a secret within the Vault service. The secret is the credential needed to access a database. This notebook is designed so that any secret can be stored as long as it is in a dictionary format. To store your secret, modify the dictionary with your OCID, see the `vault.ipynb` example notebook for detailed steps to generate this OCID.\n",
    "1. `tnsname`: A TNS name valid for the database.\n",
    "1. `wallet_path`: The local path to your wallet ZIP file, see the `autonomous_database.ipynb` example notebook for instructions on accessing the wallet file.\n",
    "<a id='coresite'></a>\n",
    "## `coresite.xml` Configuraton\n",
    "\n",
    "1. Before accessing Oracle Cloud Infrastructure (OCI) Object Storage from your local Apache Spark environment, ensure that you have the `core-site.xml` under `spark_conf_dir` configured properly. It sets the connector properties that are use to connect to Object Storage. The `core-site.xml` file can be configured in the terminal with the following command: ``odsc core-site config -o``\n",
    "1. Before creating applications in the OCI Data Flow service, ensure that you have configured your tenancy for that service. To do this configuration, follow the steps in the [Data Flow documentation](https://docs.cloud.oracle.com/en-us/iaas/data-flow/using/dfs_getting_started.htm#getting_started)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "53301ecd",
   "metadata": {},
   "outputs": [],
   "source": [
    "vault_id = \"<vault_id>\"\n",
    "key_id = \"<key_id>\"\n",
    "secret_ocid = \"<secret_ocid>\"\n",
    "tnsname = \"<tnsname>\"\n",
    "wallet_path = \"<wallet_path>\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "45fe7020",
   "metadata": {},
   "outputs": [],
   "source": [
    "if vault_id != \"<vault_id>\" and key_id != \"<key_id>\" and secret_ocid != \"<secret_ocid>\":\n",
    "    print(\"Getting wallet username and password\")\n",
    "    vault = Vault(vault_id=vault_id, key_id=key_id)\n",
    "    adb_creds = vault.get_secret(secret_ocid)\n",
    "    user = adb_creds[\"username\"]\n",
    "    password = adb_creds[\"password\"]\n",
    "else:\n",
    "    print(\n",
    "        \"Skipping as it appears that you do not have vault, key, and secret ocid specified.\"\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0e909c1f",
   "metadata": {},
   "source": [
    "<a id='setup_credentials'></a>\n",
    "## Obtain Credentials from the Vault\n",
    "\n",
    "The approach assumes that the Accelerated Data Science (ADS) library was used to store the secret. If the `vault_id`, `key_id`, and `secret_id` variables have been updated, then the notebook obtains a handle to the Vault with the `vault` variable. This uses the `get_secret()` method to return a dictionary with the user credentials. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "43e692fb",
   "metadata": {},
   "source": [
    "<a id='setup_wallet'></a>\n",
    "## Setup the Wallet\n",
    "\n",
    "An ADB requires a wallet file to access the database. The `wallet_path` variable defines the location of this file. The next cell prepares the wallet file to make a connection to the database. It also creates the ADB connection string, `adb_url`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8a400f2c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def setup_wallet(wallet_path):\n",
    "    \"\"\"\n",
    "    Prepare ADB wallet file for use in PySpark.\n",
    "    \"\"\"\n",
    "\n",
    "    temporary_directory = tempfile.mkdtemp()\n",
    "    zip_file_path = os.path.join(temporary_directory, \"wallet.zip\")\n",
    "\n",
    "    # Extract everything locally.\n",
    "    with zipfile.ZipFile(wallet_path, \"r\") as zip_ref:\n",
    "        zip_ref.extractall(temporary_directory)\n",
    "\n",
    "    return temporary_directory\n",
    "\n",
    "\n",
    "if wallet_path != \"<wallet_path>\":\n",
    "    print(\"Setting up wallet\")\n",
    "    tns_path = setup_wallet(wallet_path)\n",
    "else:\n",
    "    print(\"Skipping as it appears that you do not have wallet_path specified.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8e8233e0",
   "metadata": {},
   "outputs": [],
   "source": [
    "if \"tns_path\" in globals() and tnsname != \"<tnsname>\":\n",
    "    adb_url = f\"jdbc:oracle:thin:@{tnsname}?TNS_ADMIN={tns_path}\"\n",
    "else:\n",
    "    print(\"Skipping, as the tns_path or tnsname are not defined.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fd6265a8",
   "metadata": {},
   "source": [
    "<a id='read_adb'></a>\n",
    "# Creating Partitions from a Database\n",
    "\n",
    "The next cell creates an Apache Spark application that pullS data from the `SH.SALES` dataset. This table is built into the ADB. It demonstrates how to use the `dbtable` property in the [JDBC](https://spark.apache.org/docs/2.4.0/sql-data-sources-jdbc.html) driver along with the `partitionColumn`, `lowerBound`, `upperBound`, and `numPartitions` properties to partition the data.\n",
    "\n",
    "The `dbtable` accepts anything that is allowed in a `FROM` clause. This can be the full table name of a subquery.\n",
    "\n",
    "The next cell creates a PySpark application:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7f9b48d8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# create a spark session\n",
    "sc = SparkSession.builder.appName(\"PySpark Partition Example\").getOrCreate()\n",
    "sc.sparkContext.setLogLevel(\"ERROR\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "90832e50",
   "metadata": {},
   "source": [
    "<a id='read_table'></a>\n",
    "## Partition a Table\n",
    "\n",
    "You can partition the entire `SH.SALES` table into 10 partitions based on the `CUST_ID` column. The `read()` method is used to read from the ADB and the `option(\"dbtable\", \"SH.SALES\")` is used to select the entire database table for import into PySpark.\n",
    "\n",
    "There are 918,843 records in this dataset and multiple records per customer. The distribution of `CUST_ID` is not uniform, which poses a challenge to balance the partition sizes. Ideally, you want an equal number of records in each partition. The `lowerBound` is set to 0. The `upperBound` is set to 14,000. These numbers are chosen because it does a reasonable job of splitting the records across the partitions so that they are roughly balanced."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "21a0dfe9",
   "metadata": {},
   "outputs": [],
   "source": [
    "if \"adb_url\" in globals():\n",
    "    sales_table = (\n",
    "        sc.read.format(\"jdbc\")\n",
    "        .option(\"url\", adb_url)\n",
    "        .option(\"dbtable\", \"SH.SALES\")\n",
    "        .option(\"partitionColumn\", \"CUST_ID\")\n",
    "        .option(\"lowerBound\", 0)\n",
    "        .option(\"upperBound\", 14000)\n",
    "        .option(\"numPartitions\", 10)\n",
    "        .option(\"user\", user)\n",
    "        .option(\"password\", password)\n",
    "        .load()\n",
    "    )\n",
    "else:\n",
    "    print(\"Skipping as it appears that you do not have adb_url configured.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d85ab2bc",
   "metadata": {},
   "source": [
    "Display a subset of the data:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "405ccd47",
   "metadata": {},
   "outputs": [],
   "source": [
    "if \"sales_table\" in globals():\n",
    "    sales_table.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "11b30912",
   "metadata": {},
   "source": [
    "Display the number of partitions in the dataframe:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b11ae2c2",
   "metadata": {},
   "outputs": [],
   "source": [
    "if \"sales_table\" in globals():\n",
    "    print(f\"There are {sales_table.rdd.getNumPartitions()} partitions.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a14f6d88",
   "metadata": {},
   "source": [
    "Count the number of records in each partition:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a13a7e43",
   "metadata": {},
   "outputs": [],
   "source": [
    "if \"sales_table\" in globals():\n",
    "    sales_table.withColumn(\"partitionId\", spark_partition_id()).groupBy(\n",
    "        \"partitionId\"\n",
    "    ).count().orderBy(asc(\"partitionId\")).show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "88f45941",
   "metadata": {},
   "source": [
    "<a id='read_subquery'></a>\n",
    "## Partition a Subquery\n",
    "\n",
    "You can partition the entire subquery into 10 partitions based on the `CUST_ID` column. Again, the `dbtable` option is used on the `read()` method. This example uses `option(\"dbtable\", <SUBQUERY>)`, which contains a subquery. The subquery is an SQL command that returns a record set and it must be wrapped in parathesis.\n",
    "\n",
    "In this example, the subquery limits the number of columns and the number of rows that it returns. The subquery is given by:\n",
    "\n",
    "```SQL\n",
    "(SELECT AMOUNT_SOLD, CUST_ID FROM SH.SALES WHERE CUST_ID < 1000)\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a5fcee85",
   "metadata": {},
   "outputs": [],
   "source": [
    "subquery = \"(SELECT AMOUNT_SOLD, CUST_ID FROM SH.SALES WHERE CUST_ID < 1000)\"\n",
    "if \"adb_url\" in globals():\n",
    "    sales_subquery = (\n",
    "        sc.read.format(\"jdbc\")\n",
    "        .option(\"url\", adb_url)\n",
    "        .option(\"dbtable\", subquery)\n",
    "        .option(\"partitionColumn\", \"CUST_ID\")\n",
    "        .option(\"lowerBound\", 0)\n",
    "        .option(\"upperBound\", 1000)\n",
    "        .option(\"numPartitions\", 10)\n",
    "        .option(\"user\", user)\n",
    "        .option(\"password\", password)\n",
    "        .load()\n",
    "    )\n",
    "else:\n",
    "    print(\"Skipping as it appears that you do not have adb_url configured.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c482ecdc",
   "metadata": {},
   "source": [
    "Display a subset of the data. Only the `AMOUNT_SOLD` and `CUST_ID` are returned:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "92b029c0",
   "metadata": {},
   "outputs": [],
   "source": [
    "if \"sales_subquery\" in globals():\n",
    "    sales_table.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d78c017f",
   "metadata": {},
   "source": [
    "Display the number of partitions in the dataframe:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7dea1752",
   "metadata": {},
   "outputs": [],
   "source": [
    "if \"sales_subquery\" in globals():\n",
    "    print(f\"There are {sales_table.rdd.getNumPartitions()} partitions.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bf4e80cb",
   "metadata": {},
   "source": [
    "Count the number of records in each partition:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "35c6d5ce",
   "metadata": {},
   "outputs": [],
   "source": [
    "if \"sales_subquery\" in globals():\n",
    "    sales_table.withColumn(\"partitionId\", spark_partition_id()).groupBy(\n",
    "        \"partitionId\"\n",
    "    ).count().orderBy(asc(\"partitionId\")).show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "78352525",
   "metadata": {},
   "source": [
    "Stop the PySpark Cluster:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "810acfa0",
   "metadata": {},
   "outputs": [],
   "source": [
    "sc.stop()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9291aac9",
   "metadata": {},
   "source": [
    "<a id='ref'></a>\n",
    "# References\n",
    "\n",
    "- [ACID](https://en.wikipedia.org/wiki/ACID)\n",
    "- [ADS Library Documentation](https://docs.cloud.oracle.com/en-us/iaas/tools/ads-sdk/latest/index.html)\n",
    "- [Apache Spark](http://spark.apache.org/)\n",
    "- [Data Science YouTube Videos](https://www.youtube.com/playlist?list=PLKCk3OyNwIzv6CWMhvqSB_8MLJIZdO80L)\n",
    "- [Downloading a wallet](https://docs.oracle.com/en-us/iaas/Content/Database/Tasks/adbconnecting.htm#access)\n",
    "- [JDBC driver](https://spark.apache.org/docs/latest/sql-data-sources-jdbc.html)\n",
    "- [OCI Data Flow service](https://www.oracle.com/big-data/data-flow/)\n",
    "- [OCI Data Science Documentation](https://docs.cloud.oracle.com/en-us/iaas/data-science/using/data-science.htm)\n",
    "- [OCI Vault service](https://www.oracle.com/security/cloud-security/key-management/)\n",
    "- [Oracle Autonomous Database (ABD)](https://www.oracle.com/autonomous-database/)\n",
    "- [Oracle Data & AI Blog](https://blogs.oracle.com/datascience/)\n",
    "- [PySpark](https://spark.apache.org/docs/latest/api/python/index.html)\n",
    "- [Relational database management system (RDMS)](https://en.wikipedia.org/wiki/Relational_database)\n",
    "- [SparkSQL](https://spark.apache.org/sql/)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.9.6 64-bit",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.9"
  },
  "vscode": {
   "interpreter": {
    "hash": "aee8b7b246df8f9039afb4144a1f6fd8d2ca17a180786b69acc140d282b71a49"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
