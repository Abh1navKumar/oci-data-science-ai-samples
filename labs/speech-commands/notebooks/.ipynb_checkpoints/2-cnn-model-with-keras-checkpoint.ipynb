{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<font color=gray>Oracle Cloud Infrastructure Data Science Demo Notebook\n",
    "\n",
    "Copyright (c) 2021 Oracle, Inc.<br>\n",
    "Licensed under the Universal Permissive License v 1.0 as shown at https://oss.oracle.com/licenses/upl.\n",
    "</font>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***\n",
    "# <font> Speech Command Recognition Task with Keras </font>\n",
    "<p style=\"margin-left:10%; margin-right:10%;\">by the <font color=teal> OCI Data Science PM Team </font></p>\n",
    "\n",
    "***"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this notebook, we will train a simple convolutional neural network (CNN) to recognize utterances of different words. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![banner](https://user-images.githubusercontent.com/5395649/46774810-22395980-ccb9-11e8-8f1a-535769d657ec.png)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "absl-py==0.12.0\n",
      "alabaster==0.7.12\n",
      "anaconda-client==1.7.2\n",
      "anaconda-navigator==1.10.0\n",
      "anaconda-project==0.8.3\n",
      "applaunchservices==0.2.1\n",
      "appnope @ file:///opt/concourse/worker/volumes/live/0291c9e1-4b15-459f-623e-2770f55be269/volume/appnope_1594338395037/work\n",
      "appscript @ file:///opt/concourse/worker/volumes/live/50ca4c96-3090-40bb-6981-3a6114ed0af4/volume/appscript_1594840187551/work\n",
      "argh==0.26.2\n",
      "argon2-cffi @ file:///opt/concourse/worker/volumes/live/59af29ac-4890-416e-7ab7-794f8d6f7ecd/volume/argon2-cffi_1596828548321/work\n",
      "asn1crypto @ file:///tmp/build/80754af9/asn1crypto_1596577642040/work\n",
      "association-metrics==0.0.1\n",
      "astroid @ file:///opt/concourse/worker/volumes/live/21fd14a9-2a7e-484b-7394-5a9912cdcf80/volume/astroid_1592498459180/work\n",
      "astropy==4.0.2\n",
      "astunparse==1.6.3\n",
      "async-generator==1.10\n",
      "atomicwrites==1.4.0\n",
      "attrs @ file:///tmp/build/80754af9/attrs_1604765588209/work\n",
      "audioread==2.1.9\n",
      "autodoc==0.5.0\n",
      "autopep8 @ file:///tmp/build/80754af9/autopep8_1596578164842/work\n",
      "Babel @ file:///tmp/build/80754af9/babel_1605108370292/work\n",
      "backcall==0.2.0\n",
      "backports.functools-lru-cache==1.6.1\n",
      "backports.shutil-get-terminal-size==1.0.0\n",
      "backports.tempfile==1.0\n",
      "backports.weakref==1.0.post1\n",
      "beautifulsoup4 @ file:///tmp/build/80754af9/beautifulsoup4_1601924105527/work\n",
      "bitarray @ file:///opt/concourse/worker/volumes/live/fdfca23e-4dd8-48f7-512d-c4f3db552eeb/volume/bitarray_1605065128338/work\n",
      "bkcharts==0.2\n",
      "bleach @ file:///tmp/build/80754af9/bleach_1600439572647/work\n",
      "bokeh @ file:///opt/concourse/worker/volumes/live/b2253281-9b72-4dcb-624e-e22924b50435/volume/bokeh_1603297849453/work\n",
      "boto==2.49.0\n",
      "Bottleneck==1.3.2\n",
      "brotlipy==0.7.0\n",
      "cachetools==4.2.2\n",
      "certifi==2020.6.20\n",
      "cffi @ file:///opt/concourse/worker/volumes/live/b9607b09-b777-4ff7-53dc-287727eb8574/volume/cffi_1600699191154/work\n",
      "chardet==3.0.4\n",
      "click==7.1.2\n",
      "cloudpickle @ file:///tmp/build/80754af9/cloudpickle_1598884132938/work\n",
      "clyent==1.2.2\n",
      "colorama @ file:///tmp/build/80754af9/colorama_1603211150991/work\n",
      "conda==4.9.2\n",
      "conda-build==3.20.5\n",
      "conda-pack==0.5.0\n",
      "conda-package-handling @ file:///opt/concourse/worker/volumes/live/a7e34989-4c54-4cb6-4156-4e58ee270730/volume/conda-package-handling_1603018121300/work\n",
      "conda-verify==3.4.2\n",
      "ConfigArgParse==1.3\n",
      "configparser==4.0.2\n",
      "contextlib2==0.6.0.post1\n",
      "cryptography==3.3.2\n",
      "cycler==0.10.0\n",
      "Cython @ file:///opt/concourse/worker/volumes/live/6158b663-a4ca-4e19-7e05-8807e4f79146/volume/cython_1594835048880/work\n",
      "cytoolz==0.11.0\n",
      "dask @ file:///tmp/build/80754af9/dask-core_1602083700509/work\n",
      "decorator==4.4.2\n",
      "defusedxml==0.6.0\n",
      "dict2xml==1.7.0\n",
      "diff-match-patch @ file:///tmp/build/80754af9/diff-match-patch_1594828741838/work\n",
      "distributed @ file:///opt/concourse/worker/volumes/live/bd66aa48-5cf5-4b60-6ed4-f204fff153f6/volume/distributed_1605066538557/work\n",
      "dnspython==1.16.0\n",
      "docker==4.4.1\n",
      "docopt==0.6.2\n",
      "docutils==0.16\n",
      "entrypoints==0.3\n",
      "et-xmlfile==1.0.1\n",
      "fastcache==1.1.0\n",
      "fett==0.3.2\n",
      "filelock==3.0.12\n",
      "flake8 @ file:///tmp/build/80754af9/flake8_1601911421857/work\n",
      "Flask==1.1.2\n",
      "flatbuffers==1.12\n",
      "fsspec @ file:///tmp/build/80754af9/fsspec_1602684995936/work\n",
      "future==0.18.2\n",
      "gast==0.4.0\n",
      "gevent @ file:///opt/concourse/worker/volumes/live/e6b243ce-c4b8-40bb-4934-ef3bf1c512f2/volume/gevent_1601397552921/work\n",
      "glob2==0.7\n",
      "gmpy2==2.0.8\n",
      "google-auth==1.30.1\n",
      "google-auth-oauthlib==0.4.4\n",
      "google-pasta==0.2.0\n",
      "greenlet @ file:///opt/concourse/worker/volumes/live/02d5d57d-1f11-4cf9-580a-19e679c78dc9/volume/greenlet_1600874049903/work\n",
      "grpcio==1.34.1\n",
      "h5py==2.10.0\n",
      "HeapDict==1.0.1\n",
      "html5lib @ file:///tmp/build/80754af9/html5lib_1593446221756/work\n",
      "idna @ file:///tmp/build/80754af9/idna_1593446292537/work\n",
      "imageio @ file:///tmp/build/80754af9/imageio_1594161405741/work\n",
      "imagesize==1.2.0\n",
      "importlib-metadata @ file:///tmp/build/80754af9/importlib-metadata_1602276842396/work\n",
      "iniconfig @ file:///tmp/build/80754af9/iniconfig_1602780191262/work\n",
      "intervaltree @ file:///tmp/build/80754af9/intervaltree_1598376443606/work\n",
      "ipykernel @ file:///opt/concourse/worker/volumes/live/88f541d3-5a27-498f-7391-f2e50ca36560/volume/ipykernel_1596206680118/work/dist/ipykernel-5.3.4-py3-none-any.whl\n",
      "ipython @ file:///opt/concourse/worker/volumes/live/26969e8f-c9f7-42dc-6ffb-b3effd424c49/volume/ipython_1604101242376/work\n",
      "ipython-genutils==0.2.0\n",
      "ipywidgets @ file:///tmp/build/80754af9/ipywidgets_1601490159889/work\n",
      "isort @ file:///tmp/build/80754af9/isort_1602603989581/work\n",
      "itsdangerous==1.1.0\n",
      "jdcal==1.4.1\n",
      "jedi @ file:///opt/concourse/worker/volumes/live/1c5c293b-9147-4b4b-5a7f-d3f5eddb8470/volume/jedi_1592841952519/work\n",
      "Jinja2==2.11.2\n",
      "joblib @ file:///tmp/build/80754af9/joblib_1601912903842/work\n",
      "json5==0.9.5\n",
      "jsonschema @ file:///tmp/build/80754af9/jsonschema_1602607155483/work\n",
      "jupyter==1.0.0\n",
      "jupyter-client @ file:///tmp/build/80754af9/jupyter_client_1601311786391/work\n",
      "jupyter-console @ file:///tmp/build/80754af9/jupyter_console_1598884538475/work\n",
      "jupyter-core==4.6.3\n",
      "jupyterlab==2.2.6\n",
      "jupyterlab-pygments @ file:///tmp/build/80754af9/jupyterlab_pygments_1601490720602/work\n",
      "jupyterlab-server @ file:///tmp/build/80754af9/jupyterlab_server_1594164409481/work\n",
      "Keras==2.2.5\n",
      "Keras-Applications==1.0.8\n",
      "keras-nightly==2.5.0.dev2021032900\n",
      "Keras-Preprocessing==1.1.2\n",
      "keyring @ file:///opt/concourse/worker/volumes/live/54fc3ec2-338b-44f5-5e13-d62afa6b5820/volume/keyring_1601490916376/work\n",
      "kiwisolver @ file:///opt/concourse/worker/volumes/live/b8936fa6-0e4b-47e7-4fb4-e02dbd4505ee/volume/kiwisolver_1604014598721/work\n",
      "lazy-object-proxy==1.4.3\n",
      "libarchive-c==2.9\n",
      "librosa==0.6.2\n",
      "llvmlite==0.32.1\n",
      "locket==0.2.0\n",
      "lxml @ file:///opt/concourse/worker/volumes/live/9351a723-931c-40fa-7baa-f2f468cdccf6/volume/lxml_1603216287330/work\n",
      "Markdown==3.3.3\n",
      "MarkupSafe @ file:///opt/concourse/worker/volumes/live/cb778296-98db-45ad-411e-6f726e102dc3/volume/markupsafe_1594371638608/work\n",
      "matplotlib @ file:///opt/concourse/worker/volumes/live/f7797860-f8aa-410c-4a56-72315954816b/volume/matplotlib-base_1603378002957/work\n",
      "mccabe==0.6.1\n",
      "mistune @ file:///opt/concourse/worker/volumes/live/95802d64-d39c-491b-74ce-b9326880ca54/volume/mistune_1594373201816/work\n",
      "mkl-fft==1.2.0\n",
      "mkl-random==1.1.1\n",
      "mkl-service==2.3.0\n",
      "mock==4.0.2\n",
      "more-itertools @ file:///tmp/build/80754af9/more-itertools_1605111547926/work\n",
      "mpmath==1.1.0\n",
      "msgpack==1.0.0\n",
      "multipledispatch==0.6.0\n",
      "navigator-updater==0.2.1\n",
      "nbclient @ file:///tmp/build/80754af9/nbclient_1602783176460/work\n",
      "nbconvert @ file:///opt/concourse/worker/volumes/live/2b9c1d93-d0fd-432f-7d93-66c93d81b614/volume/nbconvert_1601914875037/work\n",
      "nbformat @ file:///tmp/build/80754af9/nbformat_1602783287752/work\n",
      "nest-asyncio @ file:///tmp/build/80754af9/nest-asyncio_1605115881283/work\n",
      "networkx @ file:///tmp/build/80754af9/networkx_1598376031484/work\n",
      "nltk @ file:///tmp/build/80754af9/nltk_1592496090529/work\n",
      "nose @ file:///opt/concourse/worker/volumes/live/a029938e-1732-4cd8-5b98-0542283d158b/volume/nose_1594377915100/work\n",
      "notebook @ file:///opt/concourse/worker/volumes/live/be0f3504-189d-4bae-4e57-c5d6da73ffcd/volume/notebook_1601501605350/work\n",
      "numba==0.49.1\n",
      "numexpr==2.7.1\n",
      "numpy @ file:///opt/concourse/worker/volumes/live/5572694e-967a-4c0c-52cf-b53d43e72de9/volume/numpy_and_numpy_base_1603491881791/work\n",
      "numpydoc @ file:///tmp/build/80754af9/numpydoc_1605117425582/work\n",
      "oauthlib==3.1.1\n",
      "oci==2.39.0\n",
      "odsc-cli==0+untagged.860.g4739bae.dirty\n",
      "olefile==0.46\n",
      "openpyxl @ file:///tmp/build/80754af9/openpyxl_1598113097404/work\n",
      "opt-einsum==3.3.0\n",
      "packaging==20.4\n",
      "pandas @ file:///opt/concourse/worker/volumes/live/f14cf8c4-c564-4eff-4b17-158e90dbf88a/volume/pandas_1602088128240/work\n",
      "pandocfilters @ file:///opt/concourse/worker/volumes/live/c330e404-216d-466b-5327-8ce8fe854d3a/volume/pandocfilters_1605120442288/work\n",
      "parquet==1.3.1\n",
      "parso==0.7.0\n",
      "partd==1.1.0\n",
      "path @ file:///opt/concourse/worker/volumes/live/fcdf620c-46d6-4284-4c1e-5b8c3bc6c5c6/volume/path_1596907417277/work\n",
      "pathlib2 @ file:///opt/concourse/worker/volumes/live/de518564-0d9f-405e-472b-38136f0c2169/volume/pathlib2_1594381084269/work\n",
      "pathtools==0.1.2\n",
      "patsy==0.5.1\n",
      "pep8==1.7.1\n",
      "pexpect @ file:///opt/concourse/worker/volumes/live/8701bb20-ad87-46c7-5108-30c178cf97e5/volume/pexpect_1594383388344/work\n",
      "pickleshare @ file:///opt/concourse/worker/volumes/live/93ec39d8-05bb-4f84-7efc-98735bc39b70/volume/pickleshare_1594384101884/work\n",
      "Pillow @ file:///opt/concourse/worker/volumes/live/991b9a87-3372-4acd-45f9-eaa52701f03c/volume/pillow_1603822262543/work\n",
      "pkginfo==1.6.1\n",
      "pluggy==0.13.1\n",
      "ply==3.11\n",
      "pockets==0.9.1\n",
      "prometheus-client==0.8.0\n",
      "prompt-toolkit @ file:///tmp/build/80754af9/prompt-toolkit_1602688806899/work\n",
      "protobuf==3.17.2\n",
      "psutil @ file:///opt/concourse/worker/volumes/live/ff72f822-991c-4030-4f3a-8c41d3ac4e4f/volume/psutil_1598370232375/work\n",
      "ptyprocess==0.6.0\n",
      "py @ file:///tmp/build/80754af9/py_1593446248552/work\n",
      "pyasn1==0.4.8\n",
      "pyasn1-modules==0.2.8\n",
      "pycodestyle==2.6.0\n",
      "pycosat==0.6.3\n",
      "pycparser @ file:///tmp/build/80754af9/pycparser_1594388511720/work\n",
      "pycurl==7.43.0.6\n",
      "pydocstyle @ file:///tmp/build/80754af9/pydocstyle_1598885001695/work\n",
      "pyflakes==2.2.0\n",
      "Pygments @ file:///tmp/build/80754af9/pygments_1604103097372/work\n",
      "pylint @ file:///opt/concourse/worker/volumes/live/ed0164b6-bcc7-4f6b-7dd4-ad89660b5dcb/volume/pylint_1598624018129/work\n",
      "pymongo==3.11.3\n",
      "pyodbc===4.0.0-unsupported\n",
      "pyOpenSSL @ file:///tmp/build/80754af9/pyopenssl_1594392929924/work\n",
      "pyparsing==2.4.7\n",
      "pyrsistent @ file:///opt/concourse/worker/volumes/live/ff11f3f0-615b-4508-471d-4d9f19fa6657/volume/pyrsistent_1600141727281/work\n",
      "PySocks @ file:///opt/concourse/worker/volumes/live/85a5b906-0e08-41d9-6f59-084cee4e9492/volume/pysocks_1594394636991/work\n",
      "pytest==0.0.0\n",
      "python-dateutil==2.8.1\n",
      "python-jsonrpc-server==0.3.4\n",
      "python-language-server @ file:///tmp/build/80754af9/python-language-server_1600454544709/work\n",
      "python-snappy==0.6.0\n",
      "pytz==2020.1\n",
      "PyWavelets @ file:///opt/concourse/worker/volumes/live/ea36e10f-66e8-43ae-511e-c4092764493f/volume/pywavelets_1601658378672/work\n",
      "PyYAML==5.3.1\n",
      "pyzmq==19.0.2\n",
      "QDarkStyle==2.8.1\n",
      "QtAwesome @ file:///tmp/build/80754af9/qtawesome_1602272867890/work\n",
      "qtconsole @ file:///tmp/build/80754af9/qtconsole_1600870028330/work\n",
      "QtPy==1.9.0\n",
      "regex @ file:///opt/concourse/worker/volumes/live/7f106f75-0e11-45be-4c20-6b071e37c646/volume/regex_1602786678165/work\n",
      "requests @ file:///tmp/build/80754af9/requests_1592841827918/work\n",
      "requests-oauthlib==1.3.0\n",
      "resampy==0.2.2\n",
      "rope @ file:///tmp/build/80754af9/rope_1602264064449/work\n",
      "rsa==4.7.2\n",
      "rstcheck==3.3.1\n",
      "Rtree==0.9.4\n",
      "ruamel-yaml==0.15.87\n",
      "scikit-image==0.17.2\n",
      "scikit-learn @ file:///opt/concourse/worker/volumes/live/111833a2-339b-4578-413b-7337bb8fe64a/volume/scikit-learn_1598376920601/work\n",
      "scipy @ file:///opt/concourse/worker/volumes/live/851446f6-a052-41c4-4243-67bb78999b49/volume/scipy_1604596178167/work\n",
      "seaborn @ file:///tmp/build/80754af9/seaborn_1600553570093/work\n",
      "Send2Trash==1.5.0\n",
      "service-pack==0+untagged.1013.gf3211f0.dirty\n",
      "simplegeneric==0.8.1\n",
      "singledispatch @ file:///tmp/build/80754af9/singledispatch_1602523705405/work\n",
      "six @ file:///opt/concourse/worker/volumes/live/5b31cb27-1e37-4ca5-6e9f-86246eb206d2/volume/six_1605205320872/work\n",
      "snooty-lextudio==1.8.10.dev0\n",
      "snowballstemmer==2.0.0\n",
      "sortedcollections==1.2.1\n",
      "sortedcontainers==2.2.2\n",
      "soupsieve==2.0.1\n",
      "Sphinx @ file:///tmp/build/80754af9/sphinx_1597428793432/work\n",
      "sphinx-rtd-theme==0.5.1\n",
      "sphinxcontrib-applehelp==1.0.2\n",
      "sphinxcontrib-devhelp==1.0.2\n",
      "sphinxcontrib-htmlhelp==1.0.3\n",
      "sphinxcontrib-jsmath==1.0.1\n",
      "sphinxcontrib-napoleon==0.7\n",
      "sphinxcontrib-qthelp==1.0.3\n",
      "sphinxcontrib-serializinghtml==1.1.4\n",
      "sphinxcontrib-websupport @ file:///tmp/build/80754af9/sphinxcontrib-websupport_1597081412696/work\n",
      "spyder @ file:///opt/concourse/worker/volumes/live/93f52c11-6bc0-49a8-541e-aa5e1de1eadc/volume/spyder_1599056974853/work\n",
      "spyder-kernels @ file:///opt/concourse/worker/volumes/live/b4ec5b57-5b3c-42d0-7731-c0691f88ee81/volume/spyder-kernels_1599056790993/work\n",
      "SQLAlchemy @ file:///opt/concourse/worker/volumes/live/0214475e-3c0a-49a9-6cb8-ab2d5c945bef/volume/sqlalchemy_1603812264100/work\n",
      "statsmodels @ file:///opt/concourse/worker/volumes/live/148a0e6d-2163-4103-6ef5-61556693c052/volume/statsmodels_1602280229372/work\n",
      "sympy @ file:///opt/concourse/worker/volumes/live/d5d0b33b-5c2f-493b-5b67-8149e5531868/volume/sympy_1605119535834/work\n",
      "tables==3.6.1\n",
      "tblib @ file:///tmp/build/80754af9/tblib_1597928476713/work\n",
      "tensorboard==2.5.0\n",
      "tensorboard-data-server==0.6.1\n",
      "tensorboard-plugin-wit==1.8.0\n",
      "tensorflow==2.5.0\n",
      "tensorflow-estimator==2.5.0\n",
      "termcolor==1.1.0\n",
      "terminado==0.9.1\n",
      "testpath==0.4.4\n",
      "threadpoolctl @ file:///tmp/tmp9twdgx9k/threadpoolctl-2.1.0-py3-none-any.whl\n",
      "thriftpy2==0.4.12\n",
      "tifffile==2020.10.1\n",
      "toml @ file:///tmp/build/80754af9/toml_1592853716807/work\n",
      "toolz @ file:///tmp/build/80754af9/toolz_1601054250827/work\n",
      "tornado==6.0.4\n",
      "tqdm @ file:///tmp/build/80754af9/tqdm_1602185206534/work\n",
      "traitlets @ file:///tmp/build/80754af9/traitlets_1602787416690/work\n",
      "typing-extensions @ file:///tmp/build/80754af9/typing_extensions_1598376058250/work\n",
      "ujson==1.35\n",
      "unicodecsv==0.14.1\n",
      "urllib3 @ file:///tmp/build/80754af9/urllib3_1603305693037/work\n",
      "versioneer==0.19\n",
      "waitress==1.4.4\n",
      "watchdog==1.0.2\n",
      "wcwidth @ file:///tmp/build/80754af9/wcwidth_1593447189090/work\n",
      "webencodings==0.5.1\n",
      "WebOb==1.8.7\n",
      "websocket-client==0.57.0\n",
      "WebTest==2.0.35\n",
      "Werkzeug==1.0.1\n",
      "widgetsnbextension==3.5.1\n",
      "wrapt==1.12.1\n",
      "wurlitzer @ file:///opt/concourse/worker/volumes/live/01a17f3d-eafe-4806-57a1-4b9ef5d1815f/volume/wurlitzer_1594753845129/work\n",
      "xlrd==1.2.0\n",
      "XlsxWriter @ file:///tmp/build/80754af9/xlsxwriter_1602692860603/work\n",
      "xlwings==0.20.8\n",
      "xlwt==1.3.0\n",
      "xmltodict==0.12.0\n",
      "yapf @ file:///tmp/build/80754af9/yapf_1593528177422/work\n",
      "zict==2.0.0\n",
      "zipp @ file:///tmp/build/80754af9/zipp_1604001098328/work\n",
      "zope.event==4.5.0\n",
      "zope.interface @ file:///opt/concourse/worker/volumes/live/de428e3b-00ba-4161-442e-b9e5d25e4219/volume/zope.interface_1602002489816/work\n"
     ]
    }
   ],
   "source": [
    "!pip freeze"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We'll first start by installing some libraries that are not installed in the notebook session environment. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%bash \n",
    "\n",
    "pip install -r requirements.txt > .install_logs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Also upgrade the version of the OCI Python SDK that is installed in your conda environment. The new version will include support for Model Deployment. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install --upgrade oci"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Re-start the kernel and start from here: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "import keras \n",
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "import librosa\n",
    "import os \n",
    "import hashlib \n",
    "import re\n",
    "import pandas as pd \n",
    "import IPython.display as ipd\n",
    "from sklearn.metrics import confusion_matrix\n",
    "from tqdm import tqdm \n",
    "import pickle as pkl \n",
    "from seaborn import heatmap \n",
    "\n",
    "#from preprocess import *\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense, Dropout, Flatten, Conv2D, MaxPooling2D\n",
    "from keras.utils import to_categorical\n",
    "\n",
    "from ads.common.model import prepare_generic_model\n",
    "import ads\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "import logging\n",
    "logging.basicConfig(format='%(levelname)s:%(message)s', level=logging.ERROR)\n",
    "import tensorflow as tf \n",
    "tf.logging.set_verbosity(tf.logging.ERROR)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[librosa](https://librosa.github.io/librosa/) is an interesting python library for audio analysis. The paper describing the library can be found [here](http://conference.scipy.org/proceedings/scipy2015/pdfs/brian_mcfee.pdf)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Before we get started... \n",
    "\n",
    "* **If you are unfamiliar with signal processing in general, we recommend you review the notebook `1-intro-to-audio-data.ipynb`. You need to run this notebook first to download the data to your notebook session**\n",
    "\n",
    "* You don't need the raw data to run this notebook. You will be able to load a pre-processed dataset that is stored in this repo. This dataset will work for all default parameters provided in this notebook. \n",
    "\n",
    "Let us define some variables that will be used throughout the notebook. \n",
    "\n",
    "* `data_dir` points to the `wav` folder of the dataset. On the platform, this means `/home/datascience/data/`.\n",
    "* `MAX_NUM_WAVS_PER_CLASS` is the maximum number of audio clip examples per word class. Leave it as is for now.\n",
    "\n",
    "## A few words on the dataset \n",
    "\n",
    "The dataset we use is the [Speech Commands Dataset](https://arxiv.org/abs/1804.03209). This dataset includes 105k utterances of 35 words. Each word has its corresponding folder containing between 1k and 4k examples. In total, **over 2K different speakers were recorded**. Each clip lasts 1 second at a sampling rate of 16 kHz. The uncompressed data take about 4Gb on disk. \n",
    "\n",
    "The dataset is licensed under the [Creative Commons BY 4.0 license](https://creativecommons.org/licenses/by/4.0/). See the LICENSE file in the dataset folder for full details. Its original location was at\n",
    "[http://download.tensorflow.org/data/speech_commands_v0.02.tar.gz](http://download.tensorflow.org/data/speech_commands_v0.02.tar.gz)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "# the root directory of the dataset. If you are running this notebook on the Platform, this\n",
    "# folder should be /home/datascience/data/\n",
    "data_dir = '../data/'\n",
    "\n",
    "MAX_NUM_WAVS_PER_CLASS = 2**27 - 1  # ~134M\n",
    "\n",
    "# Training, validation and testing data split. An 80-10-10% split is standard. \n",
    "training_fraction = 80.0\n",
    "validation_fraction = 10.0\n",
    "testing_fraction = 10.0 \n",
    "\n",
    "# If you have not downloaded the original dataset and you want to \n",
    "# use the pre-processed, transformed dataset, set the flag \"list_available_words\" to False\n",
    "list_available_words = False \n",
    "\n",
    "if list_available_words: \n",
    "    # Lets find what words are available for us to classify: \n",
    "    words_available = [ d for d in os.listdir(data_dir) \n",
    "                   if os.path.isdir(os.path.join(data_dir, d)) ] \n",
    "\n",
    "    # This is the list of available utterances one can use in their classifier\n",
    "    print(words_available)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For this particular model, **we will train a classifier whose purpose will be to \n",
    "classify utterances of the words \"right\", \"eight\", \"cat\"**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "words_selected = ['right', 'eight', 'cat']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Build a training, validation and testing sets\n",
    "\n",
    "In the cells below, we build the training, validation, and training datasets that will be used to train the CNN model. \n",
    "We recommend that you load the pre-processed data. It takes a while to compute the MFCCs for these sound clips. **The pre-processed, feature-extracted data was generated for words (`right`, `eight`, `cat`) only.** If you want to use different words, you need to download the raw, original dataset.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "# Create the mapping between index for one-hot encoding and the class label:\n",
    "class_to_int = {label:i for i, label in enumerate(words_selected)}\n",
    "print(class_to_int)\n",
    "\n",
    "# Save that lookup dictionary to disk: \n",
    "pkl.dump(class_to_int, open('./class_label_lookup.pkl','wb'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Either load the processed and feature-extracted dataset or generate your own. \n",
    "# Set this value to True unless you have downloaded the raw data locally using the shell script. \n",
    "\n",
    "load_preprocessed_data = True\n",
    "\n",
    "if load_preprocessed_data: \n",
    "    df = pkl.load(open('../data/processed_data.pkl','rb'))\n",
    "else: \n",
    "    tqdm.pandas()\n",
    "    df = pd.DataFrame(columns=['filename','label','sample'])\n",
    "\n",
    "    for w in words_selected: \n",
    "        files = [ f for f in os.listdir(data_dir + w) if f[-4:] == '.wav' ]\n",
    "        labels = [ w for f in files]\n",
    "        sample = [ which_set(f, validation_fraction, testing_fraction) for f in files ]\n",
    "        tmp = pd.DataFrame({'filename':files, 'label':labels, 'sample':sample})\n",
    "        df = df.append(tmp, ignore_index=True)\n",
    "   \n",
    "    # adding the full path, mfcc data and class label mapping \n",
    "    df['full_path'] = data_dir + df['label'] + \"/\" + df['filename']\n",
    "    df['mfcc'] = df['full_path'].progress_apply(lambda x: data_processing(x))\n",
    "    df['y_hot'] = df['label'].map(class_to_int)\n",
    "    \n",
    "    # save data to disk: \n",
    "    df.to_pickle('../data/processed_data.pkl')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "# Let's display the content of the column \"mfcc\". This column \n",
    "# contains the Mel-frequency Cepstral Coefficients (MFCC). \n",
    "# It's basically a power spectrum. \n",
    "\n",
    "def display_spectrogram(df, id): \n",
    "    \"\"\"Given the index value of the example in the pandas dataframe \n",
    "    this function will display a 2-D heatmap of the MFCC spectrum. \n",
    "    \n",
    "    Args: \n",
    "      - df (pd.DataFrame) : Dataframe of interest. \n",
    "      - id (integer) : row index of the example of interest \n",
    "    \"\"\"\n",
    "    ax = heatmap(df.iloc[id]['mfcc'], center=-150, cmap=\"YlGnBu\")\n",
    "    ax.set(xlabel=\"time chunk\", ylabel=\"mfcc\") \n",
    "\n",
    "# We will take a look at the first example in the t\n",
    "display_spectrogram(df, 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "# Sanity check: counts for each sample category. \n",
    "df['sample'].value_counts()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Splitting into training and testing sets\n",
    "\n",
    "Below we split the datasets by training, validation and evaluation (test) samples. \n",
    "We will use the mfcc values as the covariates/features for each clip. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Training dataset: \n",
    "X_train = df[df['sample'] == \"training\"]['mfcc'].values\n",
    "Y_train = df[df['sample'] == \"training\"]['y_hot'].values\n",
    "\n",
    "# Validation dataset: \n",
    "X_valid = df[df['sample'] == \"validation\"]['mfcc'].values\n",
    "Y_valid = df[df['sample'] == \"validation\"]['y_hot'].values\n",
    "\n",
    "# Evaluation dataset: \n",
    "X_test = df[df['sample'] == \"testing\"]['mfcc'].values\n",
    "Y_test = df[df['sample'] == \"testing\"]['y_hot'].values "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Reshaping the MFCCs  \n",
    "Xtrain = np.asarray([ sub.reshape(20,32,1) for sub in X_train ])\n",
    "Xvalid = np.asarray([ sub.reshape(20,32,1) for sub in X_valid ])\n",
    "Xtest = np.asarray([ sub.reshape(20,32,1) for sub in X_test ])\n",
    "\n",
    "# Converting the index values to categorical values using the Keras to_categorical() function \n",
    "Ytrain = to_categorical(Y_train)\n",
    "Yvalid = to_categorical(Y_valid)\n",
    "Ytest = to_categorical(Y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "# let's make sure we have the right dimensions for all these arrays: \n",
    "Xtrain.shape, Ytrain.shape, Xtest.shape, Ytest.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Congratulations! You are now ready to train the Convolutional Neural Network (CNN) model. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Training the CNN model \n",
    "\n",
    "I adopted a slight variation of the [Sainath & Parada (2015)](https://static.googleusercontent.com/media/research.google.com/en//pubs/archive/43969.pdf) `cnn-trad-fpool3` model for the purpose of this demo. I invite the reader to read this paper for more details. \n",
    "\n",
    "This is a simple model that performs well. I adopted convolution kernels of equal size in both frequency and time space. I removed the linear low-rank layer. I also used fewer filters and the kernel size is smaller. \n",
    "\n",
    "I invite the reader to modify the network architecture and try different approaches. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "# Convolutional Neural Network (CNN) Model Definition:  \n",
    "\n",
    "model = Sequential()\n",
    "\n",
    "# Convolution layer (4x8), 32 filters \n",
    "model.add(Conv2D(32, kernel_size=(4, 8), activation='relu', input_shape=(20, 32, 1)))\n",
    "model.add(MaxPooling2D(pool_size=(2, 2)))\n",
    "model.add(Dropout(0.25))\n",
    "\n",
    "# Convolution layer (2x4), 16 filters \n",
    "model.add(Conv2D(16, kernel_size=(2, 4), activation='relu'))\n",
    "model.add(MaxPooling2D(pool_size=(2, 2)))\n",
    "model.add(Dropout(0.25))\n",
    "\n",
    "model.add(Flatten())\n",
    "# DNN FC layer: \n",
    "model.add(Dense(128, activation='relu'))\n",
    "model.add(Dropout(0.25))\n",
    "# softmax layer (3 words)\n",
    "model.add(Dense(len(words_selected), activation='softmax'))\n",
    "model.compile(loss=keras.losses.categorical_crossentropy,\n",
    "              optimizer=keras.optimizers.Adadelta(),\n",
    "              metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's fit the model. \n",
    "\n",
    "Ten epochs is good enough to start. It will give you an accuracy superior to 0.90. I invite the reader to train the models for 100+ epochs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "# Fitting the model: \n",
    "\n",
    "model.fit(Xtrain, Ytrain, \n",
    "          batch_size=100, \n",
    "          epochs=10, \n",
    "          verbose=1, \n",
    "          shuffle=True,\n",
    "          validation_data=(Xvalid, Yvalid))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "# Evaluating the model: \n",
    "\n",
    "loss, accuracy = model.evaluate(x=Xtest, y=Ytest)\n",
    "print(\"loss = {}, accuracy = {}\".format(loss, accuracy))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "# Compute and plot Confusion matrix \n",
    "\n",
    "Ypred = model.predict_classes(Xtest)\n",
    "cm = confusion_matrix(Y_test, Ypred, labels=[0,1,2])#, labels = words_selected)\n",
    "\n",
    "ax = heatmap(cm, annot=True, fmt='d', \n",
    "                     linewidths=.2, \n",
    "                     xticklabels=words_selected, \n",
    "                     yticklabels=words_selected)\n",
    "\n",
    "ax.set(xlabel='predicted',ylabel='observed')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Looks like we are doing well with this model. Next we are going to save this model to the model catalog and later deploy it as an HTTP endpoint. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Save the model to the Model Catalog"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 1: Publish your conda environment \n",
    "\n",
    "If you have: \n",
    "* modified a data science conda environment by adding a new library for example \n",
    "* created your own conda environment \n",
    "\n",
    "then you need to publish your conda environment. In the file `runtime.yaml` of the model artifact, we keep a reference of the conda environment that was used to train the model. That same conda environment can also be used as the inference environment used by the model deployment resource, if you decide to deploy the model later on. \n",
    "\n",
    "You can publish a conda by running the following `odsc conda` commands in the terminal or by executing the following cells. Initialize `odsc conda` first by providing a target bucket destination for your conda: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# replace with your values: \n",
    "#!odsc conda init -b <YOUR_BUCKET_NAME> -n <YOUR_BUCKET_NAMESPACE>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# specify the slugname of the installed conda environment that you want to publish: \n",
    "#!odsc conda publish -s <YOUR_INSTALLED_CONDA_SLUG>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 2: Prepare the model artifact\n",
    "\n",
    "First we want to prepare a model artifact. That artifact is a zip archive of the following: \n",
    "* **score.py**:  an inference file is used to load the model object to memory and call the inference endpoint of the model (i.e. predict())\n",
    "* **runtime.yaml**: a file describing the provenance of the model as well as providing a description of the runtime Conda environment of the model. This environment will be used in the upcoming OCI Data Science Model Deployment feature. \n",
    "* **your serialized model object**. This depends on the library you are using and the format you want to use. Models can be serialized as pickle (pkl) objects, ONNX, hdf5, json, pmml, etc. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the cell below, we are using the `prepare_generic_model()` function to generate a template model artifact. This function can handle any model trained with any ML library. You just have to fill in the blanks. The following files will be created under the `artifact_dir` directory: \n",
    "\n",
    "* `score.py`\n",
    "* `runtime.yaml`\n",
    "\n",
    "Since those are templates, next step will be to modify each file to ensure that our model can be saved to the catalog.  \n",
    "\n",
    "In the `prepare_generic_model()` call below, we set the value of `inference_conda_env` to be the published conda environment path on object storage.  The `inference_conda_env` parameters keep track of which conda to use for model deployment. This is the same environment we used for training. You can find the path on object storage directly in the Environment Explorer in the card of your published environment just like the following screenshot shows: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_path = \"./model-artifact\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "artifact = prepare_generic_model(model_path,\n",
    "                                 function_artifacts=False, \n",
    "                                 force_overwrite=True,\n",
    "                                 inference_conda_env='<insert-your-published-conda-environment-path>')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's save our Keras model first: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.save(os.path.join(model_path, \"cnn-speech.hdf5\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And the mapping between classes and words: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save that lookup dictionary to disk: \n",
    "pkl.dump(class_to_int, open(os.path.join(model_path,'class_label_lookup.pkl'),'wb'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's modify score.py to account for the data transformation waveform -> mfcc with librosa: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%writefile {model_path}/score.py \n",
    "\n",
    "import json\n",
    "import os\n",
    "import keras \n",
    "import numpy as np\n",
    "from keras.models import load_model as klm \n",
    "import librosa\n",
    "import pickle as pkl\n",
    "\n",
    "model_name = 'cnn-speech.hdf5'\n",
    "\n",
    "# load the lookup dictionary:\n",
    "words_selected = pkl.load(open('class_label_lookup.pkl','rb'))\n",
    "# do a reverse lookup:\n",
    "words_selected = { value: key for key, value in words_selected.items() }\n",
    "\n",
    "def load_model(model_file_name=model_name):\n",
    "    \"\"\"\n",
    "    Loads model from the serialized format\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    model:  a model instance on which predict API can be invoked\n",
    "    \"\"\"\n",
    "    model_dir = os.path.dirname(os.path.realpath(__file__))\n",
    "    contents = os.listdir(model_dir)\n",
    "    if model_file_name in contents:\n",
    "        modelpath = os.path.join(os.path.dirname(os.path.realpath(__file__)), model_file_name)\n",
    "        tmp = klm(modelpath)\n",
    "        tmp._make_predict_function()\n",
    "        return tmp \n",
    "    else:\n",
    "        raise Exception('{0} is not found in model directory {1}'.format(model_file_name, model_dir))\n",
    "\n",
    "\n",
    "def predict(data, model=load_model()):\n",
    "    \"\"\"\n",
    "    Returns prediction given the model and data to predict\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    model: Model instance returned by load_model API\n",
    "    data: Data format as expected by the predict API of the core estimator. For eg. in case of sckit models it could be numpy array/List of list/Panda DataFrame\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    predictions: Output from scoring server\n",
    "        Format: {'prediction':output from model.predict method}\n",
    "\n",
    "    \"\"\"\n",
    "    \n",
    "    from pandas import read_json, DataFrame\n",
    "    from io import StringIO\n",
    "    #X = read_json(StringIO(data))\n",
    "    _ = json.loads(data)\n",
    "    waveform = _['input']\n",
    "    waveform = np.asarray(waveform)\n",
    "    max_size = 32\n",
    "    mfcc = librosa.feature.mfcc(waveform, sr=16000, n_mfcc=20)\n",
    "    mfcc = np.pad(mfcc, pad_width=((0, 0),(0, max_size-mfcc.shape[1])), mode='constant')\n",
    "    mfcc = mfcc.reshape(1,20,32,1)\n",
    "    class_value = model.predict_classes(mfcc)\n",
    "    return {'what-you-said-was': words_selected[class_value[0]]}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Testing the Artifact with Sample Payloads \n",
    "\n",
    "Here I use a sample waveform to test the artifact predict function: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# directory where some sample wave files are located: \n",
    "sample_data_dir = '../data/'\n",
    "\n",
    "# filenames \n",
    "files = ['6b81fead_nohash_0.wav',\n",
    "         'ff2b842e_nohash_2.wav',\n",
    "         'ccb1266b_nohash_1.wav']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "waveform0, _ = librosa.load(sample_data_dir+files[0], mono=True, sr=None)\n",
    "waveform1, _ = librosa.load(sample_data_dir+files[1], mono=True, sr=None)\n",
    "waveform2, _ = librosa.load(sample_data_dir+files[2], mono=True, sr=None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "payload = {\"input\": waveform0.tolist()}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "import json\n",
    "sys.path.insert(0, model_path)\n",
    "\n",
    "\n",
    "from score import load_model, predict\n",
    "\n",
    "_ = load_model()\n",
    "predictions_test = predict(json.dumps(payload), _)\n",
    "predictions_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from io import StringIO\n",
    "tmp = json.dumps(payload)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "_ = json.loads(tmp)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Saving the Model to the Catalog \n",
    "\n",
    "We are now ready to save the artifact to the catalog: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# using resource principal to authenticate to the model catalog: \n",
    "ads.set_auth(auth='resource_principal')\n",
    "\n",
    "compartment_id = os.environ['NB_SESSION_COMPARTMENT_OCID']\n",
    "project_id = os.environ[\"PROJECT_OCID\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mc_model = artifact.save(project_id=project_id, \n",
    "                         compartment_id=compartment_id, \n",
    "                         display_name=\"cnn-speech-commands\",\n",
    "                         description=f\"Simple CNN model to classify speech commands\",\n",
    "                         ignore_pending_changes=True)\n",
    "mc_model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Create a Model Deployment\n",
    "\n",
    "Here you have a couple of options. You can either create a model deployment directly in the console or do it programmatically via the OCI Python SDK. \n",
    "\n",
    "The cells below walk you through the programmatic deployment. If you decide to create the deployment in the console, you can skip this section and go to the \"Invoking the Deployed Model `/predict` Endpoint\" section  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import oci \n",
    "from oci.data_science import DataScienceClient, DataScienceClientCompositeOperations\n",
    "\n",
    "from oci.data_science.models import ModelConfigurationDetails, InstanceConfiguration, \\\n",
    "                                    FixedSizeScalingPolicy, CategoryLogDetails, LogDetails, \\\n",
    "                                    SingleModelDeploymentConfigurationDetails, CreateModelDeploymentDetails"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# using resource principals\n",
    "auth = oci.auth.signers.get_resource_principals_signer()\n",
    "data_science = DataScienceClient({}, signer=auth)\n",
    "\n",
    "# OR the config + pem authn flow: \n",
    "#oci_config = oci.config.from_file('~/.oci/config', \"DEFAULT\")\n",
    "#data_science = DataScienceClient(oci_config)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Configuration of the model: \n",
    "# We deploy the sklearn model we saved to the model catalog in this notebook (catalog_entry.id)\n",
    "# You can change the shape, instance_count, or the bandwidth of the load balancer (in Mbps)\n",
    "model_configuration_details_object = ModelConfigurationDetails(model_id=mc_model.id,\n",
    "                                                               instance_configuration=InstanceConfiguration(instance_shape_name='VM.Standard2.1'),\n",
    "                                                               scaling_policy=FixedSizeScalingPolicy(instance_count=1),\n",
    "                                                               bandwidth_mbps=10)\n",
    "\n",
    "# Single Model Deployment Configuration\n",
    "# Includes info about the deployment type and the model configuration details: \n",
    "# At the moment, only deployment+type='SINGLE_MODEL' is supported. \n",
    "single_model_config = SingleModelDeploymentConfigurationDetails(deployment_type='SINGLE_MODEL',\n",
    "                                                                model_configuration_details=model_configuration_details_object)\n",
    "\n",
    "\n",
    "# OPTIONAL - Configuration of the access and predict logs. \n",
    "# Make sure you have the proper policy in place to allow model deployment to emit predict/access logs. For example: \n",
    "# allow any-user to use log-content in tenancy where ALL {request.principal.type = 'datasciencemodeldeployment'}\n",
    "logging_config = False\n",
    "if logging_config: \n",
    "\n",
    "    access_log_group_id = \"<your-log-group-id>\"\n",
    "    access_log_id = \"<your-log-id>\"\n",
    "    predict_log_group_id = \"<your-log-group-id>\"\n",
    "    predict_log_id = \"<your-log-id>\"\n",
    "\n",
    "    logs_configuration_details_object = CategoryLogDetails(access=LogDetails(log_group_id=access_log_group_id,\n",
    "                                                                             log_id=access_log_id),\n",
    "                                                           predict=LogDetails(log_group_id=predict_log_group_id,\n",
    "                                                                             log_id=predict_log_id))\n",
    "else: \n",
    "    logs_configuration_details_object = {}\n",
    "\n",
    "\n",
    "# Wrapping all of these configs into a model deploy configuration: \n",
    "# Replace with your own values for display_name, description, project, and compartment OCIDs \n",
    "model_deploy_configuration = CreateModelDeploymentDetails(display_name='speech-commands',\n",
    "                                                          description='speech commands classification model',\n",
    "                                                          project_id=os.environ['PROJECT_OCID'],\n",
    "                                                          compartment_id=os.environ['NB_SESSION_COMPARTMENT_OCID'],\n",
    "                                                          model_deployment_configuration_details=single_model_config,\n",
    "                                                          category_log_details=logs_configuration_details_object)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Creating a model deployment. This action takes a few minutes\n",
    "\n",
    "data_science_composite = DataScienceClientCompositeOperations(data_science)\n",
    "model_deployment = data_science_composite.create_model_deployment_and_wait_for_state(model_deploy_configuration, \n",
    "                                                                                     wait_for_states=[\"SUCCEEDED\", \"FAILED\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Invoking the Deployed Model `/predict` Endpoint \n",
    "\n",
    "Last step is to invoke/call the model endpoint. Here again you can pick between resource principals or the config+key authn flow. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "import oci\n",
    "from oci.signer import Signer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# model deployment endpoint. Here we assume that the notebook region is the same as the region where the model deployment occurs.\n",
    "# Alternatively you can also go in the details page of your model deployment in the OCI console. Under \"Invoke Your Model\", you will find the HTTP endpoint \n",
    "# of your model. \n",
    "#uri = f\"https://modeldeployment.{os.environ['NB_REGION']}.oci.customer-oci.com/{model_deployment.data.resources[0].identifier}/predict\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Using Resource principal to authenticate against the model endpoint. Set using_rps=False if you are using \n",
    "# the config+key flow. \n",
    "using_rps = True\n",
    "endpoint = uri\n",
    "\n",
    "# payload: \n",
    "payload = {\"input\": waveform0.tolist()}\n",
    "input_data = json.dumps(payload)\n",
    "\n",
    "if using_rps: # using resource principal:     \n",
    "    auth = oci.auth.signers.get_resource_principals_signer()\n",
    "else: # using config + key: \n",
    "    config = oci.config.from_file(\"~/.oci/config\") # replace with the location of your oci config file\n",
    "    auth = Signer(\n",
    "        tenancy=config['tenancy'],\n",
    "        user=config['user'],\n",
    "        fingerprint=config['fingerprint'],\n",
    "        private_key_file_location=config['key_file'],\n",
    "        pass_phrase=config['pass_phrase'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "    \n",
    "# submit request to model endpoint: \n",
    "requests.post(endpoint, json=input_data, auth=auth).json()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# What Next?\n",
    "\n",
    "You can experiment and expand on the model trained in this notebook. Here are a few areas to explore: \n",
    "\n",
    "* increase the number of classes. Try to classify 10+ words \n",
    "* add noise to your examples. Create a more robust classifier. \n",
    "* try different CNN architectures \n",
    "* include an \"other\" class\n",
    "* include a \"silence\" class"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# References \n",
    "\n",
    "* Warden, P. 2018, \"Speech Commands: A Dataset for Limited-Vocabulary Speech Recognition\", [arXiv:1804.03209](https://arxiv.org/abs/1804.03209)\n",
    "* Sainath, T.N., Parada, C. 2015 [\"Convolutional Neural Networks for Small-footprint Keyword Spotting\"]( https://static.googleusercontent.com/media/research.google.com/en//pubs/archive/43969.pdf)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Appendix"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here's the network architecture for reference. \n",
    "\n",
    "![network-architecture](https://user-images.githubusercontent.com/5395649/46776515-cb378280-ccc0-11e8-8dcb-84db4156e981.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Below is a function I took directly from the `README` file that came with the dataset. This is a very useful function that ensures testing/validation data does not make its way in the training dataset. We will be using it to assign the sample (training, validation, testing) to which each audio clip belongs. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# From Warden (2018) : https://arxiv.org/abs/1804.03209\n",
    "\n",
    "def which_set(filename, validation_percentage, testing_percentage):\n",
    "    \"\"\"Determines which data partition the file should belong to.\n",
    "    We want to keep files in the same training, validation, or testing sets even\n",
    "    if new ones are added over time. This makes it less likely that testing\n",
    "    samples will accidentally be reused in training when long runs are restarted \n",
    "    for example. To keep this stability, a hash of the filename is taken and used \n",
    "    to determine which set it should belong to. This determination only depends on \n",
    "    the name and the set proportions, so it won't change as other files are added.\n",
    "    \n",
    "    It's also useful to associate particular files as related (for example words \n",
    "    spoken by the same person), so anything after '_nohash_' in a filename is \n",
    "    ignored for set determination. This ensures that 'bobby_nohash_0.wav' and \n",
    "    'bobby_nohash_1.wav' are always in the same set, for example.\n",
    "    \n",
    "    Args\n",
    "    filename: File path of the data sample.\n",
    "    validation_percentage: How much of the data set to use for validation.\n",
    "    testing_percentage: How much of the data set to use for testing.\n",
    "    \n",
    "    Returns:\n",
    "    String, one of 'training', 'validation', or 'testing'.\n",
    "    \"\"\"\n",
    "    base_name = os.path.basename(filename)\n",
    "    #print(base_name)\n",
    "    # We want to ignore anything after '_nohash_' in the file name when\n",
    "    # deciding which set to put a wav in, so the data set creator has a way of\n",
    "    # grouping wavs that are close variations of each other.\n",
    "    hash_name = re.sub(r'_nohash_.*$', '', base_name).encode('utf-8')\n",
    "    # This looks a bit magical, but we need to decide whether this file should\n",
    "    # go into the training, testing, or validation sets, and we want to keep\n",
    "    # existing files in the same set even if more files are subsequently\n",
    "    # added.\n",
    "    # To do that, we need a stable way of deciding based on just the file name\n",
    "    # itself, so we do a hash of that and then use that to generate a\n",
    "    # probability value that we use to assign it. \n",
    "    hash_name_hashed = hashlib.sha1(hash_name).hexdigest() \n",
    "    percentage_hash = ((int(hash_name_hashed, 16) %\n",
    "                      (MAX_NUM_WAVS_PER_CLASS + 1)) *\n",
    "                     (100.0 / MAX_NUM_WAVS_PER_CLASS)) \n",
    "    if percentage_hash < validation_percentage:\n",
    "        result = 'validation'\n",
    "    elif percentage_hash < (testing_percentage + validation_percentage):\n",
    "        result = 'testing'\n",
    "    else:\n",
    "        result = 'training'\n",
    "    return result"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Below is the data transformation that we apply to each audio clip before feeding the clip to the CNN model. Take a look at the [intro notebook](1-intro-to-audio-data.ipynb) where we discuss spectrogram and MFCCs. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def data_processing(filepath, max_len=32):\n",
    "    \"\"\" Compute MFCCs for a given clip\n",
    "    \n",
    "    Args: \n",
    "        - filepath (str) : path of the wav file to analyze. \n",
    "        - max_len (int) : \n",
    "        \n",
    "    Returns: \n",
    "        - Mel-frequency cepstral coefficients array. \n",
    "    \"\"\"\n",
    "    \n",
    "    # Here we are loading the audio clip using librosa.\n",
    "    waveform, sampling_rate = librosa.load(filepath, mono=True, sr=None)\n",
    "    # compute the Mel-frequency cepstral coefficients. n_mfcc \n",
    "    # represents the number of coefficients to return. \n",
    "    mfcc = librosa.feature.mfcc(waveform, sr=16000, n_mfcc=20)\n",
    "    \n",
    "    # Not all clips have the same duration. Padding along the time dimension to \n",
    "    # ensure each clip has the same dimensions. Each image fed to the CNN needs \n",
    "    # to have the same dimensions. \n",
    "    mfcc = np.pad(mfcc, pad_width=((0, 0),(0, max_len-mfcc.shape[1])), mode='constant')\n",
    "    return mfcc "
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "speech",
   "language": "python",
   "name": "speech"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
