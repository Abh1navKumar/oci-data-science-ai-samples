PyTorch Conda Environment Family 
================================

[PyTorch](https://pytorch.org/) is a machine learning library that is used in applications such as NLP, computer vision, and much more. The [PyTorch conda environment family](https://docs.oracle.com/en-us/iaas/data-science/using/conda-pytorch-fam.htm) supports CPU and GPU versions. It provides high-level features for tensor computing and deep neural networks.  This environment also includes acceleration support on Intel's CPUs with the use of `daal4py`. This library enhances `scikit-learn` algorithms by using Intel(R) `oneAPI` Data Analytics library. Use the ADSTuner to speed up your data science by performing smart hyperparameter tuning. It includes sane default values and the ability to adjust them based on based experience.

The notebooks in this folder are meant to run the [PyTorch conda environment family](https://docs.oracle.com/en-us/iaas/data-science/using/conda-pytorch-fam.htm) conda environments.

# Notebook Descriptions

* `adstuner.ipynb`: A hyperparameter is a parameter that is used to control a learning process. This is in contrast to other parameters that are learned in the training process. The process of hyperparameter optimization is to search for hyperparameter values by building many models and assessing their quality. This notebook provides an overview of the `ADSTuner` hyperparameter optimization engine. `ADSTuner` can optimize any estimator object that follows the [scikit-learn API](https://scikit-learn.org/stable/modules/classes.html). 
* `adstuner_search_space_update.ipynb`: This notebook demonstrates how to perform a hyperparameter tuning on a toy model. It explains how to adjust the search space used by the `ADSTuner`. It illustrates how to add and remove hyperparameters. Also, the notebook shows you the techniques that are needed to adjust the parameters that define the exact distributions used to define the search space. 
* `adstuner_sync_and_async.ipynb`: Hyperparameter tuning can take a few seconds on a model that is parsimonious and has a small training set or it can be computationally expensive on a complex model with a large dataset. For short tuning operations, it's easiest to perform synchronous hyperparameter tuning. Synchronous hyperparameter tuning blocks the notebook from running any other code during the tuning process. For long-running processes, it's best to have the tuning be performed asynchronously. It doesn't block you from running other code in the notebook, such as graphing the tuning performance. This notebook demonstrates how to use the `tune()` method to create, halt, resume, and terminate the synchronous and asynchronous hyperparameter tuning jobs.
* `oneDAL.pynb`: This notebook demonstrates an easy way to enhance the performance of scikit-learn models using Intel-provided Python accelerators. Acceleration is achieved by using the Intel(R) oneAPI Data Analytics Library (oneDAL) that allows fast use of the framework suited for data scientists or machine learning users. Daal4py was created to give data scientists the easiest way to get better performance while using the familiar `scikit-learn` package. 

