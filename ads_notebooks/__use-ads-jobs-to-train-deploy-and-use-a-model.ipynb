{
 "cells": [
  {
   "cell_type": "raw",
   "id": "eb8d5685",
   "metadata": {},
   "source": [
    "@notebook{use-ads-jobs-to-train-deploy-and-use-a-model.ipynb,\n",
    "    title: Model Deployment Using Jobs,\n",
    "    summary: Build a machine learning model in a job, deploy the model, and make a prediction from it.,\n",
    "    developed on: generalml_p37_cpu_v1,\n",
    "    keywords: model training, model deployment,\n",
    "    license: Universal Permissive License v 1.0\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "41f424f6",
   "metadata": {},
   "source": [
    "Oracle Data Science service sample notebook.\n",
    "\n",
    "Copyright (c) 2021, 2022 Oracle, Inc. All rights reserved. Licensed under the [Universal Permissive License v 1.0](https://oss.oracle.com/licenses/upl).\n",
    "\n",
    "---\n",
    "\n",
    "# <font color=\"red\">Model Deployment Using Jobs</font>\n",
    "<p style=\"margin-left:10%; margin-right:10%;\">by the <font color=\"teal\"> Oracle Cloud Infrastructure Data Science Service</font></p>\n",
    "\n",
    "---\n",
    "\n",
    "# Overview:\n",
    "\n",
    "This notebook demonstrates an end-to-end workflow of building a machine learning model in a job, deploying the model and then making a prediction from it. A job allows you to use on-demand infrastructure that will spin up, run some tasks, capture outputs, and clean up. The `ads.jobs` module in the Accelerated Data Science (ADS) SDK allows you to create and run jobs using the Oracle Cloud Infrastructure (OCI) Data Science service.\n",
    "\n",
    "The focus of this notebook is to demonstrate how to train a model using a job, deploy the model, and perform a prediction on the model. The notebook covers how to define the script that will run in the job. The script is a toy decision tree model based on the `iris` dataset. You will then create a job that will run the script using the `ads.jobs` module. The script trains a model and stores it in the model catalog. You will then use the `ads.model.deployment` module to deploy the model and perform a prediction. In addition, the notebook shows how to programmatically, create a log group and a log to capture the jobs' logs.\n",
    "\n",
    "Developed on Compatible conda pack: [General Machine Learning](https://docs.oracle.com/en-us/iaas/data-science/using/conda-gml-fam.htm) for CPU on Python 3.7 (version 1.0)\n",
    "\n",
    "--- \n",
    "\n",
    "## Contents:\n",
    "\n",
    "- <a href=\"#intro\">Introduction</a>\n",
    "    - <a href=\"#intro_config\">Configuration</a>\n",
    "- <a href=\"#script\">Job Script</a>\n",
    "- <a href=\"#train\">Train the Model</a>\n",
    "- <a href=\"#inference\">Inference</a>\n",
    "- <a href=\"#clean_up\">Clean Up</a>\n",
    "- <a href='#ref'>References</a>\n",
    "\n",
    "---\n",
    "\n",
    "\n",
    "Datasets are provided as a convenience.  Datasets are considered third-party content and are not considered materials \n",
    "under your agreement with Oracle.\n",
    "\n",
    "You can access the `iris` dataset license [here](https://github.com/scikit-learn/scikit-learn/blob/master/COPYING).  \n",
    "\n",
    "---\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2c10f2b5",
   "metadata": {},
   "outputs": [],
   "source": [
    "import ads\n",
    "import logging\n",
    "import oci\n",
    "import os\n",
    "import random\n",
    "import string\n",
    "import tempfile\n",
    "\n",
    "from ads.catalog.model import ModelCatalog\n",
    "from ads.common.oci_logging import OCILogGroup, OCILog\n",
    "from ads.jobs import Job, infrastructure, PythonRuntime\n",
    "from ads.model.deployment.common.utils import State\n",
    "from ads.model.deployment.model_deployer import ModelDeployer\n",
    "from ads.model.deployment.model_deployment_properties import ModelDeploymentProperties\n",
    "from sklearn import datasets\n",
    "\n",
    "ads.set_auth(\"resource_principal\")\n",
    "logging.getLogger(\"ocifs\").setLevel(level=logging.ERROR)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6b5d5916",
   "metadata": {},
   "source": [
    "<a id=\"intro\"></a>\n",
    "# Introduction\n",
    "\n",
    "In the prototyping stages of model building, you often train smaller models and a subset of the data. This allows for fast iteration and a notebook is an ideal place to do that work. However, as you learn more about the data, and what model classes and hyperparameters are best, you generally want to train larger models and the entire dataset. Building complex models on large datasets can pose various challenges when just using a notebook session. Building the model can be time consuming and you don't want to slow down your other work as the model builds. Generally, the virtual machine (VM) for the notebook is relatively small as the trade-off between computational power and cost tends to lend itself to using smaller VM shapes. Further, you often want to build multiple models simultaneously, and this can significantly slow down the model building process as all the models are running on the same hardware.\n",
    "\n",
    "The solution these challenges is to offload the model building to another computational resource. The Oracle Data Science Jobs service provides an ideal solution. A notebook session can be used to launch a job to compute the model. This allows you to scale the VM shape to the computational and memory requirements to build the model. It also allow the computation to be offloaded from the notebook session that you are working it. Thus, there is no waiting round for the model to build. By using multiple jobs, you can train many models at the same time and thus improve your workflow.\n",
    "\n",
    "<a id=\"intro_config\"></a>\n",
    "## Configuration\n",
    "\n",
    "Jobs and model deployment has a number of configuration values. This notebook uses sane default values that will allow it to run on common tenancy configurations. To configure the job, you must provide an OCID for a subnet. Update the value in the next cell by replacing `<subnet_id>` with your subnet OCID. It should look something like this:\n",
    "\n",
    "```python\n",
    "SUBNET_ID = \"ocid1.subnet.oc1.iad.aaaaaaaa6fsov7mavp7nrh7t4yc...\"\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "827cbc7f",
   "metadata": {},
   "outputs": [],
   "source": [
    "SUBNET_ID = \"<subnet_id>\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f9ee880a",
   "metadata": {},
   "source": [
    "One of the advantages of using a job to build the model is that you can specify the VM shape that has the computing power and memory that you need to build the model. Once the model is built the VM shape will terminate and not waste valuable resources. The next cell specifies that a `VM.Standard2.1` VM shape will be used for the job and lists the supported shapes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c5f4d5ef",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the VM shape for the job\n",
    "vm_shape = \"VM.Standard2.1\"\n",
    "\n",
    "[s.name for s in infrastructure.DataScienceJob.instance_shapes()]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1c99e087",
   "metadata": {},
   "source": [
    "This notebook creates a number of resources such as logs, log group, job, model, and a model deployment. A unique name will be created to identify all these resources. The following cell creates and prints the common name used on all the resources."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6f88a886",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a unique ID that is used for the name of the resources that will be created\n",
    "resource_name = \"model_deployment_jobs_\" + \"\".join(\n",
    "    random.choices(string.ascii_letters + string.digits, k=4)\n",
    ")\n",
    "print(f\"Unique ID used in all the resources: {resource_name}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "400fa16e",
   "metadata": {},
   "source": [
    "<a id=\"script\"></a>\n",
    "# Job Script\n",
    "\n",
    "A Python script is needed to train the model and save it in the model catalog. The next cell writes this script so that it can be used when the job is run. The script uses the popular [`iris`](https://scikit-learn.org/stable/auto_examples/datasets/plot_iris_dataset.html) dataset to train a multiclass decision tree classifier using the `DecisionTreeClassifier` class. This model is then converted to an `ADSModel` object.\n",
    "\n",
    "The model artifacts are created using the `.prepare()` method. Since the model will be deployed the `data_science_env`, and `inference_conda_env` parameters must be specified. In the model deployment, a conda environment is specified so that the required libraries are installed. The conda environments are provided as part of the OCI Data Science service and these are called Data Science Environments. Alternatively, you or third parties can Publish conda environments. The parameter `data_science_env` indicates that the environment is provided by the Data Science service if it is set to `True`. Otherwise, it is a Published environment. The `inference_conda_env` is the URL that the model deployment service will obtain the conda environment from and install it in the model deployment instances.\n",
    "\n",
    "The `.prepare()` method returns an object that represents the model artifact. The `.save()` method is used to store the model artifacts into the model catalog. The model deployment service will then use this model to deploy the model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e1a2f019",
   "metadata": {},
   "outputs": [],
   "source": [
    "job_script = tempfile.NamedTemporaryFile(suffix=\".py\", delete=True)\n",
    "\n",
    "with open(job_script.name, mode=\"w\") as f:\n",
    "    f.write(\n",
    "        f\"\"\"\n",
    "import pandas as pd\n",
    "\n",
    "from ads import set_auth\n",
    "from ads.catalog.model import ModelCatalog\n",
    "from ads.common.model import ADSModel\n",
    "from sklearn import datasets\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from tempfile import mkdtemp\n",
    "\n",
    "set_auth(\"resource_principal\")\n",
    "\n",
    "# Train the model\n",
    "iris = datasets.load_iris()\n",
    "X=iris.data\n",
    "y=iris.target\n",
    "clf = DecisionTreeClassifier().fit(X=X, y=y)\n",
    "model = ADSModel.from_estimator(clf)\n",
    "\n",
    "# Prepare the model artifacts\n",
    "model_artifact = model.prepare(\n",
    "    target_dir=mkdtemp(), \n",
    "    X_sample=pd.DataFrame(X), \n",
    "    y_sample=pd.Series(y), \n",
    "    force_overwrite=True, \n",
    "    data_science_env=True, \n",
    "    inference_conda_env=\"oci://service-conda-packs@id19sfcrra6z/service_pack/cpu/General Machine Learning for CPUs/1.0/mlcpuv1\") \n",
    "\n",
    "# Save the model to the model catalog\n",
    "mc_model = model_artifact.save(\n",
    "    project_id=\"{os.environ['PROJECT_OCID']}\", \n",
    "    compartment_id=\"{os.environ['NB_SESSION_COMPARTMENT_OCID']}\", \n",
    "    display_name=\"{resource_name}\", \n",
    "    description=\"Model produced in the model_deployment_using_jobs.ipynb notebook\")\n",
    "\n",
    "\"\"\"\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c353c5ca",
   "metadata": {},
   "source": [
    "<a id=\"train\"></a>\n",
    "# Train the Model\n",
    "\n",
    "The Oracle Data Science Jobs service is used to run the <a href=\"#script\">Job Script</a>. The following cell sets up the required resources to do this and executes the job.\n",
    "\n",
    "Jobs has the ability to log messages from the script along with other events. Therefore, the next cell sets up a log group to contain the logs along with a log. It then creates a `Job` object and sets up the infrastructure that is needed to run the job. This includes information such as what compartment, project, and subnet should be used. It also has information about the instance such as the VM shape, the size of the block storage and it attaches the logging destination.\n",
    "\n",
    "While the `.with_intrastructure()` method defines the infrastructure that will be used, the `.with_runtime()` method defined the basic environment. The `PythonRunTime` class is used to specify that a runtime environment should be configured. The `.with_script()` method contains the path to the <a href=\"#script\">Job Script</a> that is executed.\n",
    "\n",
    "The `.create()` method is used to create the job and the `.run()` method will asynchronously run the job. Generally, this is the behavior that you want so that you can continue to work in your notebook. However, for this notebook, we need the job to complete before creating the model deployment. Thus, the `.watch()` method is called. This will block the notebook and print logging messages to the screen so that you can see the progress of the job.\n",
    "\n",
    "Running the job can take a significant amount of time as resources such as an instance and network resources much be provisioned, and the environment must be configured."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ed11e786",
   "metadata": {},
   "outputs": [],
   "source": [
    "if SUBNET_ID != \"<subnet_id>\":\n",
    "\n",
    "    # Create a log and log group\n",
    "    log_group = OCILogGroup(display_name=resource_name).create()\n",
    "    log = log_group.create_log(resource_name)\n",
    "\n",
    "    # Create and run the job\n",
    "    job = Job(name=resource_name)\n",
    "    job.with_infrastructure(\n",
    "        infrastructure.DataScienceJob()\n",
    "        .with_block_storage_size(100)\n",
    "        .with_compartment_id(os.environ[\"NB_SESSION_COMPARTMENT_OCID\"])\n",
    "        .with_subnet_id(SUBNET_ID)\n",
    "        .with_project_id(os.environ[\"PROJECT_OCID\"])\n",
    "        .with_shape_name(vm_shape)\n",
    "        .with_log_id(log.id)\n",
    "        .with_log_group_id(log_group.id)\n",
    "    )\n",
    "\n",
    "    job.with_runtime(\n",
    "        PythonRuntime()\n",
    "        .with_script(job_script.name)\n",
    "        .with_service_conda(os.path.split(os.environ[\"CONDA_PREFIX\"])[1])\n",
    "    )\n",
    "\n",
    "    job.create()\n",
    "    job_run = job.run()\n",
    "    job_run.watch()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "abd20873",
   "metadata": {},
   "source": [
    "<a id=\"deploy\"></a>\n",
    "# Deploy the Model\n",
    "\n",
    "The Oracle Data Science Model Deployment service is used to deploy the model that the job created. Since the <a href=\"#script\">Job Script</a> created the model, this notebook will need to determine the model OCID that is going to be used in the model deployment. The `ModelCatalog` class is used to make a connection to the model catalog in the specific compartment where the model was stored. It then lists all the models in that compartment until it finds the model with a matching name. From this, it can obtain the model OCID.\n",
    "\n",
    "The Model Deployment service needs to be configured in a way that is similar to what was done for the Jobs service. The compartment, project, display name, VM shape, instance count and logging information needs to be provided. The `.deploy()` is used to deploy the model. Normally, you would want this to be an asynchronous process so that you do not need to wait for the deployment. Similar to the jobs deployment this can take several minutes as it provisions and configures the infrastructure. However, in this case, the `wait_for_completion` parameter is set to `True` as the rest of the notebook assumes the model has been deployed."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d68e9ae7",
   "metadata": {},
   "outputs": [],
   "source": [
    "if SUBNET_ID != \"<subnet_id>\":\n",
    "\n",
    "    # Find and model catalog OCID\n",
    "    mc = ModelCatalog(compartment_id=os.environ[\"PROJECT_COMPARTMENT_OCID\"])\n",
    "    for model in mc.list_models():\n",
    "        if model.display_name == resource_name:\n",
    "            model_id = model.id\n",
    "            break\n",
    "\n",
    "    # Deploy the model\n",
    "    properties = (\n",
    "        ModelDeploymentProperties(model_id)\n",
    "        .with_prop(\"compartment_id\", os.environ[\"PROJECT_COMPARTMENT_OCID\"])\n",
    "        .with_prop(\"project_id\", os.environ[\"PROJECT_OCID\"])\n",
    "        .with_prop(\"display_name\", resource_name)\n",
    "        .with_instance_configuration(\n",
    "            config={\n",
    "                \"INSTANCE_SHAPE\": vm_shape,\n",
    "                \"INSTANCE_COUNT\": 1,\n",
    "                \"bandwidth_mbps\": 10,\n",
    "            }\n",
    "        )\n",
    "        .with_access_log(log_group.id, log.id)\n",
    "        .with_predict_log(log_group.id, log.id)\n",
    "        .build()\n",
    "    )\n",
    "    deployment = ModelDeployer().deploy(properties, wait_for_completion=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "807da455",
   "metadata": {},
   "source": [
    "<a id=\"inference\"></a>\n",
    "# Inference\n",
    "\n",
    "The ultimate goal for deploying a model is to make inferences from it. In the following cell, the `.predict()` of the `ModelDeployment` class is used to predict the classes. Making inferences, from within the notebook session, is best done using the [ADS `model.deployment`](https://docs.oracle.com/en-us/iaas/tools/ads-sdk/latest/user_guide/model_deployment/model_deployment.html) module. Outside of a notebook session, there are a number of other choices such as the OCI command-line interface, OCI Python SDK, and OCI Java SDK."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "58206faa",
   "metadata": {},
   "outputs": [],
   "source": [
    "if SUBNET_ID != \"<subnet_id>\":\n",
    "    iris = datasets.load_iris()\n",
    "    print(deployment.predict(iris.data.tolist()))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2a5211fa",
   "metadata": {},
   "source": [
    "<a id=\"clean_up\"></a>\n",
    "# Clean Up\n",
    "\n",
    "This notebook created a number of resources. This section will remove them from your tenancy. It can take several minutes to execute."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e71ad946",
   "metadata": {},
   "outputs": [],
   "source": [
    "# remove the `job_script` file from the system\n",
    "job_script.close()\n",
    "\n",
    "if SUBNET_ID != \"<subnet_id>\":\n",
    "    # remove job\n",
    "    job = Job.from_datascience_job(job.id)\n",
    "    job.delete()\n",
    "\n",
    "    # Remove the model deployment.\n",
    "    deployment = ModelDeployer().get_model_deployment(deployment.model_deployment_id)\n",
    "    deployment.delete(wait_for_completion=True)\n",
    "\n",
    "    # Remove the model from the model catalog\n",
    "    mc.delete_model(model=model_id)\n",
    "\n",
    "    # Delete the log group and logs\n",
    "    OCILog(log_group_id=log_group.id, id=log.id).delete()\n",
    "    _ = OCILogGroup(id=log_group.id).delete()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fe04797a",
   "metadata": {},
   "source": [
    "<a id='ref'></a>\n",
    "# References\n",
    "\n",
    "- [ADS Library Documentation](https://docs.cloud.oracle.com/en-us/iaas/tools/ads-sdk/latest/index.html)\n",
    "- [Data Science YouTube Videos](https://www.youtube.com/playlist?list=PLKCk3OyNwIzv6CWMhvqSB_8MLJIZdO80L)\n",
    "- [OCI Data Science Documentation](https://docs.cloud.oracle.com/en-us/iaas/data-science/using/data-science.htm)\n",
    "- [Oracle Data & AI Blog](https://blogs.oracle.com/datascience/)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.9.6 64-bit",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.9"
  },
  "vscode": {
   "interpreter": {
    "hash": "aee8b7b246df8f9039afb4144a1f6fd8d2ca17a180786b69acc140d282b71a49"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
