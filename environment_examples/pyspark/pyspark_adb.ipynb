{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<font color=gray>ADS Sample Notebook.\n",
    "\n",
    "Copyright (c) 2021 Oracle, Inc. All rights reserved.\n",
    "Licensed under the Universal Permissive License v 1.0 as shown at https://oss.oracle.com/licenses/upl.\n",
    "</font>\n",
    "\n",
    "***\n",
    "# <font color=red>Using the Autonomous Database with PySpark</font>\n",
    "<p style=\"margin-left:10%; margin-right:10%;\">by the <font color=teal> Oracle Cloud Infrastructure Data Science Team</font></p>\n",
    "\n",
    "***\n",
    "\n",
    "## Overview:\n",
    "\n",
    "This notebook demonstrates how to use PySpark to process data in Oracle Cloud Infrastructure (OCI) Object Storage and save the results to an Oracle Autonomous Database. It also demonstrates how to query data from an ADB using a local PySpark session.\n",
    "\n",
    "**Important:**\n",
    "\n",
    "Placeholder text for required values are surrounded by angle brackets that must be removed when adding the indicated content. For example, when adding a database name to `database_name = \"<database_name>\"` would become `database_name = \"production\"`.\n",
    "\n",
    "## Prerequisites:\n",
    "- Experience with the topic: Intermediate\n",
    "- Professional experience: Intermediate\n",
    "\n",
    "\n",
    "## Objectives:\n",
    "This notebook covers the following topics:\n",
    " - <a href='#intro'>Introduction</a>\n",
    "     - <a href='#variables'>Setup the Required Variables</a>\n",
    "     - <a href='#credentials'>Obtain Credentials from the Vault</a>\n",
    "     - <a href='#setup_wallet'>Setup the Wallet</a>\n",
    " - <a href='#read_os'>Reading Data from Object Storage</a>\n",
    " - <a href='#save_adb'>Save the Data to the Database</a>\n",
    " - <a href='#read_adb'>Read from the Database using PySpark</a>\n",
    " - <a href='#clean_up'>Clean Up Artifacts</a>\n",
    " - <a href='#ref'>References</a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import base64\n",
    "import cx_Oracle\n",
    "import oci\n",
    "import os\n",
    "import shutil\n",
    "import tempfile\n",
    "import zipfile\n",
    "\n",
    "from ads.database import connection \n",
    "from ads.vault.vault import Vault\n",
    "from pyspark import SparkConf\n",
    "from pyspark.sql import SparkSession\n",
    "from urllib.parse import urlparse"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='intro'></a>\n",
    "# Introduction\n",
    "\n",
    "It has become a common practice to store structured and semi-structured data using services such as Object Storage. This provides a scalable solution to store vast quantities of data that can be post-processed. However, using a relational database management system (RDMS) such as the Oracle Autonomous Database provides advantages like ACID compliance, rapid relational joins, support for complex business logic, and more. It is important to be able to access information stored in Object Storage, process that information, and load it into an RBMS. This notebook demonstrates how to use PySpark, a Python interface to Apache Spark, to perform these operations.\n",
    "\n",
    "This notebook uses a publically accessible Object Storage location to read from. However, an Autonomous Database needs to be configured with permissions to create a table, write to that table, and read from it. It also assumes that the credentials to access the database are stored in the Vault. This is the best practice as it prevents the credentials from being stored locally or in the notebook where they may be accessible to others. If you do not have credentials stored in the Vault, see the `vault.ipynb` example notebook to guide you through the process of storing the credentials. Once credentials to the database, are stored in the Vault, you need the OCIDs for the Vault, encryption key, and the secret.\n",
    "\n",
    "Autonomous Databases have an additional level of security that is needed to access them and are wallet file. You can obtain the wallet file from your account administrator or download it using the steps that are outlined in the [downloading a wallet(https://docs.oracle.com/en-us/iaas/Content/Database/Tasks/adbconnecting.htm#access). The wallet file is a ZIP file. This notebook unzips the wallet and updates the configuration settings so you don't have to.\n",
    "\n",
    "The database connection also needs the TNS name of the database. Your database administrator can give you the TNS name of the database that you have access to.\n",
    "\n",
    "<a id='variables'></a>\n",
    "## Setup the Required Variables\n",
    "\n",
    "The required variables to set up are:\n",
    "\n",
    "1. `vault_id`, `key_id`, `secret_ocid`: The OCID of the secret by storing the username and password required to connect to your Autonomous Database in a secret within the OCI Vault service. Note that the secret is the credential needed to access a database. This notebook is designed so that any secret can be stored as long as it is in the form of a dictionary. To store your secret, just modify the dictionary, see the `vault.ipynb` example notebook for detailed steps to generate this OCID.\n",
    "1. `tnsname`: A TNS name valid for the database.\n",
    "1. `wallet_path`: The local path to your wallet ZIP file, see the `autonomous_database.ipynb` example notebook for instructions on accessing the wallet file."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "vault_id = \"<vault_id>\"\n",
    "key_id = \"<key_id>\"\n",
    "secret_ocid = \"<secret_ocid>\"\n",
    "tnsname = \"<tnsname>\"\n",
    "wallet_path = \"<wallet_path>\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='credentials'></a>\n",
    "## Obtain Credentials from the Vault\n",
    "\n",
    "If the `vault_id`, `key_id`, and `secret_id` have been updated, then the notebook obtains a handle to the vault with a variable called `vault`. This uses the `get_secret()` method to return a dictionary with the user credentials. The approach assumes that the Accelerated Data Science (ADS) library was used to store the secret."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if vault_id != \"<vault_id>\" and key_id != \"<key_id>\" and secret_ocid != \"<secret_ocid>\":\n",
    "    print(\"Getting wallet username and password\")\n",
    "    vault = Vault(vault_id=vault_id, key_id=key_id)\n",
    "    adb_creds = vault.get_secret(secret_ocid)\n",
    "    user = adb_creds[\"username\"]\n",
    "    password = adb_creds[\"password\"]\n",
    "else:\n",
    "    print(\"Skipping as it appears that you do not have vault, key, and secret ocid specified.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='setup_wallet'></a>\n",
    "## Setup the Wallet\n",
    "\n",
    "An Autonomous Database requires a wallet file to access the database. The `wallet_path` variable defines the location of this file. The next cell prepares the wallet file to make a connection to the database. It also creates the Autonomous Database connection string, `adb_url`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def setup_wallet(wallet_path):\n",
    "    \"\"\"\n",
    "    Prepare ADB wallet file for use in PySpark.\n",
    "    \"\"\"\n",
    "\n",
    "    temporary_directory = tempfile.mkdtemp()\n",
    "    zip_file_path = os.path.join(temporary_directory, \"wallet.zip\")\n",
    "\n",
    "    # Extract everything locally.\n",
    "    with zipfile.ZipFile(wallet_path, \"r\") as zip_ref:\n",
    "        zip_ref.extractall(temporary_directory)\n",
    "\n",
    "    return temporary_directory\n",
    "\n",
    "if wallet_path != \"<wallet_path>\":\n",
    "    print(\"Setting up wallet\")\n",
    "    tns_path = setup_wallet(wallet_path)\n",
    "else:\n",
    "    print(\"Skipping as it appears that you do not have wallet_path specified.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if \"tns_path\" in globals() and tnsname != \"<tnsname>\":\n",
    "    adb_url = f\"jdbc:oracle:thin:@{tnsname}?TNS_ADMIN={tns_path}\"\n",
    "else:\n",
    "    print(\"Skipping, as the tns_path or tnsname are not defined.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='read_os'></a>\n",
    "# Reading Data from Object Storage\n",
    "\n",
    "This notebook uses PySpark to access the Object Storage file. The next cell creates a Spark application called \"Python Spark SQL Example\" and returns a SparkContext. The `SparkContext`, normally called `sc`,  is a handle to the Spark application.\n",
    "\n",
    "The data file that is used is relatively small so the notebook uses PySpark by running a version of Spark in local mode. That means, it is running in the notebook session. For larger jobs, we recommended that you use the [Oracle Data Flow](https://www.oracle.com/big-data/data-flow/) service, which is an Oracle managed Spark service."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create a spark session\n",
    "sc = SparkSession \\\n",
    "    .builder \\\n",
    "    .appName(\"Python Spark SQL Example\") \\\n",
    "    .getOrCreate()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This notebook reads in a data file that is stored in an Oracle Object Storage file. This is defined with the `file_path` variable. The `SparkContext` with the `read.option().csv()` methods is used to read in the CSV file from Object Storage into a data frame."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "file_path = \"oci://hosted-ds-datasets@bigdatadatasciencelarge/synthetic/orcl_attrition.csv\"\n",
    "input_dataframe = sc.read.option(\"header\", \"true\").csv(file_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='save_adb'></a>\n",
    "# Save the Data to the Database\n",
    "\n",
    "This notebook creates a table in your database with the name specified with `table_name`. The name that is defined should be unique so that it does not interfere with any existing table in your database. If it does, change the value to something that is unique."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "table_name = \"ODSC_PYSPARK_ADB_DEMO\"\n",
    "\n",
    "if tnsname != \"<tnsname>\" and \"adb_url\" in globals():\n",
    "    print(\"Saving processed data to \" + adb_url)\n",
    "    properties = {\n",
    "        \"oracle.net.tns_admin\": tnsname,\n",
    "        \"password\": password,\n",
    "        \"user\": user,\n",
    "    }\n",
    "    input_dataframe.write.jdbc(\n",
    "        url=adb_url, table=table_name, properties=properties\n",
    "    )\n",
    "else:\n",
    "    print(\"Skipping as it appears that you do not have tnsname specified.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='read_adb'></a>\n",
    "# Read from the Database using PySpark\n",
    "\n",
    "PySpark can be used to load data from the Autonomous Database into a Spark application. The next cell makes a JDBC connection to the database defined using the `adb_url` variable and accesses the table defined with `table_name`. The credentials stored in the vault and previously read into memory are used. Once this command is executed, you can perform Spark operations on it.\n",
    "\n",
    "This table is relatively small so the notebook uses PySpark in the notebook session. However, for larger jobs, we recommended that you use the [Oracle Data Flow](https://www.oracle.com/big-data/data-flow/) service."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if \"adb_url\" in globals():\n",
    "    output_dataframe = sc.read \\\n",
    "        .format(\"jdbc\") \\\n",
    "        .option(\"url\", adb_url) \\\n",
    "        .option(\"dbtable\", table_name) \\\n",
    "        .option(\"user\", user) \\\n",
    "        .option(\"password\", password) \\\n",
    "        .load()\n",
    "else:\n",
    "    print(\"Skipping as it appears that you do not have adb_url configured.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The database table is loaded into Spark so that you can perform operations to transform, model, and much more. In the next cell, the notebook prints the table demonstrating that it was successfully loaded into Spark from the Autonomous Database."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if \"adb_url\" in globals():\n",
    "    output_dataframe.show()\n",
    "else:\n",
    "    print(\"Skipping as it appears that you do not have output_dataframe configured.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='clean_up'></a>\n",
    "# Clean Up Artifacts\n",
    "\n",
    "This notebook created a number of artifacts such as unzipping the wallet file, creating a database table, and starting a Spark cluster. The next cell removes these resources."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if wallet_path != \"<wallet_path>\":\n",
    "    connection.update_repository(key=\"pyspark_adb\", value=adb_creds) \n",
    "    connection.import_wallet(wallet_path=wallet_path, key=\"pyspark_adb\")\n",
    "    conn = cx_Oracle.connect(user, password, tnsname)\n",
    "    cursor = conn.cursor()\n",
    "    cursor.execute(f\"DROP TABLE {table_name}\")\n",
    "    cursor.close()\n",
    "    conn.close()\n",
    "else:\n",
    "    print(\"Skipping as it appears that you do not have wallet_path specified.\")\n",
    "    \n",
    "if \"tns_path\" in globals():\n",
    "    shutil.rmtree(tns_path)\n",
    "    \n",
    "sc.stop()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='ref'></a>\n",
    "# References\n",
    "\n",
    "* [PySpark Documentation](https://spark.apache.org/docs/latest/api/python/)\n",
    "* [Using sqlnet.ora file with JDBC](https://stackoverflow.com/questions/63696611/can-the-oracle-jdbc-thin-driver-use-a-sqlnet-ora-file-for-configuration)\n",
    "* [Connecting to an Autonomous Database](https://docs.oracle.com/en-us/iaas/Content/Database/Tasks/adbconnecting.htm)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:pyspv10]",
   "language": "python",
   "name": "conda-env-pyspv10-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
